{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udcbe SLM Fine-Tuning: Checkpoint Strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do We Need Checkpoints?\n",
    "\n",
    "Imagine this scenario:\n",
    "\n",
    "```\n",
    "Training Progress:\n",
    "Step 100/500 \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 loss: 2.1\n",
    "Step 200/500 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 loss: 1.8\n",
    "Step 300/500 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591 loss: 1.5\n",
    "Step 400/500 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 loss: 1.3\n",
    "\n",
    "\ud83d\udd34 COLAB DISCONNECTED - GPU SESSION EXPIRED\n",
    "\n",
    "4 hours of training... gone.\n",
    "```\n",
    "\n",
    "**Without checkpoints:** You start from zero. All progress lost.\n",
    "\n",
    "**With checkpoints:** You resume from step 300. Only 100 steps lost.\n",
    "\n",
    "---\n",
    "\n",
    "## The Four Reasons You Need Checkpoints\n",
    "\n",
    "### 1\ufe0f\u20e3 Recovery from Crashes\n",
    "\n",
    "Training can fail for many reasons:\n",
    "- Colab session timeout (most common!)\n",
    "- GPU out of memory\n",
    "- Network disconnection\n",
    "- Power outage\n",
    "- Accidental notebook restart\n",
    "\n",
    "Checkpoints are your **insurance policy**.\n",
    "\n",
    "### 2\ufe0f\u20e3 Experiment with Different Training Lengths\n",
    "\n",
    "Not sure if 200 steps or 500 steps is better?\n",
    "\n",
    "```\n",
    "checkpoint-100/ \u2192 Test this version\n",
    "checkpoint-200/ \u2192 Test this version\n",
    "checkpoint-300/ \u2192 Test this version\n",
    "```\n",
    "\n",
    "Compare outputs without retraining!\n",
    "\n",
    "### 3\ufe0f\u20e3 Catch Overfitting\n",
    "\n",
    "Remember our Part 3 experiment? Loss went UP after step 150.\n",
    "\n",
    "```\n",
    "Step 100: loss 1.6  \u2190 Good\n",
    "Step 150: loss 1.5  \u2190 Best!\n",
    "Step 200: loss 1.8  \u2190 Overfitting\n",
    "```\n",
    "\n",
    "With checkpoints, you can **go back** to checkpoint-150 and use that instead.\n",
    "\n",
    "### 4\ufe0f\u20e3 Share and Deploy Specific Versions\n",
    "\n",
    "```\n",
    "checkpoint-200/ \u2192 Good for task A\n",
    "checkpoint-400/ \u2192 Good for task B\n",
    "```\n",
    "\n",
    "Different checkpoints = different model behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "## What's Inside a Checkpoint?\n",
    "\n",
    "```\n",
    "./outputs/checkpoint-100/\n",
    "\u2502\n",
    "\u251c\u2500\u2500 adapter_model.safetensors   \u2190 LoRA weights (the A & B matrices)\n",
    "\u251c\u2500\u2500 adapter_config.json         \u2190 LoRA settings (r, alpha, targets)\n",
    "\u2502\n",
    "\u251c\u2500\u2500 optimizer.pt                \u2190 Optimizer momentum & state\n",
    "\u251c\u2500\u2500 scheduler.pt                \u2190 Learning rate scheduler state\n",
    "\u251c\u2500\u2500 trainer_state.json          \u2190 Step count, loss history, etc.\n",
    "\u251c\u2500\u2500 rng_state.pth               \u2190 Random seeds (reproducibility)\n",
    "\u2514\u2500\u2500 training_args.bin           \u2190 Training configuration\n",
    "```\n",
    "\n",
    "| File | For Resuming | For Inference |\n",
    "|------|:------------:|:-------------:|\n",
    "| adapter_model.safetensors | \u2705 | \u2705 |\n",
    "| adapter_config.json | \u2705 | \u2705 |\n",
    "| optimizer.pt | \u2705 | \u274c |\n",
    "| scheduler.pt | \u2705 | \u274c |\n",
    "| trainer_state.json | \u2705 | \u274c |\n",
    "\n",
    "**Key insight:** Inference only needs adapter files. Resuming needs everything.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "-XJZdus-k8nf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1\ufe0f\u20e3 Setup"
   ],
   "metadata": {
    "id": "wgXy6pxck8nj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers datasets peft trl accelerate bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbEbxOxwk8nk"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\u2705 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"\u274c No GPU! Change runtime to GPU.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmxzXFJbk8nm"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 GPU: Tesla T4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2\ufe0f\u20e3 Configuration with Checkpoint Strategy"
   ],
   "metadata": {
    "id": "1lnbb3mEk8nn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATASET_ID = \"mlabonne/guanaco-llama2-1k\"\n",
    "OUTPUT_DIR = \"./checkpoint_demo\"\n",
    "\n",
    "# Training settings\n",
    "MAX_SEQ_LEN = 512\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM = 8\n",
    "MAX_STEPS = 200\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# LoRA settings\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "\n",
    "# ============================================================\n",
    "# CHECKPOINT STRATEGY \u2b50\n",
    "# ============================================================\n",
    "\n",
    "SAVE_STRATEGY = \"steps\"    # \"steps\", \"epoch\", or \"no\"\n",
    "SAVE_STEPS = 50            # Save every 50 steps\n",
    "SAVE_TOTAL_LIMIT = 4       # Keep only 4 checkpoints (saves disk space)\n",
    "\n",
    "print(\"\u2705 Configuration loaded\")\n",
    "print(f\"\")\n",
    "print(f\"\ud83d\udcc1 Checkpoint Strategy:\")\n",
    "print(f\"   Save every: {SAVE_STEPS} steps\")\n",
    "print(f\"   Keep max: {SAVE_TOTAL_LIMIT} checkpoints\")\n",
    "print(f\"   Location: {OUTPUT_DIR}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwuuQhedk8nn"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Configuration loaded\n",
      "\n",
      "\ud83d\udcc1 Checkpoint Strategy:\n",
      "   Save every: 50 steps\n",
      "   Keep max: 4 checkpoints\n",
      "   Location: ./checkpoint_demo\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3\ufe0f\u20e3 Load Dataset, Model, and Apply LoRA"
   ],
   "metadata": {
    "id": "MkCIL-S-k8no"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Dataset\n",
    "dataset = load_dataset(DATASET_ID, split=\"train\")\n",
    "print(f\"\u2705 Dataset: {len(dataset)} samples\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"\u2705 Model loaded: {MODEL_ID}\")\n",
    "\n",
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531,
     "referenced_widgets": [
      "13d3066131fd49618b9526db1de8eb6f",
      "db10f8f4c0e04fafbf6df2ab0faa9edb",
      "fac95c2089314737b5e4226f92c66349",
      "a728fa108e7d4654b26aabad289288a9",
      "d5abe7e9dccb4cf8baaf9a0e4658c41a",
      "ebd20d1c371740e1b242b567f44443ae",
      "b0ef1739cd5e42729983e215762cd886",
      "4536f5e4b55c4f96bf67589ee4001a77",
      "9396d36f2165472883b8e2093a1f983f",
      "4b3c75e0ffc041c292832ef947a25517",
      "487c48606df64704848cc416c36a525d",
      "9c7beccb6e07402c8f487d8c2eb3fd80",
      "d04ff6c9eda841a7a4d537a4173b11b2",
      "9d90ef2f63bb4992a941cabc8992536f",
      "24361b133ae94544a0f7bda2c24f19f7",
      "e875fe3b9ce04fc6a4953b4572927323",
      "9ef39bd5bcf14c459bfb6e3d9c20435e",
      "611579d7251a4b598c1244c0b8253da1",
      "8e06f90057944727ae55173d1bd33203",
      "9044d0d4ec1c4598b6e4c7718957adfd",
      "1b7cddeb27c74d9d870ade0be87f5fc5",
      "7caebf930c034f0fb33930cb547b5d47",
      "c7d17f36680641a2abfee67e6dc93e67",
      "8459d74bbb154d85b3d00c68a08fc509",
      "7a63bae8c35a4231b4f9ed39460d7ea4",
      "8335ee49fb614cb1b1e6eefd2f1549f8",
      "80ce4feae14f4ee9a83350f256bb66a3",
      "9442a552486d4acebf03f377b35ec2b1",
      "f5c5a45543084fe78a26f4b642a31877",
      "f8c0610dc0b94ceb8d5be1cd2e9631f0",
      "69711a6918ff48b7b1ab135da3100e51",
      "4f470e3b68ac474eacbcd995eeab53e4",
      "8f1045f6f4d1431890b111f4c667484e",
      "3e4790a194d748669125b7b2000a7558",
      "a50dcf7d72f942ecbb6e36774865e76c",
      "a5352ebe9de54cdb95f8820bc3b5aeb0",
      "c0e1067d0f99481baf13335c1044889b",
      "b5f5679af6674072b086405f2da2d609",
      "d28ad5e02acd4ea3859ccc45dc0e4f1f",
      "4fe5d2012eba49e49fbe5338e53a3ed9",
      "c490b8e0d3f342f6bbd7daa587af837a",
      "7ab3e63fcad9410db43d6389b73fb1a1",
      "1a2ee5c59ce44f38963d0d080e5bead2",
      "91a1fcf902064e0fbea29d7cfde49336",
      "a393f7aeac92449b8b39c0f4ee966b1a",
      "1eacf08703814d9d8fe3433da97660e3",
      "ad123794480542c49be8390bd488a9de",
      "1697ffffea0b49f48ca6aed099f12167",
      "321b520b58ca422ebab3a27a504c14b3",
      "2d2b7d1594a741f7a86b871487a4f805",
      "d67403da04a64c3d9c97278a4f7f2796",
      "1c4138f42c534426ad2fd98185d0d09a",
      "fa6a53a63af74702ad699305523096b9",
      "daf981ad25b04c9cbbc152be90cbbe77",
      "dff7fa72d41c4adf9ceba3a5b64f1cce",
      "71eba8e6c87348519afbe797a2a6c906",
      "e9d5702bbb6d470abefe6925ded4ea82",
      "293f21189c77400983dcaae90eba10ce",
      "6c6230c6371146a7847e747a511f0bdc",
      "b1e70142c3f94cd6b6c70081a368416b",
      "b7c566eb881e4b4c8c9b445da344d3e3",
      "6e5d03cab3824209abc355f2817bcd66",
      "82f6d0c74291430a9e640e63308fcd9b",
      "37d58547729e4e439da8e5eca48ba39c",
      "ff29659c2a544716a178c255f602f8e3",
      "88250c9425b74057be733b3dfcc2280a",
      "4c6cc305b4a14f588991af576c612aa4",
      "211c2c075ab742318071d783b9355ee6",
      "df1bcd0a8e7e4818a2b4a15a24be2bd4",
      "c86a7e8d041646748b47a2de2a9d8ae0",
      "f2efcab9a80f4e049715a5ffc2cbd28f",
      "a716671359ff4171aa985b158fe8f265",
      "31176e9aeb3f4351be90751e9ba013bf",
      "d4373c25738d43d08fb897c31a235dcb",
      "887cc066b1ca43d0b7dc5b24c85bf1b2",
      "f1cf3a8e693d4ec4ad590ab0bd48a83b",
      "6449e94c2a6b420d96294bbfe4e5c206",
      "a71dff15e8904032beb013925409308d",
      "71461f39b641404abb8835455b4b3d05",
      "70be0cda0c19450399d44db7ed5de997",
      "1ad8e920263449919ddf079ed537510e",
      "ab3ee85d7bb54048b3641efbdc954747",
      "ff2e31d373814ce6b6d6f3eaa29a9979",
      "5c5a10c7d97e4bec9f1ff329e0306886",
      "99e58e50814f45679a3fd6579cfc2339",
      "71a8d62e43fd4f77a36d731c2954f20a",
      "4793d7c06468414d8540f93d6b08e583",
      "d5651a7fbc8f4a4b8e94d9ba1dbcbd8e",
      "1ba5fa35ac2140c5812d7ab40355de0b",
      "bcdc14ed39be4cbe87d85288dc82d30b",
      "f1cb183545da4176ac26efb6e21a2da4",
      "44a0e93ebf1a4aa99fc1f26dd19bbe4e",
      "2c50bdae7aed450abca7887237d32f92",
      "86381bb1275d487eb1edce2e098e2a7e",
      "45dba3b2118443d38a1e223086d64462",
      "ca3a5045e0bc4dfc86130a0b4e774a11",
      "0bd8ab3787ae48959d6ad0e79db715d2",
      "a774278b9c6e4d93ba7a8ce1dc98f62d",
      "4f39ccbe84034dc98f5c2e061e23f993",
      "58868f179d0e4036a7e2c034534016d7",
      "8ad1bb6dce3a4126b2f16051288934bd",
      "9b0bd009db614eaa86aeea9bc3626ed8",
      "acdb83e618ac43b6b423a967ddaa1028",
      "5ce0b2f2279e427a99f9059fceb668f5",
      "ad0a6f1cbe504b34aa2bdfda7c226ece",
      "d9ba0faa8f404195a517323445b232c6",
      "2200e03f51bc4708ba498248bd51d19a",
      "ecc5d23f6b07449f9e6d6bfb7eaaec56",
      "767a12e6905643a1b5cdbd517918cd0c",
      "d6646a76bf3643bc830aeba8e86e1e5a"
     ]
    },
    "id": "p-Iwbv8yk8np"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001-9ad84bb9cf65a4(\u2026):   0%|          | 0.00/967k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Dataset: 1000 samples\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Model loaded: Qwen/Qwen2.5-0.5B-Instruct\n",
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4\ufe0f\u20e3 Training Arguments with Checkpoints\n",
    "\n",
    "Here's where checkpoint strategy is configured:"
   ],
   "metadata": {
    "id": "2k316ogak8nq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # Training settings\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # ============================================================\n",
    "    # CHECKPOINT SETTINGS \u2b50\n",
    "    # ============================================================\n",
    "    save_strategy=SAVE_STRATEGY,      # When to save: \"steps\", \"epoch\", \"no\"\n",
    "    save_steps=SAVE_STEPS,            # Save every N steps\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,# Max checkpoints to keep\n",
    "\n",
    "    # Misc\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"\u2705 Training arguments configured\")\n",
    "print(f\"\")\n",
    "print(f\"\ud83d\udcc1 Checkpoints will be saved at steps:\")\n",
    "print(f\"   50, 100, 150, 200\")\n",
    "print(f\"\")\n",
    "print(f\"\ud83d\udcc1 With save_total_limit={SAVE_TOTAL_LIMIT}, oldest will be deleted\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0qrbECbk8nq"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Training arguments configured\n",
      "\n",
      "\ud83d\udcc1 Checkpoints will be saved at steps:\n",
      "   50, 100, 150, 200\n",
      "\n",
      "\ud83d\udcc1 With save_total_limit=4, oldest will be deleted\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5\ufe0f\u20e3 Create Trainer and Train"
   ],
   "metadata": {
    "id": "wYLgwtq_k8nr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    # max_seq_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "print(\"\u2705 Trainer ready\")\n",
    "print(\"\")\n",
    "print(\"\ud83d\ude80 Starting training...\")\n",
    "print(\"   Watch for checkpoint saves!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254,
     "referenced_widgets": [
      "8b7cf7417ad54aa087c20211ec60c9e6",
      "37ff3a49de4a4605825489abacd5c477",
      "0a00baa86dad4d77bb9ee7e66a827414",
      "6c4325e9790441dfae0bbbaaecc00c2a",
      "9b14c5eb72964d0c940d06e11d91864e",
      "f5cf5d7c3dce4b92a2c7f8b198b242ea",
      "620121afc6fb445ba8344866bd396587",
      "86db63f1c637445bb586977d4a06c491",
      "e4386218f41049e4a04590e8e21fd79c",
      "6316adb820b24f17962ff448454275b4",
      "29caff958e6046aeba410d71a9330c5b",
      "70822601bc1f462da572942195a71d6e",
      "118308a0287c476b9db2353c9e51c0d5",
      "f0c2fc6d397d4bf4addf421ab8213e6a",
      "3174cbb4fa984561a32f064ffcd2ef14",
      "b2fc576cf4a14d73b0c0050ecb4cbf00",
      "3a42733f69f4411899851b221a691ab2",
      "73d958a2ceb442e8a1f794bbd973ba50",
      "059b49714c4147848c78e58337a0ef36",
      "363cc836077b43a99e6a279516da4e11",
      "d430957df4104c64844309b419290e38",
      "dcbd01a12c264817b59ff2cd371f7ab4",
      "97f3298ef6854d7883e89253bdc0e823",
      "b7b1aa1129454198ae480852bb36f669",
      "728342b7ecb34115a0b7c111cd4581a2",
      "f17a3ae9e60a4f63947ed6cd7c321785",
      "4e773f16d89b47fb994a59f068521f4e",
      "a3e973b4b2594dbfa1ab57e22dc7a7aa",
      "e3d4ed04afa24c4fbe7a3d28407c605b",
      "8ccee2ca56f6476b9c35f45291dc1145",
      "feb486af42f54aefb684c65e16d59709",
      "eb9ad5169a5142df8d00e2f6bd5d244f",
      "c694db27cb7a47488691823ce2d84dd3"
     ]
    },
    "id": "FgF1bpFRk8nr"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of \ud83e\udd17 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Trainer ready\n",
      "\n",
      "\ud83d\ude80 Starting training...\n",
      "   Watch for checkpoint saves!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\u2705 Training complete!\")\n",
    "print(\"=\"*50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "TiouxaFnk8nr"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 08:14, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.875900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.782200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.637000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.680500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "==================================================\n",
      "\u2705 Training complete!\n",
      "==================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6\ufe0f\u20e3 Explore Your Checkpoints\n",
    "\n",
    "Let's see what was saved:"
   ],
   "metadata": {
    "id": "NcUiaU3wk8nr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(\"\ud83d\udcc1 Contents of output directory:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for item in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    item_path = os.path.join(OUTPUT_DIR, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        # Calculate folder size\n",
    "        size = sum(os.path.getsize(os.path.join(item_path, f))\n",
    "                   for f in os.listdir(item_path)\n",
    "                   if os.path.isfile(os.path.join(item_path, f)))\n",
    "        print(f\"\ud83d\udcc2 {item}/ ({size/1024/1024:.1f} MB)\")\n",
    "    else:\n",
    "        size = os.path.getsize(item_path)\n",
    "        print(f\"\ud83d\udcc4 {item} ({size/1024:.1f} KB)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac_j-1mLk8ns"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcc1 Contents of output directory:\n",
      "==================================================\n",
      "\ud83d\udcc4 README.md (1.4 KB)\n",
      "\ud83d\udcc2 checkpoint-100/ (27.7 MB)\n",
      "\ud83d\udcc2 checkpoint-150/ (27.7 MB)\n",
      "\ud83d\udcc2 checkpoint-200/ (27.7 MB)\n",
      "\ud83d\udcc2 checkpoint-50/ (27.7 MB)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Look inside a checkpoint\n",
    "checkpoint_dir = os.path.join(OUTPUT_DIR, \"checkpoint-100\")\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"\ud83d\udcc2 Inside {checkpoint_dir}:\")\n",
    "    print(\"=\"*50)\n",
    "    for f in sorted(os.listdir(checkpoint_dir)):\n",
    "        size = os.path.getsize(os.path.join(checkpoint_dir, f))\n",
    "        if size > 1024*1024:\n",
    "            print(f\"   {f} ({size/1024/1024:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   {f} ({size/1024:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"Checkpoint-100 not found (may have been deleted by save_total_limit)\")\n",
    "    print(f\"Available checkpoints: {[d for d in os.listdir(OUTPUT_DIR) if d.startswith('checkpoint')]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "debuZgSBk8ns"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcc2 Inside ./checkpoint_demo/checkpoint-100:\n",
      "==================================================\n",
      "   README.md (5.1 KB)\n",
      "   adapter_config.json (1.0 KB)\n",
      "   adapter_model.safetensors (4.1 MB)\n",
      "   added_tokens.json (0.6 KB)\n",
      "   chat_template.jinja (2.4 KB)\n",
      "   merges.txt (1.6 MB)\n",
      "   optimizer.pt (8.4 MB)\n",
      "   rng_state.pth (14.3 KB)\n",
      "   scaler.pt (1.4 KB)\n",
      "   scheduler.pt (1.4 KB)\n",
      "   special_tokens_map.json (0.6 KB)\n",
      "   tokenizer.json (10.9 MB)\n",
      "   tokenizer_config.json (4.6 KB)\n",
      "   trainer_state.json (3.4 KB)\n",
      "   training_args.bin (6.1 KB)\n",
      "   vocab.json (2.6 MB)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udd04 Resuming from Checkpoint\n",
    "\n",
    "## Simulating a Crash\n",
    "\n",
    "Let's pretend our training crashed at step 100. We'll:\n",
    "1. Clear the current model from memory\n",
    "2. Reload everything fresh\n",
    "3. Resume from checkpoint"
   ],
   "metadata": {
    "id": "NDAO-3tvk8ns"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# SIMULATE CRASH: Clear everything\n",
    "# ============================================================\n",
    "\n",
    "import gc\n",
    "\n",
    "# Delete model and trainer\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\ud83d\udca5 SIMULATED CRASH\")\n",
    "print(\"   Model deleted from memory\")\n",
    "print(\"   Pretend Colab disconnected here...\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vllrFmXtk8ns"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udca5 SIMULATED CRASH\n",
      "   Model deleted from memory\n",
      "   Pretend Colab disconnected here...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# RECOVERY: Reload and Resume\n",
    "# ============================================================\n",
    "\n",
    "print(\"\ud83d\udd04 RECOVERING FROM CRASH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Reload base model\n",
    "print(\"\\n1\ufe0f\u20e3 Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Step 2: Apply LoRA config (same as before)\n",
    "print(\"2\ufe0f\u20e3 Applying LoRA configuration...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Step 3: Create trainer with SAME settings\n",
    "print(\"3\ufe0f\u20e3 Creating trainer...\")\n",
    "\n",
    "# For resuming, we might want to train MORE steps\n",
    "resume_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    max_steps=300,  # \u2b50 Extended! Was 200, now 300\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=resume_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    # max_seq_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Ready to resume!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6tZdcYSk8nt"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udd04 RECOVERING FROM CRASH\n",
      "==================================================\n",
      "\n",
      "1\ufe0f\u20e3 Loading base model...\n",
      "2\ufe0f\u20e3 Applying LoRA configuration...\n",
      "3\ufe0f\u20e3 Creating trainer...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of \ud83e\udd17 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Ready to resume!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# RESUME TRAINING\n",
    "# ============================================================\n",
    "\n",
    "# Find available checkpoints\n",
    "checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith('checkpoint')]\n",
    "print(f\"\ud83d\udcc1 Available checkpoints: {sorted(checkpoints)}\")\n",
    "print(\"\")\n",
    "\n",
    "# Option A: Resume from LATEST checkpoint automatically\n",
    "print(\"\ud83d\ude80 Resuming from latest checkpoint...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)  # \u2b50 Auto-finds latest\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\u2705 Training resumed and completed!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "wnN8xHNnk8nt"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcc1 Available checkpoints: ['checkpoint-100', 'checkpoint-150', 'checkpoint-200', 'checkpoint-50']\n",
      "\n",
      "\ud83d\ude80 Resuming from latest checkpoint...\n",
      "==================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 03:39, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.608300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.728800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "==================================================\n",
      "\u2705 Training resumed and completed!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# ALTERNATIVE: Resume from SPECIFIC checkpoint\n",
    "# ============================================================\n",
    "\n",
    "# If you want to resume from a specific checkpoint:\n",
    "# trainer.train(resume_from_checkpoint=\"./checkpoint_demo/checkpoint-100\")\n",
    "\n",
    "print(\"\ud83d\udca1 To resume from a specific checkpoint:\")\n",
    "print(\"\")\n",
    "print('   trainer.train(resume_from_checkpoint=\"./checkpoint_demo/checkpoint-100\")')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvvG1uMsk8nt"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udca1 To resume from a specific checkpoint:\n",
      "\n",
      "   trainer.train(resume_from_checkpoint=\"./checkpoint_demo/checkpoint-100\")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udd0d Comparing Checkpoints\n",
    "\n",
    "Now let's load different checkpoints and compare their outputs.\n",
    "\n",
    "This helps you:\n",
    "- See how the model evolves during training\n",
    "- Find the best checkpoint (not always the last one!)\n",
    "- Understand when overfitting starts"
   ],
   "metadata": {
    "id": "y64KsTI2k8nt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# First, clear memory\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\ud83e\uddf9 Memory cleared\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALfprXSHk8nt"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddf9 Memory cleared\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model once\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"\u2705 Base model loaded\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VJxjqsnk8nu"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading base model...\n",
      "\u2705 Base model loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# COMPARE OUTPUTS FROM DIFFERENT CHECKPOINTS\n",
    "# ============================================================\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"### Human: Explain what machine learning is in simple terms.\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "# Find all checkpoints\n",
    "checkpoints = sorted([d for d in os.listdir(OUTPUT_DIR) if d.startswith('checkpoint')])\n",
    "print(f\"\ud83d\udcc1 Found checkpoints: {checkpoints}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Compare each checkpoint\n",
    "for ckpt_name in checkpoints:\n",
    "    ckpt_path = os.path.join(OUTPUT_DIR, ckpt_name)\n",
    "\n",
    "    # Load adapter onto base model\n",
    "    model = PeftModel.from_pretrained(base_model, ckpt_path)\n",
    "    model.eval()\n",
    "\n",
    "    # Generate\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"### Assistant:\")[-1].strip()\n",
    "\n",
    "    print(f\"\\n\ud83d\udd39 {ckpt_name}\")\n",
    "    print(\"-\"*70)\n",
    "    print(response[:300] + \"...\" if len(response) > 300 else response)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Unload adapter to load next one\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp5nOvaAk8nu"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcc1 Found checkpoints: ['checkpoint-150', 'checkpoint-200', 'checkpoint-250', 'checkpoint-300']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udd39 checkpoint-150\n",
      "----------------------------------------------------------------------\n",
      "Machine Learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to learn from data, identify patterns, and make predictions or decisions. It can be applied to a wide range of tasks such as image recognition, natural language proc...\n",
      "======================================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udd39 checkpoint-200\n",
      "----------------------------------------------------------------------\n",
      "Machine Learning is a subset of artificial intelligence that involves the development and application of algorithms and statistical models to enable computers to learn from data, identify patterns, make predictions or decisions, and improve performance over time. It can be used for a wide range of t...\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udd39 checkpoint-250\n",
      "----------------------------------------------------------------------\n",
      "Machine Learning is a type of artificial intelligence that allows computers to learn and improve their performance on a specific task by analyzing data. It involves using algorithms and statistical models to identify patterns, trends, and relationships within large sets of data, which can then be us...\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udd39 checkpoint-300\n",
      "----------------------------------------------------------------------\n",
      "Machine learning is a subset of artificial intelligence that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions based on that data. It can be used for a variety of tasks, such as image recognition, speech recognition, natural ...\n",
      "======================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udd17 Merging Adapter into Base Model\n",
    "\n",
    "Two ways to use your trained model:\n",
    "\n",
    "| Method | File Size | Speed | Flexibility |\n",
    "|--------|-----------|-------|-------------|\n",
    "| **Keep adapter separate** | Small (~10MB) | Slightly slower | Can swap adapters |\n",
    "| **Merge into base** | Large (~1GB) | Slightly faster | Single model file |\n"
   ],
   "metadata": {
    "id": "ufWCqOfKk8nu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# METHOD 1: Load Adapter Separately (Recommended)\n",
    "# ============================================================\n",
    "\n",
    "print(\"Method 1: Loading adapter separately\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Choose best checkpoint (let's use the latest)\n",
    "best_checkpoint = sorted(checkpoints)[-1]\n",
    "adapter_path = os.path.join(OUTPUT_DIR, best_checkpoint)\n",
    "\n",
    "print(f\"Loading adapter from: {adapter_path}\")\n",
    "\n",
    "# Load\n",
    "model_with_adapter = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "print(\"\u2705 Model ready for inference!\")\n",
    "print(f\"\")\n",
    "print(f\"\ud83d\udcca Adapter size: ~{sum(p.numel() for p in model_with_adapter.parameters() if p.requires_grad) * 2 / 1024 / 1024:.1f} MB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BjsCZrWk8nu"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Method 1: Loading adapter separately\n",
      "==================================================\n",
      "Loading adapter from: ./checkpoint_demo/checkpoint-300\n",
      "\u2705 Model ready for inference!\n",
      "\n",
      "\ud83d\udcca Adapter size: ~0.0 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# METHOD 2: Merge Adapter into Base Model\n",
    "# ============================================================\n",
    "\n",
    "print(\"Method 2: Merging adapter into base model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Merge\n",
    "merged_model = model_with_adapter.merge_and_unload()\n",
    "\n",
    "print(\"\u2705 Adapter merged!\")\n",
    "print(f\"\")\n",
    "print(f\"\ud83d\udcca Merged model size: ~{sum(p.numel() for p in merged_model.parameters()) * 2 / 1024 / 1024:.1f} MB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MfPUb7ztk8nu"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Method 2: Merging adapter into base model\n",
      "==================================================\n",
      "\u2705 Adapter merged!\n",
      "\n",
      "\ud83d\udcca Merged model size: ~942.3 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# SAVE MERGED MODEL\n",
    "# ============================================================\n",
    "\n",
    "merged_output_dir = \"./qwen_finetuned_merged\"\n",
    "\n",
    "print(f\"Saving merged model to: {merged_output_dir}\")\n",
    "\n",
    "merged_model.save_pretrained(merged_output_dir)\n",
    "tokenizer.save_pretrained(merged_output_dir)\n",
    "\n",
    "print(\"\\n\u2705 Merged model saved!\")\n",
    "print(f\"\")\n",
    "print(\"\ud83d\udcc1 Contents:\")\n",
    "for f in os.listdir(merged_output_dir):\n",
    "    size = os.path.getsize(os.path.join(merged_output_dir, f))\n",
    "    if size > 1024*1024:\n",
    "        print(f\"   {f} ({size/1024/1024:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   {f} ({size/1024:.1f} KB)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eBJiyFUk8nv"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving merged model to: ./qwen_finetuned_merged\n",
      "\n",
      "\u2705 Merged model saved!\n",
      "\n",
      "\ud83d\udcc1 Contents:\n",
      "   merges.txt (1.6 MB)\n",
      "   special_tokens_map.json (0.6 KB)\n",
      "   vocab.json (2.6 MB)\n",
      "   model.safetensors (942.3 MB)\n",
      "   generation_config.json (0.2 KB)\n",
      "   config.json (1.2 KB)\n",
      "   added_tokens.json (0.6 KB)\n",
      "   tokenizer.json (10.9 MB)\n",
      "   chat_template.jinja (2.4 KB)\n",
      "   tokenizer_config.json (4.6 KB)\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image](Part_4_checkpoints.PNG)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udcdd Quick Reference: Checkpoint Strategies\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "```python\n",
    "TrainingArguments(\n",
    "    # When to save\n",
    "    save_strategy=\"steps\",      # \"steps\", \"epoch\", or \"no\"\n",
    "    save_steps=100,             # Save every N steps\n",
    "    \n",
    "    # How many to keep\n",
    "    save_total_limit=3,         # Keep only 3 most recent\n",
    "    \n",
    "    # Save best model (requires evaluation)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "```\n",
    "\n",
    "## Common Operations\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Resume from latest | `trainer.train(resume_from_checkpoint=True)` |\n",
    "| Resume from specific | `trainer.train(resume_from_checkpoint=\"./checkpoint-100\")` |\n",
    "| Load adapter | `PeftModel.from_pretrained(base_model, \"./checkpoint-100\")` |\n",
    "| Merge adapter | `model.merge_and_unload()` |\n",
    "| Save merged | `merged_model.save_pretrained(\"./merged\")` |\n",
    "\n",
    "## Recommended Strategy by Use Case\n",
    "\n",
    "| Situation | Strategy |\n",
    "|-----------|----------|\n",
    "| Quick experiment | `save_strategy=\"no\"`, save at end |\n",
    "| Long training | `save_steps=100`, `save_total_limit=3` |\n",
    "| Finding best model | Add evaluation + `load_best_model_at_end=True` |\n",
    "| Unreliable connection | `save_steps=50` (frequent saves) |\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "1. **Checkpoints are insurance** \u2014 Always save during long training runs\n",
    "\n",
    "2. **Use `save_total_limit`** \u2014 Don't fill your disk with checkpoints\n",
    "\n",
    "3. **Resume is easy** \u2014 Just pass `resume_from_checkpoint=True`\n",
    "\n",
    "4. **Compare checkpoints** \u2014 Best model isn't always the last one\n",
    "\n",
    "5. **Two ways to deploy:**\n",
    "   - Keep adapter separate (small, flexible)\n",
    "   - Merge into base (standalone model)"
   ],
   "metadata": {
    "id": "s9sU65ZSk8nv"
   }
  }
 ]
}