{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \ud83d\udcca Part 6: Evaluation Methods\n",
        "\n",
        "**The Core Question:** How do you know if fine-tuning actually worked?\n",
        "\n",
        "---\n",
        "\n",
        "## What We'll Cover\n",
        "\n",
        "| Method | Description | Difficulty |\n",
        "|--------|-------------|------------|\n",
        "| Perplexity | Mathematical measure of model confidence | Easy |\n",
        "| Generation Comparison | Side-by-side output comparison | Easy |\n",
        "| Automated Metrics | Task-specific measurements | Medium |\n",
        "| LLM-as-Judge | Use stronger model to evaluate | Medium |\n",
        "| A/B Testing | Pairwise comparison | Medium |\n",
        "\n",
        "---\n",
        "\n",
        "## The Evaluation Hierarchy\n",
        "\n",
        "```\n",
        "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "                    \u2502 Human Evaluation \u2502 \u2190 Gold standard (expensive)\n",
        "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                             \u2502\n",
        "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "                    \u2502  LLM-as-Judge   \u2502 \u2190 Scalable approximation\n",
        "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                             \u2502\n",
        "         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "         \u2502                   \u2502                   \u2502\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502   Perplexity    \u2502 \u2502   Generation    \u2502 \u2502   Benchmarks    \u2502\n",
        "\u2502   (automatic)   \u2502 \u2502   Comparison    \u2502 \u2502   (if available)\u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## \u26a0\ufe0f Important: Fresh Runtime\n",
        "\n",
        "**Before running this notebook:**\n",
        "1. Go to `Runtime` \u2192 `Restart runtime` (or `Disconnect and delete runtime`)\n",
        "2. This clears any previous memory usage\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "f1vJ5tKPodGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Setup & Training\n",
        "\n",
        "First, let's train a model so we have something to evaluate!"
      ],
      "metadata": {
        "id": "40nAeOkEodGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\u2705 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"   Total Memory: {total_mem:.1f} GB\")\n",
        "\n",
        "    # Show current usage\n",
        "    allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    print(f\"   Currently Used: {allocated:.2f} GB\")\n",
        "else:\n",
        "    print(\"\u274c No GPU - this notebook requires a GPU!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVdgMnl8odGr",
        "outputId": "b9d00727-ce42-4bb6-84f1-379065e24399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 GPU: Tesla T4\n",
            "   Total Memory: 15.8 GB\n",
            "   Currently Used: 0.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "OUTPUT_DIR = \"./eval_adapter\"\n",
        "\n",
        "print(f\"Base model: {MODEL_ID}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H-r5D2aodGr",
        "outputId": "678f40f8-b0b6-494e-caf2-539efa0eb776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model: Qwen/Qwen2.5-0.5B-Instruct\n",
            "Output: ./eval_adapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model with 4-bit Quantization\n",
        "\n",
        "To save memory on T4, we'll use 4-bit quantization."
      ],
      "metadata": {
        "id": "t6727UoUodGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-bit quantization config for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"\u2705 4-bit quantization config ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ-JqR3QodGt",
        "outputId": "49acdeb4-a4e3-4556-d731-0ee2c3c7bfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 4-bit quantization config ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "print(\"Loading model with 4-bit quantization...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Prepare for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(f\"\u2705 Model loaded\")\n",
        "print(f\"   GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "3b4d37b4e6f84d2e96de753a34ff2b21",
            "d30827a53df44b8ea6652b6889a5b7c8",
            "2540e23ecfe347aabbf604ef17de4335",
            "7f0db63da98145aa84f9187667823e4c",
            "9e613d311e51423888bf7059d6a16cc4",
            "10c2f8d773704f80ba78a2b2e6d16b72",
            "146680f6540140288aaa68df0e850ac4",
            "9c00207960a2456b89fbfa6b61eca84f",
            "85c3b299d95f4975a57fdf362d4bcc15",
            "824b47a96182436ca3a3e9fef5073418",
            "f85144f3ab1b4753bf24bd12b24ff648",
            "9121d3dbca034d83902d487645aede8f",
            "d1ce76d598464e02b106bffbe31a2a94",
            "e1a3ec4b7ab842dea83cab845eb83539",
            "78d5d9f9ea24415db8a6504bcf2aed49",
            "97f70a06a73e4e30823669968b68168e",
            "8b3a39a5f53e4c00b69cdce53de50624",
            "6f6bc898225a4dee843cc0e62aec3bcd",
            "f6a9a5a4c0c74071a2b8212702b5423d",
            "a6354b6ccb9d4c2099298f00abeedd00",
            "601871aac8e7435987d556094b09ca43",
            "08a888e709bc4b358743b5bbb1d98348",
            "efab61c617894edbb803d34d383ba9b9",
            "066fa32357b742a2bea76fac9afcfc03",
            "074f064e92454c77985bece04be98a80",
            "4681c5d041ba4aa0aa2d9328631f2823",
            "437efce8043e4d40b6f68d4a7dd32fda",
            "bfb426cdf2d04ed095d7b1c0b4c68eae",
            "43c11a422d934160be1cbf6459bba3d8",
            "005f22671f9d43f681cf6a89ceb60d07",
            "df69f55482f74c2a8eeb710d3e2c1d3e",
            "275638c0274e4f7e9b366dc7e50347ab",
            "84c0e813a5cd4b90b091a73a29e7ed7b",
            "f062b69269374bbcaff372041058b175",
            "2dbc025c287f4bbb94e5d60b507d5938",
            "6135576a4adf4ee8964cd54dd19dca8d",
            "6ac1350e297d42269eff5472613be9ca",
            "1335c9c6f6804efca4f6a419f8988e43",
            "402e348d2cd445fbaf0654c1d8d2bfe7",
            "f2d94034c17b4952a0978cc5981291bc",
            "5c599c60b8cd41b790c2983dceac813a",
            "2dcd24dbe1e74cc78fa46143a790c4c0",
            "38311fa9358e4a1bae93dd75ea30e122",
            "b83db8d5df1b414793eb98e875a432ec",
            "0c0f2e0664f04798ba88306e4b0bda84",
            "8e05179ab8964b49bba9faa17c638f55",
            "9b9d43da64cf4e32bffe758836b008c8",
            "7118f3f504f445c699bcc899c95c4828",
            "813ef5ed40da445b8495a9b22c9822e4",
            "255994a092984148984772346538fd0c",
            "ec301799ae8a4c00b0907c3ddfd52f19",
            "875efb7606bd4fe4bcea8d4356a2f365",
            "adb71aaecefe4bce9de5b9778986fa0c",
            "56f9c6387cd94f6089d690d1bd29ad70",
            "7278ce631bac42f4b3e7df72311398bc",
            "935fe9b7683a4743a359981b159eac4f",
            "279c0474d5b742a9b094e10cf5069c35",
            "484b969511c4430a8380c0c3bc58fe02",
            "99b50dbccaf749d8b0eeeb8b8c41f6a7",
            "62522f58dc3c4565992074431e4c3efe",
            "d58eb22392f249f2938aa5e726198dfd",
            "766c9420792c4a7b8b3b4e88ae3ad5b5",
            "b88a2819ae684b1b8aa4ed4c327c3791",
            "e531f8e258c74d908eb0ccf14cc60f2b",
            "4b1d734b876040d5a9db17020cb69103",
            "fe395555bea044aeab0195c80e277b3d",
            "e66d8c3aa5b0415680fab9b750bd50ac",
            "b1fa31b322364a05927d2daef4e1905b",
            "f218c94ab87b4b25b1c3f54c301a2b09",
            "f4e9746cc21d419499104f2f2039bc1b",
            "7d3c5767498244d495cbeff2f60a50e7",
            "8bc5483676ed48a8832082d4fdeb870e",
            "7da8a2c555144ca4840e9538cf970645",
            "6dd38559a47d442aaf5bbef411c1d154",
            "d0a3a73fa5a24159bfedaf85ce5090d1",
            "93cc2af0c262463abfd5e44e082f0c22",
            "8223ddf467d44ab782d5703c2deac7f5"
          ]
        },
        "id": "vph93AZModGu",
        "outputId": "7b7917c4-5a82-4bab-ca1e-0ad3f1155546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with 4-bit quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Model loaded\n",
            "   GPU Memory: 0.73 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
        "\n",
        "# Split into train and eval\n",
        "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"\u2705 Train: {len(train_dataset)} examples\")\n",
        "print(f\"\u2705 Eval:  {len(eval_dataset)} examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "1dd6040b38334a1c980f82c6d551c21c",
            "4c94ac3f69784f788a5a2b58eaa8fc76",
            "193206fbfd9c4fa1ba0ccbae6424e6bc",
            "cb373789bd0d40a1b1fa304fc1eec848",
            "cdc813e7d4d84f2f9e66230ed9332bd9",
            "dd618ae20efc42cd9ff55b3dec479af2",
            "6df969a5ebdd4d72a43b5117100cb41d",
            "bf8b57226d3c476f93320db4d02dae85",
            "587cf006c68943209898db26f86dc26c",
            "7a02a37958aa4ec3a5577761f3fd36eb",
            "e4c4df4eb4d14dde8019d235ce7afe48",
            "f87fda284f7348dab0c576d31da54321",
            "db5b7f24cf0e4e569559211a7272846b",
            "daae9890946c488395121c2146aa8343",
            "45761b43f23246c8998e348cccc14e4f",
            "b6f7fac4330646a982ebd41521600fb6",
            "10431281430c4a1c98835125614276c0",
            "6bdb309579864e21b836e1596e4f5919",
            "3772677d109643e2b9996e69e1561c4c",
            "6f1628f053cd42ac81e917087587736c",
            "5b91cf34c37546908f296188159439b8",
            "a290a615a0364e37a393edf28ddf19a6",
            "bf6ece2dd73b4f7a8ad002f850a39c90",
            "a5fca85b9df04619948bc6a995ed6cc9",
            "5547d7108b7e4d64afdecf786d0c2453",
            "5c3847212d9740b59b97fa78d7897814",
            "a600b5703ee644c2a58ab22514a474b0",
            "11c86acb011d478185444a506c3d0371",
            "b4db96acde1649f8aa976ed250a3bad2",
            "f548c68823c9462380d9d243de268334",
            "2dd929851bd94a93966e17e91ef3bc34",
            "4c869ce4e95f49c6bc0954428c3e6659",
            "2670256e846e41e3bdbcc03a02e88aa8"
          ]
        },
        "id": "_ZK99YPhodGv",
        "outputId": "a2cd2615-7c47-43c4-f6ea-a6d9ee4bcffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Train: 900 examples\n",
            "\u2705 Eval:  100 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with Memory-Efficient Settings"
      ],
      "metadata": {
        "id": "R5lvtM19odGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA Configuration - smaller for memory efficiency\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                  # Reduced from 16\n",
        "    lora_alpha=16,        # Reduced from 32\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Fewer modules\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx3DGVm4odGw",
        "outputId": "5de98314-0c9d-466d-efb0-f4f2b85d4cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory-efficient training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,      # Reduced from 4\n",
        "    gradient_accumulation_steps=4,       # Increased to compensate\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"adamw_torch\",\n",
        "    warmup_ratio=0.03,\n",
        "    gradient_checkpointing=True,         # Saves memory!\n",
        "    max_grad_norm=0.3,\n",
        ")\n",
        "\n",
        "print(\"\u2705 Training arguments configured\")\n",
        "print(f\"   Effective batch size: {2 * 4} = 8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YyRnVPyodGw",
        "outputId": "9c634719-f7bb-451f-a765-fcbe49b4bc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Training arguments configured\n",
            "   Effective batch size: 8 = 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer with smaller sequence length\n",
        "MAX_seq_length = 256\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    #max_seq_length=MAX_seq_length,                  # Reduced from 512\n",
        ")\n",
        "\n",
        "print(\"\u2705 Trainer ready\")\n",
        "print(f\"   GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXvB-23_odGx",
        "outputId": "f346835d-e2d4-4cad-d53d-6eeb012e4f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Trainer ready\n",
            "   GPU Memory: 0.75 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of \ud83e\udd17 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!\n",
        "print(\"\ud83d\ude80 Starting training...\")\n",
        "print(\"   This will take ~3-5 minutes on T4\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\u2705 Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Cv22WZ9LodGx",
        "outputId": "6a6b3e21-9277-4386-f3ca-e94e25f20767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\ude80 Starting training...\n",
            "   This will take ~3-5 minutes on T4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the adapter\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "print(f\"\u2705 Adapter saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0p5rWtdodGy",
        "outputId": "a61f57ee-df55-492f-b041-30e0bdbc138b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Adapter saved to ./eval_adapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload for Evaluation\n",
        "\n",
        "Now we'll reload a fresh model (without quantization) for accurate evaluation."
      ],
      "metadata": {
        "id": "BQd76zIrodGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload base model (fp16, no quantization for accurate eval)\n",
        "print(\"Loading model for evaluation...\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load fine-tuned model (base + adapter)\n",
        "print(\"Loading fine-tuned adapter...\")\n",
        "finetuned_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    OUTPUT_DIR,\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Model ready for evaluation!\")\n",
        "print(f\"   GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ZUzvqHodGy",
        "outputId": "116e5b60-362e-485c-953e-c7b0df38d2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model for evaluation...\n",
            "Loading fine-tuned adapter...\n",
            "\n",
            "\u2705 Model ready for evaluation!\n",
            "   GPU Memory: 1.75 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part B: Evaluation Methods\n",
        "\n",
        "Now let's evaluate our fine-tuned model using multiple methods!"
      ],
      "metadata": {
        "id": "y9DEJ_-sodGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Method 1: Perplexity\n",
        "\n",
        "## The Math\n",
        "\n",
        "**Loss** (Cross-Entropy):\n",
        "$$\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P(x_i | x_{<i})$$\n",
        "\n",
        "**Perplexity**:\n",
        "$$\\text{PPL} = e^{\\mathcal{L}}$$\n",
        "\n",
        "## Intuition\n",
        "\n",
        "```\n",
        "Perplexity = 1:    Perfect prediction (impossible in practice)\n",
        "Perplexity = 10:   Model choosing between ~10 likely tokens\n",
        "Perplexity = 100:  Model very uncertain\n",
        "Perplexity = 1000: Model is basically guessing\n",
        "```\n",
        "\n",
        "## What's Good?\n",
        "\n",
        "| Model Type | Typical Perplexity |\n",
        "|------------|-------------------|\n",
        "| State-of-the-art LLM | 5-15 |\n",
        "| Good fine-tuned model | 10-30 |\n",
        "| Average model | 30-100 |\n",
        "| Poor model | 100+ |\n",
        "\n",
        "**Important:** Perplexity depends heavily on the evaluation dataset!"
      ],
      "metadata": {
        "id": "ZIfq9UXgodGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, tokenizer, texts, max_length=256):\n",
        "    \"\"\"\n",
        "    Calculate perplexity on a list of texts.\n",
        "\n",
        "    Perplexity = exp(average_loss)\n",
        "    Lower is better.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            inputs = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=max_length\n",
        "            ).to(model.device)\n",
        "\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "\n",
        "            num_tokens = inputs[\"input_ids\"].numel()\n",
        "            total_loss += outputs.loss.item() * num_tokens\n",
        "            total_tokens += num_tokens\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return {\n",
        "        \"perplexity\": perplexity,\n",
        "        \"avg_loss\": avg_loss,\n",
        "        \"total_tokens\": total_tokens\n",
        "    }\n",
        "\n",
        "print(\"\u2705 calculate_perplexity() defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHKPuVwWodGz",
        "outputId": "77f64402-239d-4cc4-8527-f13e51b27fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 calculate_perplexity() defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare evaluation texts\n",
        "eval_texts = [item[\"text\"] for item in eval_dataset]\n",
        "\n",
        "print(f\"\ud83d\udcca Evaluating on {len(eval_texts)} texts...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGlmlY22odG0",
        "outputId": "b7c922cb-04c7-45ba-f6dc-eee33808b813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcca Evaluating on 100 texts...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for FINE-TUNED model (adapter enabled)\n",
        "print(\"=\"*50)\n",
        "print(\"FINE-TUNED MODEL (with adapter)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "finetuned_ppl = calculate_perplexity(finetuned_model, tokenizer, eval_texts)\n",
        "print(f\"Perplexity: {finetuned_ppl['perplexity']:.2f}\")\n",
        "print(f\"Avg Loss:   {finetuned_ppl['avg_loss']:.4f}\")\n",
        "print(f\"Tokens:     {finetuned_ppl['total_tokens']:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pCGQ8KQodG0",
        "outputId": "609763c6-91ef-4565-bba7-95aba47c3ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "FINE-TUNED MODEL (with adapter)\n",
            "==================================================\n",
            "Perplexity: 7.68\n",
            "Avg Loss:   2.0381\n",
            "Tokens:     21,746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate BASE model perplexity (disable adapter)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"BASE MODEL (adapter disabled)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Use PEFT's native method instead of transformers' method\n",
        "with finetuned_model.disable_adapter():\n",
        "    base_ppl = calculate_perplexity(finetuned_model, tokenizer, eval_texts)\n",
        "\n",
        "print(f\"Perplexity: {base_ppl['perplexity']:.2f}\")\n",
        "print(f\"Avg Loss:   {base_ppl['avg_loss']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pqq6JdJodG1",
        "outputId": "3cb20948-c6a9-4853-beca-1db3820ae73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "BASE MODEL (adapter disabled)\n",
            "==================================================\n",
            "Perplexity: 9.01\n",
            "Avg Loss:   2.1983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERPLEXITY COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "ppl_change = ((finetuned_ppl['perplexity'] - base_ppl['perplexity']) / base_ppl['perplexity']) * 100\n",
        "\n",
        "print(f\"\\n{'Model':<20} {'Perplexity':<15} {'Loss':<10}\")\n",
        "print(\"-\" * 45)\n",
        "print(f\"{'Base Model':<20} {base_ppl['perplexity']:<15.2f} {base_ppl['avg_loss']:<10.4f}\")\n",
        "print(f\"{'Fine-tuned':<20} {finetuned_ppl['perplexity']:<15.2f} {finetuned_ppl['avg_loss']:<10.4f}\")\n",
        "print(\"-\" * 45)\n",
        "print(f\"{'Change':<20} {ppl_change:+.1f}%\")\n",
        "\n",
        "if ppl_change < 0:\n",
        "    print(\"\\n\u2705 Fine-tuning IMPROVED perplexity (lower is better)\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f Fine-tuning INCREASED perplexity\")\n",
        "    print(\"   This can happen if the model specialized on different content.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bdQ4YvOodG1",
        "outputId": "84138734-92ba-40be-8371-ca1b0cd9843b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "PERPLEXITY COMPARISON\n",
            "==================================================\n",
            "\n",
            "Model                Perplexity      Loss      \n",
            "---------------------------------------------\n",
            "Base Model           9.01            2.1983    \n",
            "Fine-tuned           7.68            2.0381    \n",
            "---------------------------------------------\n",
            "Change               -14.8%\n",
            "\n",
            "\u2705 Fine-tuning IMPROVED perplexity (lower is better)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Method 2: Generation Comparison\n",
        "\n",
        "**The most intuitive evaluation:** Look at actual outputs!\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502   Same Prompt   \u2502\u2500\u2500\u2500\u2500\u25ba\u2502   Base Model    \u2502\u2500\u2500\u2500\u2500\u25ba Response A\n",
        "\u2502                 \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\u2502                 \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                 \u2502\u2500\u2500\u2500\u2500\u25ba\u2502 Fine-tuned Model\u2502\u2500\u2500\u2500\u2500\u25ba Response B\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                                \u2502\n",
        "                                \u25bc\n",
        "                        Compare A vs B\n",
        "```"
      ],
      "metadata": {
        "id": "fKJfhA1jodG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(model, tokenizer, prompt, max_new_tokens=150):\n",
        "    \"\"\"\n",
        "    Generate a response from the model.\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    try:\n",
        "        formatted = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except:\n",
        "        formatted = f\"User: {prompt}\\nAssistant:\"\n",
        "\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "print(\"\u2705 generate_response() defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg2RvY8QodG2",
        "outputId": "e4f072a6-4b74-4f8d-bbb5-c088177e8ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 generate_response() defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Explain machine learning to a 10-year-old.\",\n",
        "    \"What are three tips for being more productive?\",\n",
        "    \"Write a short poem about coding.\",\n",
        "    \"How do I make a simple HTTP request in Python?\",\n",
        "    \"What's the difference between AI and machine learning?\"\n",
        "]\n",
        "\n",
        "print(f\"\ud83d\udcdd {len(test_prompts)} test prompts ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKaSWqDVodG2",
        "outputId": "8db9f222-fd5d-4caf-8b68-66abf3804a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcdd 5 test prompts ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and compare\n",
        "print(\"=\"*70)\n",
        "print(\"GENERATION COMPARISON: BASE vs FINE-TUNED\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "base_responses = []\n",
        "finetuned_responses = []\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\n{'\u2500'*70}\")\n",
        "    print(f\"\ud83d\udcdd Prompt {i+1}: {prompt}\")\n",
        "    print(f\"{'\u2500'*70}\")\n",
        "\n",
        "    # Base model (adapter disabled)\n",
        "    with finetuned_model.disable_adapter():\n",
        "        base_resp = generate_response(finetuned_model, tokenizer, prompt)\n",
        "    base_responses.append(base_resp)\n",
        "\n",
        "    # Fine-tuned model (adapter enabled) - no context manager needed, it's the default\n",
        "    ft_resp = generate_response(finetuned_model, tokenizer, prompt)\n",
        "    finetuned_responses.append(ft_resp)\n",
        "\n",
        "    print(f\"\\n\ud83d\udd35 Base Model:\")\n",
        "    print(base_resp[:400] + \"...\" if len(base_resp) > 400 else base_resp)\n",
        "\n",
        "    print(f\"\\n\ud83d\udfe2 Fine-tuned Model:\")\n",
        "    print(ft_resp[:400] + \"...\" if len(ft_resp) > 400 else ft_resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZbyk_yjodG2",
        "outputId": "66658e9e-02ee-45f7-ed96-0a5160c7ebdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "GENERATION COMPARISON: BASE vs FINE-TUNED\n",
            "======================================================================\n",
            "\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\ud83d\udcdd Prompt 1: Explain machine learning to a 10-year-old.\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\n",
            "\ud83d\udd35 Base Model:\n",
            "Sure! Machine learning is like a magical game where computers learn from data and do better at guessing what's happening without being told exactly how to guess.\n",
            "\n",
            "Imagine you have a toy car that can move around on the floor. If I tell it to go forward for three times, the computer will remember that it went forward once and then another time, just like how you might remember going to bed and wakin...\n",
            "\n",
            "\ud83d\udfe2 Fine-tuned Model:\n",
            "Sure! Machine Learning is like when you have a toy car that can learn how to drive itself in the park. It's like teaching your car what to do without being told every single time.\n",
            "\n",
            "Imagine your toy car has a memory of where it's been before and how it got there. Then one day, you put the car on the road and ask it to go somewhere else. The car will try its best to figure out the path and get there...\n",
            "\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\ud83d\udcdd Prompt 2: What are three tips for being more productive?\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\n",
            "\ud83d\udd35 Base Model:\n",
            "1. Prioritize tasks: Identify what needs to be done first and then tackle those tasks in order of importance.\n",
            "2. Set goals: Define clear, measurable objectives that you want to achieve and break them down into smaller, manageable steps.\n",
            "3. Take breaks: Regularly take short breaks to refresh your mind and prevent burnout. This can help improve focus and productivity over time.\n",
            "\n",
            "\ud83d\udfe2 Fine-tuned Model:\n",
            "1. Prioritize tasks: Start your day with a clear understanding of what needs to be accomplished and prioritize the most important ones first.\n",
            "2. Use a planner or schedule: Keep track of deadlines, appointments, and other commitments so that you can stay organized and on top of things.\n",
            "3. Break down large tasks into smaller steps: Break larger tasks into smaller, manageable steps and set specific g...\n",
            "\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\ud83d\udcdd Prompt 3: Write a short poem about coding.\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\n",
            "\ud83d\udd35 Base Model:\n",
            "Coding is the code of life,\n",
            "A language that connects us all.\n",
            "It's the magic word that makes our world run smoothly,\n",
            "And it's the bridge we cross to connect with others.\n",
            "\n",
            "Coding is like solving puzzles,\n",
            "Finding solutions that fit together perfectly.\n",
            "It's like making art or building houses,\n",
            "Each step a new piece of the puzzle to solve.\n",
            "\n",
            "Coding is the foundation for innovation and progress,\n",
            "The abili...\n",
            "\n",
            "\ud83d\udfe2 Fine-tuned Model:\n",
            "In the heart of code's realm,\n",
            "A world where programs and algorithms dance.\n",
            "From simple to complex, each line is a story,\n",
            "Where logic meets mind, and ideas bloom.\n",
            "\n",
            "The lines flow like rivers through stone,\n",
            "Through loops and circuits, they weave.\n",
            "Each step a calculation, each decision made,\n",
            "As numbers and symbols play their part.\n",
            "\n",
            "But in this realm of programming codes,\n",
            "There lies beauty, and wonder...\n",
            "\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\ud83d\udcdd Prompt 4: How do I make a simple HTTP request in Python?\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\n",
            "\ud83d\udd35 Base Model:\n",
            "To create and send an HTTP GET request in Python, you can use the `requests` library. First, ensure that you have it installed in your environment. If not, you can install it using pip:\n",
            "\n",
            "```bash\n",
            "pip install requests\n",
            "```\n",
            "\n",
            "Here's a simple example of how to make a basic GET request with the `requests` library:\n",
            "\n",
            "1. **Install the `requests` library**: As mentioned earlier, you need to install the `requ...\n",
            "\n",
            "\ud83d\udfe2 Fine-tuned Model:\n",
            "To make a simple HTTP request in Python, you can use the `requests` library. Here's an example of how to send a GET request using this library:\n",
            "\n",
            "```python\n",
            "import requests\n",
            "\n",
            "# Define your URL and data (if needed)\n",
            "url = \"http://example.com\"\n",
            "data = {\"key\": \"value\"}\n",
            "\n",
            "# Send a GET request with specified headers and data\n",
            "response = requests.get(url, params=data)\n",
            "\n",
            "# Check if the response was successful\n",
            "if...\n",
            "\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\ud83d\udcdd Prompt 5: What's the difference between AI and machine learning?\n",
            "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
            "\n",
            "\ud83d\udd35 Base Model:\n",
            "AI (Artificial Intelligence) is the umbrella term for the research and development of intelligent machines that can perform tasks typically associated with humans. It encompasses various applications such as speech recognition, natural language processing, robotics, autonomous vehicles, virtual assistants, and more.\n",
            "\n",
            "Machine Learning (ML), on the other hand, is a subset of AI that focuses specific...\n",
            "\n",
            "\ud83d\udfe2 Fine-tuned Model:\n",
            "AI (Artificial Intelligence) is a broad field that encompasses the creation of intelligent machines that can perform tasks requiring intelligence, such as reasoning, problem-solving, decision-making, creativity, learning, self-correction, self-improvement, and communication.\n",
            "\n",
            "Machine learning, on the other hand, is a subset of AI that focuses on developing algorithms for computers to learn from da...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Method 3: Response Statistics"
      ],
      "metadata": {
        "id": "WyznA82eodG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_responses(responses):\n",
        "    \"\"\"Calculate basic statistics about generated responses.\"\"\"\n",
        "    lengths = [len(r.split()) for r in responses]\n",
        "    unique_words = [len(set(r.lower().split())) for r in responses]\n",
        "\n",
        "    return {\n",
        "        \"avg_length\": sum(lengths) / len(lengths),\n",
        "        \"min_length\": min(lengths),\n",
        "        \"max_length\": max(lengths),\n",
        "        \"avg_unique_words\": sum(unique_words) / len(unique_words),\n",
        "        \"lexical_diversity\": sum(unique_words) / sum(lengths) if sum(lengths) > 0 else 0\n",
        "    }\n",
        "\n",
        "# Analyze both\n",
        "print(\"\ud83d\udcca Response Statistics\\n\")\n",
        "print(f\"{'Metric':<25} {'Base':<15} {'Fine-tuned':<15}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "base_stats = analyze_responses(base_responses)\n",
        "ft_stats = analyze_responses(finetuned_responses)\n",
        "\n",
        "for key in base_stats:\n",
        "    print(f\"{key:<25} {base_stats[key]:<15.2f} {ft_stats[key]:<15.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW5kazf8odG3",
        "outputId": "9ef79665-c2fb-404a-9c62-d4e7efb46970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcca Response Statistics\n",
            "\n",
            "Metric                    Base            Fine-tuned     \n",
            "-------------------------------------------------------\n",
            "avg_length                111.20          109.80         \n",
            "min_length                62.00           81.00          \n",
            "max_length                135.00          133.00         \n",
            "avg_unique_words          82.40           81.40          \n",
            "lexical_diversity         0.74            0.74           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Method 4: LLM-as-Judge\n",
        "\n",
        "Use a stronger model (GPT-4, Claude) to evaluate responses."
      ],
      "metadata": {
        "id": "s_0x9e5wodG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_judge_prompt(question, response):\n",
        "    \"\"\"Create a prompt for LLM-as-Judge evaluation.\"\"\"\n",
        "    return f\"\"\"You are evaluating an AI assistant's response. Rate it on three criteria.\n",
        "\n",
        "## Question Asked:\n",
        "{question}\n",
        "\n",
        "## AI Response:\n",
        "{response}\n",
        "\n",
        "## Evaluation Criteria:\n",
        "\n",
        "1. **Helpfulness** (1-10): Does it actually answer the question?\n",
        "2. **Accuracy** (1-10): Is the information correct?\n",
        "3. **Clarity** (1-10): Is it easy to understand?\n",
        "\n",
        "## Your Evaluation:\n",
        "\n",
        "Helpfulness: [score]/10 - [reason]\n",
        "Accuracy: [score]/10 - [reason]\n",
        "Clarity: [score]/10 - [reason]\n",
        "\n",
        "Overall: [average]/10\n",
        "\"\"\"\n",
        "\n",
        "# Example\n",
        "print(\"\ud83d\udcdd Example Judge Prompt:\")\n",
        "print(\"=\"*60)\n",
        "print(create_judge_prompt(test_prompts[0], finetuned_responses[0][:200] + \"...\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcOW0qRBodG3",
        "outputId": "f498f8ef-0224-4c3b-d7bb-5282f58f3b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcdd Example Judge Prompt:\n",
            "============================================================\n",
            "You are evaluating an AI assistant's response. Rate it on three criteria.\n",
            "\n",
            "## Question Asked:\n",
            "Explain machine learning to a 10-year-old.\n",
            "\n",
            "## AI Response:\n",
            "Sure! Machine Learning is like when you have a toy car that can learn how to drive itself in the park. It's like teaching your car what to do without being told every single time.\n",
            "\n",
            "Imagine your toy ca...\n",
            "\n",
            "## Evaluation Criteria:\n",
            "\n",
            "1. **Helpfulness** (1-10): Does it actually answer the question?\n",
            "2. **Accuracy** (1-10): Is the information correct?\n",
            "3. **Clarity** (1-10): Is it easy to understand?\n",
            "\n",
            "## Your Evaluation:\n",
            "\n",
            "Helpfulness: [score]/10 - [reason]\n",
            "Accuracy: [score]/10 - [reason]\n",
            "Clarity: [score]/10 - [reason]\n",
            "\n",
            "Overall: [average]/10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To use this prompt:**\n",
        "- Copy to ChatGPT or Claude web interface (free)\n",
        "- Or use OpenAI/Anthropic APIs"
      ],
      "metadata": {
        "id": "c-oeR252odG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "======================================================================\n",
        "GENERATION COMPARISON: BASE vs FINE-TUNED\n",
        "======================================================================\n",
        "\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\ud83d\udcdd Prompt 1: Explain machine learning to a 10-year-old.\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "\ud83d\udd35 Base Model:\n",
        "Sure! Machine learning is like a magical game where computers learn from data and do better at guessing what's happening without being told exactly how to guess.\n",
        "\n",
        "Imagine you have a toy car that can move around on the floor. If I tell it to go forward for three times, the computer will remember that it went forward once and then another time, just like how you might remember going to bed and wakin...\n",
        "\n",
        "\ud83d\udfe2 Fine-tuned Model:\n",
        "Sure! Machine Learning is like when you have a toy car that can learn how to drive itself in the park. It's like teaching your car what to do without being told every single time.\n",
        "\n",
        "Imagine your toy car has a memory of where it's been before and how it got there. Then one day, you put the car on the road and ask it to go somewhere else. The car will try its best to figure out the path and get there...\n",
        "\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\ud83d\udcdd Prompt 2: What are three tips for being more productive?\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "\ud83d\udd35 Base Model:\n",
        "1. Prioritize tasks: Identify what needs to be done first and then tackle those tasks in order of importance.\n",
        "2. Set goals: Define clear, measurable objectives that you want to achieve and break them down into smaller, manageable steps.\n",
        "3. Take breaks: Regularly take short breaks to refresh your mind and prevent burnout. This can help improve focus and productivity over time.\n",
        "\n",
        "\ud83d\udfe2 Fine-tuned Model:\n",
        "1. Prioritize tasks: Start your day with a clear understanding of what needs to be accomplished and prioritize the most important ones first.\n",
        "2. Use a planner or schedule: Keep track of deadlines, appointments, and other commitments so that you can stay organized and on top of things.\n",
        "3. Break down large tasks into smaller steps: Break larger tasks into smaller, manageable steps and set specific g...\n",
        "\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\ud83d\udcdd Prompt 3: Write a short poem about coding.\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "\ud83d\udd35 Base Model:\n",
        "Coding is the code of life,\n",
        "A language that connects us all.\n",
        "It's the magic word that makes our world run smoothly,\n",
        "And it's the bridge we cross to connect with others.\n",
        "\n",
        "Coding is like solving puzzles,\n",
        "Finding solutions that fit together perfectly.\n",
        "It's like making art or building houses,\n",
        "Each step a new piece of the puzzle to solve.\n",
        "\n",
        "Coding is the foundation for innovation and progress,\n",
        "The abili...\n",
        "\n",
        "\ud83d\udfe2 Fine-tuned Model:\n",
        "In the heart of code's realm,\n",
        "A world where programs and algorithms dance.\n",
        "From simple to complex, each line is a story,\n",
        "Where logic meets mind, and ideas bloom.\n",
        "\n",
        "The lines flow like rivers through stone,\n",
        "Through loops and circuits, they weave.\n",
        "Each step a calculation, each decision made,\n",
        "As numbers and symbols play their part.\n",
        "\n",
        "But in this realm of programming codes,\n",
        "There lies beauty, and wonder...\n",
        "\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\ud83d\udcdd Prompt 4: How do I make a simple HTTP request in Python?\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "\ud83d\udd35 Base Model:\n",
        "To create and send an HTTP GET request in Python, you can use the `requests` library. First, ensure that you have it installed in your environment. If not, you can install it using pip:\n",
        "\n",
        "```bash\n",
        "pip install requests\n",
        "```\n",
        "\n",
        "Here's a simple example of how to make a basic GET request with the `requests` library:\n",
        "\n",
        "1. **Install the `requests` library**: As mentioned earlier, you need to install the `requ...\n",
        "\n",
        "\ud83d\udfe2 Fine-tuned Model:\n",
        "To make a simple HTTP request in Python, you can use the `requests` library. Here's an example of how to send a GET request using this library:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "# Define your URL and data (if needed)\n",
        "url = \"http://example.com\"\n",
        "data = {\"key\": \"value\"}\n",
        "\n",
        "# Send a GET request with specified headers and data\n",
        "response = requests.get(url, params=data)\n",
        "\n",
        "# Check if the response was successful\n",
        "if...\n",
        "\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\ud83d\udcdd Prompt 5: What's the difference between AI and machine learning?\n",
        "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "\n",
        "\ud83d\udd35 Base Model:\n",
        "AI (Artificial Intelligence) is the umbrella term for the research and development of intelligent machines that can perform tasks typically associated with humans. It encompasses various applications such as speech recognition, natural language processing, robotics, autonomous vehicles, virtual assistants, and more.\n",
        "\n",
        "Machine Learning (ML), on the other hand, is a subset of AI that focuses specific...\n",
        "\n",
        "\ud83d\udfe2 Fine-tuned Model:\n",
        "AI (Artificial Intelligence) is a broad field that encompasses the creation of intelligent machines that can perform tasks requiring intelligence, such as reasoning, problem-solving, decision-making, creativity, learning, self-correction, self-improvement, and communication.\n",
        "\n",
        "Machine learning, on the other hand, is a subset of AI that focuses on developing algorithms for computers to learn from da...\n",
        "\n",
        "\n",
        "\n",
        "## Evaluation Criteria:\n",
        "\n",
        "1. **Helpfulness** (1-10): Does it actually answer the question?\n",
        "2. **Accuracy** (1-10): Is the information correct?\n",
        "3. **Clarity** (1-10): Is it easy to understand?\n",
        "\n",
        "## Your Evaluation:\n",
        "\n",
        "Helpfulness: [score]/10 - [reason]\n",
        "Accuracy: [score]/10 - [reason]\n",
        "Clarity: [score]/10 - [reason]\n",
        "\n",
        "Overall: [average]/10\n",
        "\n",
        "Give the results for each response and calculate overall results for based model and finetuned model.\n"
      ],
      "metadata": {
        "id": "6nI2eKFbttqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\ud83d\udcdd Prompt 1\n",
        "\n",
        "Explain machine learning to a 10-year-old\n",
        "\n",
        "| Model          | Helpfulness | Accuracy | Clarity | Overall    |\n",
        "| -------------- | ----------- | -------- | ------- | ---------- |\n",
        "| **Base**       | 7/10        | 7/10     | 6/10    | **6.7/10** |\n",
        "| **Fine-tuned** | 8/10        | 7/10     | 8/10    | **7.7/10** |\n",
        "\n",
        "\n",
        "\ud83d\udcdd Prompt 2\n",
        "\n",
        "Three tips for being more productive\n",
        "\n",
        "| Model          | Helpfulness | Accuracy | Clarity | Overall    |\n",
        "| -------------- | ----------- | -------- | ------- | ---------- |\n",
        "| **Base**       | 8/10        | 9/10     | 9/10    | **8.7/10** |\n",
        "| **Fine-tuned** | 7/10        | 9/10     | 7/10    | **7.7/10** |\n",
        "\n",
        "\n",
        "\ud83d\udcdd Prompt 3\n",
        "\n",
        "Short poem about coding\n",
        "\n",
        "| Model          | Helpfulness | Accuracy | Clarity | Overall    |\n",
        "| -------------- | ----------- | -------- | ------- | ---------- |\n",
        "| **Base**       | 7/10        | 9/10     | 8/10    | **8.0/10** |\n",
        "| **Fine-tuned** | 8/10        | 9/10     | 8/10    | **8.3/10** |\n",
        "\n",
        "\n",
        "\ud83d\udcdd Prompt 4\n",
        "\n",
        "Simple HTTP request in Python\n",
        "| Model          | Helpfulness | Accuracy | Clarity | Overall    |\n",
        "| -------------- | ----------- | -------- | ------- | ---------- |\n",
        "| **Base**       | 9/10        | 9/10     | 9/10    | **9.0/10** |\n",
        "| **Fine-tuned** | 8/10        | 9/10     | 8/10    | **8.3/10** |\n",
        "\n",
        "\n",
        "\ud83d\udcdd Prompt 5\n",
        "\n",
        "Difference between AI and Machine Learning\n",
        "| Model          | Helpfulness | Accuracy | Clarity | Overall    |\n",
        "| -------------- | ----------- | -------- | ------- | ---------- |\n",
        "| **Base**       | 9/10        | 9/10     | 9/10    | **9.0/10** |\n",
        "| **Fine-tuned** | 8/10        | 9/10     | 8/10    | **8.3/10** |\n",
        "\n",
        "\n",
        "\n",
        "Final\n",
        "\n",
        "| Model                | Avg Helpfulness | Avg Accuracy | Avg Clarity | **Overall Average** |\n",
        "| -------------------- | --------------- | ------------ | ----------- | ------------------- |\n",
        "| **Base Model**       | 8.0 / 10        | 8.6 / 10     | 8.2 / 10    | **8.3 / 10**        |\n",
        "| **Fine-tuned Model** | 7.8 / 10        | 8.6 / 10     | 7.9 / 10    | **8.1 / 10**        |\n",
        "\n"
      ],
      "metadata": {
        "id": "NSsTSuKptySW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Method 5: A/B Testing (Pairwise Comparison)\n",
        "\n",
        "Ask: **\"Which response is better?\"** \u2014 This is the basis for DPO!"
      ],
      "metadata": {
        "id": "rTfV9vxHodG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pairwise_prompt(question, response_a, response_b):\n",
        "    \"\"\"Create a pairwise comparison prompt.\"\"\"\n",
        "    return f\"\"\"Compare these two AI responses and decide which is better.\n",
        "\n",
        "## Question:\n",
        "{question}\n",
        "\n",
        "## Response A:\n",
        "{response_a}\n",
        "\n",
        "## Response B:\n",
        "{response_b}\n",
        "\n",
        "## Your Judgment:\n",
        "\n",
        "Which response is better? Choose one:\n",
        "- A is much better\n",
        "- A is slightly better\n",
        "- About the same\n",
        "- B is slightly better\n",
        "- B is much better\n",
        "\n",
        "Choice: [Your choice]\n",
        "Reason: [Brief explanation]\n",
        "\"\"\"\n",
        "\n",
        "# Example\n",
        "print(\"\ud83d\udcdd Example Pairwise Prompt:\")\n",
        "print(\"=\"*60)\n",
        "print(create_pairwise_prompt(\n",
        "    test_prompts[0],\n",
        "    base_responses[0][:250],\n",
        "    finetuned_responses[0][:250]\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAzgJ4pCodG4",
        "outputId": "e8adb5fb-22dc-416a-e450-2163355c800e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcdd Example Pairwise Prompt:\n",
            "============================================================\n",
            "Compare these two AI responses and decide which is better.\n",
            "\n",
            "## Question:\n",
            "Explain machine learning to a 10-year-old.\n",
            "\n",
            "## Response A:\n",
            "Sure! Machine learning is like a magical game where computers learn from data and do better at guessing what's happening without being told exactly how to guess.\n",
            "\n",
            "Imagine you have a toy car that can move around on the floor. If I tell it to go forwar\n",
            "\n",
            "## Response B:\n",
            "Sure! Machine Learning is like when you have a toy car that can learn how to drive itself in the park. It's like teaching your car what to do without being told every single time.\n",
            "\n",
            "Imagine your toy car has a memory of where it's been before and how i\n",
            "\n",
            "## Your Judgment:\n",
            "\n",
            "Which response is better? Choose one:\n",
            "- A is much better\n",
            "- A is slightly better  \n",
            "- About the same\n",
            "- B is slightly better\n",
            "- B is much better\n",
            "\n",
            "Choice: [Your choice]\n",
            "Reason: [Brief explanation]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Summary Report"
      ],
      "metadata": {
        "id": "pz_EHSuModG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udcca EVALUATION SUMMARY REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n\ud83c\udff7\ufe0f  Model: {MODEL_ID}\")\n",
        "print(f\"\ud83d\udce6 Adapter: {OUTPUT_DIR}\")\n",
        "print(f\"\ud83d\udcdd Eval samples: {len(eval_texts)}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"1. PERPLEXITY (lower is better)\")\n",
        "print(\"-\"*70)\n",
        "print(f\"   Base Model:     {base_ppl['perplexity']:.2f}\")\n",
        "print(f\"   Fine-tuned:     {finetuned_ppl['perplexity']:.2f}\")\n",
        "print(f\"   Change:         {ppl_change:+.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"2. RESPONSE CHARACTERISTICS\")\n",
        "print(\"-\"*70)\n",
        "print(f\"   {'Metric':<25} {'Base':<12} {'Fine-tuned':<12}\")\n",
        "for key in ['avg_length', 'lexical_diversity']:\n",
        "    print(f\"   {key:<25} {base_stats[key]:<12.2f} {ft_stats[key]:<12.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"3. QUALITATIVE\")\n",
        "print(\"-\"*70)\n",
        "print(\"   Review the side-by-side comparisons above!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\u2705 Evaluation complete!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvQFoo0RodG5",
        "outputId": "c8442f32-6c9d-49f3-ebcb-2790a8c41cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\ud83d\udcca EVALUATION SUMMARY REPORT\n",
            "======================================================================\n",
            "\n",
            "\ud83c\udff7\ufe0f  Model: Qwen/Qwen2.5-0.5B-Instruct\n",
            "\ud83d\udce6 Adapter: ./eval_adapter\n",
            "\ud83d\udcdd Eval samples: 100\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "1. PERPLEXITY (lower is better)\n",
            "----------------------------------------------------------------------\n",
            "   Base Model:     9.01\n",
            "   Fine-tuned:     7.68\n",
            "   Change:         -14.8%\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "2. RESPONSE CHARACTERISTICS\n",
            "----------------------------------------------------------------------\n",
            "   Metric                    Base         Fine-tuned  \n",
            "   avg_length                111.20       109.80      \n",
            "   lexical_diversity         0.74         0.74        \n",
            "\n",
            "----------------------------------------------------------------------\n",
            "3. QUALITATIVE\n",
            "----------------------------------------------------------------------\n",
            "   Review the side-by-side comparisons above!\n",
            "\n",
            "======================================================================\n",
            "\u2705 Evaluation complete!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Key Takeaways\n",
        "\n",
        "| Method | Pros | Cons | When to Use |\n",
        "|--------|------|------|-------------|\n",
        "| **Perplexity** | Fast, automatic | Doesn't measure usefulness | Quick sanity check |\n",
        "| **Generation Comparison** | Intuitive, shows real behavior | Subjective | Always! |\n",
        "| **LLM-as-Judge** | Scalable, nuanced | Costs money | Large-scale eval |\n",
        "| **A/B Testing** | Simple, clear winner | Need both models | Comparing models |\n",
        "\n",
        "## Remember:\n",
        "\n",
        "1. **Perplexity alone isn't enough** \u2014 a model can have low perplexity but give bad responses\n",
        "2. **Always look at actual outputs** \u2014 generation comparison is most important\n",
        "3. **Pairwise comparison is the basis for DPO** \u2014 which we'll cover later!\n",
        "\n",
        "---\n",
        "\n",
        "## Next: Part 7 - Unsloth vs Standard\n",
        "\n",
        "We'll compare training speed and memory usage!"
      ],
      "metadata": {
        "id": "4Y80fVN1odHG"
      }
    }
  ]
}