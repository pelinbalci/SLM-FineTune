{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udd2c LoRA vs QLoRA vs DoRA: Math & Comparison\n",
    "\n",
    "This notebook explains the mathematical foundations of each adapter method and compares them experimentally.\n",
    "\n",
    "---\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "All three methods solve the same problem: **How to fine-tune a large model efficiently?**\n",
    "\n",
    "```\n",
    "Full Fine-tuning:     Update ALL parameters (expensive!)\n",
    "                      7B model = 14GB+ of gradients & optimizer states\n",
    "\n",
    "Adapter Methods:      Update SMALL adapter, freeze base model\n",
    "                      7B model = ~50MB of trainable parameters\n",
    "```\n",
    "\n",
    "| Method | Year | Key Innovation | Memory | Quality |\n",
    "|--------|------|----------------|--------|--------|\n",
    "| LoRA | 2021 | Low-rank decomposition | Medium | Baseline |\n",
    "| QLoRA | 2023 | 4-bit quantized base | Very Low | ~99% of LoRA |\n",
    "| DoRA | 2024 | Magnitude/Direction split | Medium | ~101% of LoRA |\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: The Mathematics\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 LoRA: Low-Rank Adaptation\n",
    "\n",
    "### The Problem\n",
    "\n",
    "A weight matrix in a transformer might be:\n",
    "\n",
    "$$W \\in \\mathbb{R}^{d \\times d}$$\n",
    "\n",
    "For a typical model with $d = 4096$:\n",
    "\n",
    "$$\\text{Parameters} = 4096 \\times 4096 = 16,777,216 \\text{ per layer!}$$\n",
    "\n",
    "### The LoRA Insight\n",
    "\n",
    "The weight **change** during fine-tuning ($\\Delta W$) has low intrinsic rank.\n",
    "\n",
    "Instead of learning full $\\Delta W$, factorize it:\n",
    "\n",
    "$$\\Delta W = B \\cdot A$$\n",
    "\n",
    "Where:\n",
    "- $A \\in \\mathbb{R}^{r \\times d}$ (down-projection)\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (up-projection)\n",
    "- $r \\ll d$ (typically $r = 8$ or $16$)\n",
    "\n",
    "### Parameter Savings\n",
    "\n",
    "$$\\text{Full: } d \\times d = 4096^2 = 16.7M$$\n",
    "$$\\text{LoRA: } r \\times d + d \\times r = 2 \\times r \\times d = 2 \\times 8 \\times 4096 = 65K$$\n",
    "\n",
    "**Reduction: 257\u00d7 fewer parameters!**\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "$$h = W_{frozen} \\cdot x + \\frac{\\alpha}{r} \\cdot B \\cdot A \\cdot x$$\n",
    "\n",
    "```\n",
    "Input x \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "             \u2502                                \u2502\n",
    "             \u25bc                                \u25bc\n",
    "      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "      \u2502  W (frozen) \u2502                  \u2502    A        \u2502\n",
    "      \u2502  d \u00d7 d      \u2502                  \u2502    r \u00d7 d    \u2502\n",
    "      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502                                \u2502\n",
    "             \u2502                                \u25bc\n",
    "             \u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "             \u2502                         \u2502    B        \u2502\n",
    "             \u2502                         \u2502    d \u00d7 r    \u2502\n",
    "             \u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502                                \u2502\n",
    "             \u2502                                \u25bc\n",
    "             \u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "             \u2502                         \u2502   \u00d7 \u03b1/r     \u2502\n",
    "             \u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502                                \u2502\n",
    "             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u2502\n",
    "                           \u25bc\n",
    "                       Output h\n",
    "```\n",
    "\n",
    "### Initialization\n",
    "\n",
    "- $A$ ~ $\\mathcal{N}(0, \\sigma^2)$ (Gaussian)\n",
    "- $B = 0$ (zero matrix)\n",
    "\n",
    "This ensures $\\Delta W = B \\cdot A = 0$ at start, so the model begins identical to pretrained.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 QLoRA: Quantized Low-Rank Adaptation\n",
    "\n",
    "### The Problem\n",
    "\n",
    "LoRA still requires the base model in memory:\n",
    "\n",
    "$$\\text{7B model in FP16} = 7 \\times 10^9 \\times 2 \\text{ bytes} = 14 \\text{ GB}$$\n",
    "\n",
    "**Doesn't fit on consumer GPUs!**\n",
    "\n",
    "### The QLoRA Solution\n",
    "\n",
    "Store base model in 4-bit precision:\n",
    "\n",
    "$$\\text{7B model in NF4} = 7 \\times 10^9 \\times 0.5 \\text{ bytes} = 3.5 \\text{ GB}$$\n",
    "\n",
    "### What is Quantization?\n",
    "\n",
    "Map continuous values to discrete levels:\n",
    "\n",
    "$$Q(w) = \\text{round}\\left(\\frac{w - \\text{min}}{\\text{max} - \\text{min}} \\times (2^b - 1)\\right)$$\n",
    "\n",
    "For 4-bit: $2^4 = 16$ possible values.\n",
    "\n",
    "### NF4: Normal Float 4-bit\n",
    "\n",
    "Neural network weights follow approximately normal distribution:\n",
    "\n",
    "$$w \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "NF4 places quantization levels at **quantiles** of the normal distribution:\n",
    "\n",
    "$$q_i = \\Phi^{-1}\\left(\\frac{i + 0.5}{16}\\right)$$\n",
    "\n",
    "Where $\\Phi^{-1}$ is the inverse CDF of standard normal.\n",
    "\n",
    "```\n",
    "Standard 4-bit (uniform):        NF4 (normal-optimized):\n",
    "\n",
    "\u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502    \u2502\u2502\u2502\u2502\u2502  \u2502  \u2502  \u2502  \u2502  \u2502\u2502\u2502\u2502\u2502\n",
    "\u2514\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2518    \u2514\u2534\u2534\u2534\u2534\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2534\u2534\u2534\u2534\u2518\n",
    "-8      0       +8                 -3\u03c3  -\u03c3   0   +\u03c3  +3\u03c3\n",
    "\n",
    "Uniform spacing                   More levels near 0 (where most weights are)\n",
    "```\n",
    "\n",
    "### Block-wise Quantization\n",
    "\n",
    "Weights are quantized in blocks of 64:\n",
    "\n",
    "$$W_{quantized}[i] = Q\\left(\\frac{W[i]}{s_b}\\right)$$\n",
    "\n",
    "Where $s_b$ is the scale factor for block $b$.\n",
    "\n",
    "### Double Quantization\n",
    "\n",
    "QLoRA also quantizes the scale factors:\n",
    "\n",
    "```\n",
    "Level 1: Weights \u2192 4-bit (NF4)\n",
    "Level 2: Scale factors \u2192 8-bit (FP8)\n",
    "```\n",
    "\n",
    "### Forward Pass in QLoRA\n",
    "\n",
    "$$h = \\text{Dequant}(W_{4bit}) \\cdot x + \\frac{\\alpha}{r} \\cdot B \\cdot A \\cdot x$$\n",
    "\n",
    "```\n",
    "Input x \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "             \u2502                                \u2502\n",
    "             \u25bc                                \u25bc\n",
    "      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "      \u2502 W (4-bit)   \u2502                  \u2502    A (FP16) \u2502\n",
    "      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502                                \u2502\n",
    "             \u25bc                                \u25bc\n",
    "      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "      \u2502 Dequantize  \u2502                  \u2502    B (FP16) \u2502\n",
    "      \u2502 to FP16     \u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n",
    "             \u2502                                \u2502\n",
    "             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u2502\n",
    "                           \u25bc\n",
    "                       Output h\n",
    "```\n",
    "\n",
    "### Memory Breakdown\n",
    "\n",
    "| Component | LoRA (FP16) | QLoRA (NF4) |\n",
    "|-----------|-------------|-------------|\n",
    "| Base model (7B) | 14.0 GB | 3.5 GB |\n",
    "| LoRA adapters | 0.05 GB | 0.05 GB |\n",
    "| Optimizer states | 0.1 GB | 0.1 GB |\n",
    "| Activations | ~2 GB | ~2 GB |\n",
    "| **Total** | **~16 GB** | **~6 GB** |\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 DoRA: Weight-Decomposed Low-Rank Adaptation\n",
    "\n",
    "### The Problem with LoRA\n",
    "\n",
    "LoRA updates both **magnitude** and **direction** together:\n",
    "\n",
    "$$W' = W + B \\cdot A$$\n",
    "\n",
    "But full fine-tuning can change them **independently**. This coupling limits LoRA's expressiveness.\n",
    "\n",
    "### Vector Decomposition\n",
    "\n",
    "Any vector can be written as:\n",
    "\n",
    "$$\\vec{v} = \\|\\vec{v}\\| \\cdot \\frac{\\vec{v}}{\\|\\vec{v}\\|}$$\n",
    "\n",
    "$$\\vec{v} = \\underbrace{m}_{\\text{magnitude}} \\cdot \\underbrace{\\hat{d}}_{\\text{direction}}$$\n",
    "\n",
    "### DoRA's Decomposition\n",
    "\n",
    "Apply this to each **column** of the weight matrix:\n",
    "\n",
    "$$W = M \\cdot D$$\n",
    "\n",
    "Where:\n",
    "- $M = \\text{diag}(\\|W_{:,1}\\|, \\|W_{:,2}\\|, ..., \\|W_{:,d}\\|)$ \u2014 magnitudes\n",
    "- $D = W / \\|W\\|_{col}$ \u2014 directions (column-normalized)\n",
    "\n",
    "### DoRA Update Rule\n",
    "\n",
    "$$W' = m \\cdot \\frac{V + B \\cdot A}{\\|V + B \\cdot A\\|_c}$$\n",
    "\n",
    "Where:\n",
    "- $m \\in \\mathbb{R}^{d}$ \u2014 **learnable** magnitude vector\n",
    "- $V = W / \\|W\\|_c$ \u2014 original direction (frozen)\n",
    "- $B \\cdot A$ \u2014 LoRA update for direction\n",
    "- $\\|\\cdot\\|_c$ \u2014 column-wise normalization\n",
    "\n",
    "### Visualization\n",
    "\n",
    "```\n",
    "LoRA:                              DoRA:\n",
    "\n",
    "    Original W                         Original W\n",
    "        \u2502                                  \u2502\n",
    "        \u25bc                                  \u25bc\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502 + BA  \u2502                      \u2502 Decompose    \u2502\n",
    "    \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518                      \u2502 m = ||W||    \u2502\n",
    "        \u2502                          \u2502 V = W/||W||  \u2502\n",
    "        \u25bc                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "    W' = W + BA                           \u2502\n",
    "                                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    (magnitude &                   \u2502              \u2502\n",
    "     direction                     \u25bc              \u25bc\n",
    "     coupled)                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                               \u2502 m     \u2502    \u2502 V + BA   \u2502\n",
    "                               \u2502(learn)\u2502    \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2502\n",
    "                               \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518    \u2502 ||V+BA|| \u2502\n",
    "                                   \u2502        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                   \u2502             \u2502\n",
    "                                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                          \u2502\n",
    "                                          \u25bc\n",
    "                                   W' = m \u00b7 normalized\n",
    "\n",
    "                                   (magnitude & direction\n",
    "                                    independent!)\n",
    "```\n",
    "\n",
    "### Why This Helps\n",
    "\n",
    "**Scenario:** Fine-tune to make feature #1 stronger, feature #2 weaker.\n",
    "\n",
    "| Method | How it achieves this |\n",
    "|--------|---------------------|\n",
    "| LoRA | BA must encode magnitude changes as direction shifts (inefficient) |\n",
    "| DoRA | $m_1 = 1.5$, $m_2 = 0.5$ \u2014 direct! BA focuses on actual direction changes |\n",
    "\n",
    "### Parameter Count\n",
    "\n",
    "$$\\text{DoRA} = \\text{LoRA} + d_{out}$$\n",
    "\n",
    "For $d = 4096$, $r = 8$:\n",
    "- LoRA: $2 \\times 8 \\times 4096 = 65,536$\n",
    "- DoRA: $65,536 + 4,096 = 69,632$\n",
    "\n",
    "**Only 6% more parameters!**\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Summary: The Three Methods\n",
    "\n",
    "| | LoRA | QLoRA | DoRA |\n",
    "|--|------|-------|------|\n",
    "| **Update formula** | $W + BA$ | $\\text{Dequant}(W_{4b}) + BA$ | $m \\cdot \\frac{V + BA}{\\|V + BA\\|}$ |\n",
    "| **Base model** | FP16 | NF4 (4-bit) | FP16 |\n",
    "| **Adapters** | FP16 | FP16 | FP16 + magnitude |\n",
    "| **Magnitude/Direction** | Coupled | Coupled | Decoupled |\n",
    "| **Memory (7B)** | ~14 GB | ~4 GB | ~14 GB |\n",
    "| **Quality** | Baseline | ~99% | ~101% |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "```\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502      Do you have enough VRAM?   \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                    \u2502\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502                               \u2502\n",
    "                    \u25bc                               \u25bc\n",
    "               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "               \u2502   NO   \u2502                     \u2502  YES   \u2502\n",
    "               \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n",
    "                    \u2502                              \u2502\n",
    "                    \u25bc                              \u25bc\n",
    "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "              \u2502  QLoRA   \u2502              \u2502 Need best quality?  \u2502\n",
    "              \u2502  (4-bit) \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n",
    "                                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                                       \u2502                       \u2502\n",
    "                                       \u25bc                       \u25bc\n",
    "                                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                                  \u2502  YES   \u2502              \u2502   NO   \u2502\n",
    "                                  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n",
    "                                       \u2502                       \u2502\n",
    "                                       \u25bc                       \u25bc\n",
    "                                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                                 \u2502   DoRA   \u2502            \u2502   LoRA   \u2502\n",
    "                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: Experimental Comparison\n",
    "\n",
    "Now let's run all three methods and compare!\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers datasets peft trl accelerate bitsandbytes"
   ],
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\u2705 GPU: {gpu_name}\")\n",
    "    print(f\"\u2705 VRAM: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"\u274c No GPU!\")"
   ],
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 GPU: Tesla T4\n",
      "\u2705 VRAM: 15.8 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATASET_ID = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# Training settings (same for all methods)\n",
    "MAX_SEQ_LEN = 512\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM = 8\n",
    "MAX_STEPS = 100  # Short runs for comparison\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# LoRA settings (same for all methods)\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "print(\"\u2705 Configuration loaded\")"
   ],
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Configuration loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset once\n",
    "dataset = load_dataset(DATASET_ID, split=\"train\")\n",
    "print(f\"\u2705 Dataset: {len(dataset)} samples\")\n",
    "\n",
    "# Load tokenizer once\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"\u2705 Tokenizer loaded\")"
   ],
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001-9ad84bb9cf65a4(\u2026):   0%|          | 0.00/967k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Dataset: 1000 samples\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Tokenizer loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1e9\n",
    "    return 0\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable and total parameters.\"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "print(\"\u2705 Helper functions defined\")"
   ],
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Helper functions defined\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 2.4 Method 1: Standard LoRA\n",
    "\n",
    "$$W' = W_{frozen} + \\frac{\\alpha}{r} \\cdot B \\cdot A$$\n",
    "\n",
    "Base model in **FP16**, adapters in **FP16**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udd35 METHOD 1: Standard LoRA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clear_memory()\n",
    "mem_before = get_gpu_memory()\n",
    "\n",
    "# Load model in FP16\n",
    "model_lora = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "mem_after_load = get_gpu_memory()\n",
    "print(f\"\\n\ud83d\udcca Memory after loading base model: {mem_after_load:.2f} GB\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    use_dora=False,  # Standard LoRA\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "\n",
    "mem_after_lora = get_gpu_memory()\n",
    "trainable, total = count_parameters(model_lora)\n",
    "\n",
    "print(f\"\ud83d\udcca Memory after LoRA: {mem_after_lora:.2f} GB\")\n",
    "print(f\"\ud83d\udcca Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"\ud83d\udcca Total params: {total:,}\")"
   ],
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "\ud83d\udd35 METHOD 1: Standard LoRA\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udcca Memory after loading base model: 1.00 GB\n",
      "\ud83d\udcca Memory after LoRA: 1.00 GB\n",
      "\ud83d\udcca Trainable params: 1,081,344 (0.22%)\n",
      "\ud83d\udcca Total params: 495,114,112\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Train LoRA\n",
    "print(\"\\n\ud83d\ude80 Training LoRA...\")\n",
    "\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"./outputs/lora\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_lora = SFTTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    # max_seq_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer_lora.train()\n",
    "lora_time = time.time() - start_time\n",
    "\n",
    "lora_loss = trainer_lora.state.log_history[-1].get('train_loss', 'N/A')\n",
    "print(f\"\\n\u2705 LoRA complete!\")\n",
    "print(f\"   Time: {lora_time:.1f}s\")\n",
    "print(f\"   Final loss: {lora_loss}\")"
   ],
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\ude80 Training LoRA...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of \ud83e\udd17 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:25, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.838700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.678800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 LoRA complete!\n",
      "   Time: 279.2s\n",
      "   Final loss: 1.7227166748046876\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Store results and clean up\n",
    "lora_results = {\n",
    "    'method': 'LoRA',\n",
    "    'memory_gb': mem_after_lora,\n",
    "    'trainable_params': trainable,\n",
    "    'time_seconds': lora_time,\n",
    "    'final_loss': lora_loss,\n",
    "}\n",
    "\n",
    "del model_lora, trainer_lora\n",
    "clear_memory()\n",
    "print(\"\ud83e\uddf9 Memory cleared\")"
   ],
   "metadata": {},
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddf9 Memory cleared\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 2.5 Method 2: QLoRA\n",
    "\n",
    "$$W' = \\text{Dequant}(W_{4bit}) + \\frac{\\alpha}{r} \\cdot B \\cdot A$$\n",
    "\n",
    "Base model in **NF4 (4-bit)**, adapters in **FP16**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udfe2 METHOD 2: QLoRA (4-bit)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clear_memory()\n",
    "mem_before = get_gpu_memory()\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # Enable 4-bit\n",
    "    bnb_4bit_quant_type=\"nf4\",      # Use NF4 (normal float)\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16\n",
    "    bnb_4bit_use_double_quant=True, # Double quantization\n",
    ")\n",
    "\n",
    "# Load model in 4-bit\n",
    "model_qlora = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "mem_after_load = get_gpu_memory()\n",
    "print(f\"\\n\ud83d\udcca Memory after loading 4-bit model: {mem_after_load:.2f} GB\")\n",
    "\n",
    "# Prepare for k-bit training\n",
    "model_qlora = prepare_model_for_kbit_training(model_qlora)\n",
    "\n",
    "# Apply LoRA\n",
    "qlora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    use_dora=False,\n",
    ")\n",
    "\n",
    "model_qlora = get_peft_model(model_qlora, qlora_config)\n",
    "\n",
    "mem_after_qlora = get_gpu_memory()\n",
    "trainable, total = count_parameters(model_qlora)\n",
    "\n",
    "print(f\"\ud83d\udcca Memory after QLoRA: {mem_after_qlora:.2f} GB\")\n",
    "print(f\"\ud83d\udcca Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"\ud83d\udcca Total params: {total:,}\")"
   ],
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "\ud83d\udfe2 METHOD 2: QLoRA (4-bit)\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcca Memory after loading 4-bit model: 0.48 GB\n",
      "\ud83d\udcca Memory after QLoRA: 0.75 GB\n",
      "\ud83d\udcca Trainable params: 1,081,344 (0.34%)\n",
      "\ud83d\udcca Total params: 316,200,832\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Train QLoRA\n",
    "print(\"\\n\ud83d\ude80 Training QLoRA...\")\n",
    "\n",
    "training_args_qlora = TrainingArguments(\n",
    "    output_dir=\"./outputs/qlora\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True, # fp16=True -> gies error\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_qlora = SFTTrainer(\n",
    "    model=model_qlora,\n",
    "    args=training_args_qlora,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    # max_seq_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer_qlora.train()\n",
    "qlora_time = time.time() - start_time\n",
    "\n",
    "qlora_loss = trainer_qlora.state.log_history[-1].get('train_loss', 'N/A')\n",
    "print(f\"\\n\u2705 QLoRA complete!\")\n",
    "print(f\"   Time: {qlora_time:.1f}s\")\n",
    "print(f\"   Final loss: {qlora_loss}\")"
   ],
   "metadata": {},
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\ude80 Training QLoRA...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of \ud83e\udd17 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:44, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.976400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.835200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 QLoRA complete!\n",
      "   Time: 413.6s\n",
      "   Final loss: 1.877843017578125\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Store results and clean up\n",
    "qlora_results = {\n",
    "    'method': 'QLoRA',\n",
    "    'memory_gb': mem_after_qlora,\n",
    "    'trainable_params': trainable,\n",
    "    'time_seconds': qlora_time,\n",
    "    'final_loss': qlora_loss,\n",
    "}\n",
    "\n",
    "del model_qlora, trainer_qlora\n",
    "clear_memory()\n",
    "print(\"\ud83e\uddf9 Memory cleared\")"
   ],
   "metadata": {},
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddf9 Memory cleared\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 2.6 Method 3: DoRA\n",
    "\n",
    "$$W' = m \\cdot \\frac{V + B \\cdot A}{\\|V + B \\cdot A\\|_c}$$\n",
    "\n",
    "Base model in **FP16**, adapters in **FP16**, plus **magnitude vector**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udfe3 METHOD 3: DoRA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clear_memory()\n",
    "mem_before = get_gpu_memory()\n",
    "\n",
    "# Load model in FP16\n",
    "model_dora = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "mem_after_load = get_gpu_memory()\n",
    "print(f\"\\n\ud83d\udcca Memory after loading base model: {mem_after_load:.2f} GB\")\n",
    "\n",
    "# Apply DoRA (just flip the flag!)\n",
    "dora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    use_dora=True,  # \u2190 Enable DoRA!\n",
    ")\n",
    "\n",
    "model_dora = get_peft_model(model_dora, dora_config)\n",
    "\n",
    "mem_after_dora = get_gpu_memory()\n",
    "trainable, total = count_parameters(model_dora)\n",
    "\n",
    "print(f\"\ud83d\udcca Memory after DoRA: {mem_after_dora:.2f} GB\")\n",
    "print(f\"\ud83d\udcca Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"\ud83d\udcca Total params: {total:,}\")"
   ],
   "metadata": {},
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "\ud83d\udfe3 METHOD 3: DoRA\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcca Memory after loading base model: 1.75 GB\n",
      "\ud83d\udcca Memory after DoRA: 1.76 GB\n",
      "\ud83d\udcca Trainable params: 1,130,496 (0.23%)\n",
      "\ud83d\udcca Total params: 495,163,264\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Train DoRA\n",
    "print(\"\\n\ud83d\ude80 Training DoRA...\")\n",
    "\n",
    "training_args_dora = TrainingArguments(\n",
    "    output_dir=\"./outputs/dora\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer_dora = SFTTrainer(\n",
    "    model=model_dora,\n",
    "    args=training_args_dora,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    # max_seq_length=MAX_SEQ_LEN,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer_dora.train()\n",
    "dora_time = time.time() - start_time\n",
    "\n",
    "dora_loss = trainer_dora.state.log_history[-1].get('train_loss', 'N/A')\n",
    "print(f\"\\n\u2705 DoRA complete!\")\n",
    "print(f\"   Time: {dora_time:.1f}s\")\n",
    "print(f\"   Final loss: {dora_loss}\")"
   ],
   "metadata": {},
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\ude80 Training DoRA...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of \ud83e\udd17 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:47, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.837100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.677200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 DoRA complete!\n",
      "   Time: 293.7s\n",
      "   Final loss: 1.7216070556640626\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Store results and clean up\n",
    "dora_results = {\n",
    "    'method': 'DoRA',\n",
    "    'memory_gb': mem_after_dora,\n",
    "    'trainable_params': trainable,\n",
    "    'time_seconds': dora_time,\n",
    "    'final_loss': dora_loss,\n",
    "}\n",
    "\n",
    "del model_dora, trainer_dora\n",
    "clear_memory()\n",
    "print(\"\ud83e\uddf9 Memory cleared\")"
   ],
   "metadata": {},
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddf9 Memory cleared\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Part 3: Results Comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udcca COMPARISON RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = [lora_results, qlora_results, dora_results]\n",
    "\n",
    "print(f\"\\n{'Method':<10} {'Memory (GB)':<15} {'Trainable':<15} {'Time (s)':<12} {'Loss':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['method']:<10} {r['memory_gb']:<15.2f} {r['trainable_params']:<15,} {r['time_seconds']:<12.1f} {r['final_loss']:<10}\")"
   ],
   "metadata": {},
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "\ud83d\udcca COMPARISON RESULTS\n",
      "======================================================================\n",
      "\n",
      "Method     Memory (GB)     Trainable       Time (s)     Loss      \n",
      "----------------------------------------------------------------------\n",
      "LoRA       1.00            1,081,344       279.2        1.7227166748046876\n",
      "QLoRA      0.75            1,081,344       413.6        1.877843017578125\n",
      "DoRA       1.76            1,130,496       293.7        1.7216070556640626\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Visual comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udcca VISUAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Memory comparison\n",
    "print(\"\\n\ud83e\udde0 Memory Usage:\")\n",
    "max_mem = max(r['memory_gb'] for r in results)\n",
    "for r in results:\n",
    "    bar_len = int(30 * r['memory_gb'] / max_mem)\n",
    "    bar = '\u2588' * bar_len + '\u2591' * (30 - bar_len)\n",
    "    print(f\"  {r['method']:<8} [{bar}] {r['memory_gb']:.2f} GB\")\n",
    "\n",
    "# Time comparison\n",
    "print(\"\\n\u23f1\ufe0f Training Time:\")\n",
    "max_time = max(r['time_seconds'] for r in results)\n",
    "for r in results:\n",
    "    bar_len = int(30 * r['time_seconds'] / max_time)\n",
    "    bar = '\u2588' * bar_len + '\u2591' * (30 - bar_len)\n",
    "    print(f\"  {r['method']:<8} [{bar}] {r['time_seconds']:.1f}s\")\n",
    "\n",
    "# Parameters comparison\n",
    "print(\"\\n\ud83d\udce6 Trainable Parameters:\")\n",
    "max_params = max(r['trainable_params'] for r in results)\n",
    "for r in results:\n",
    "    bar_len = int(30 * r['trainable_params'] / max_params)\n",
    "    bar = '\u2588' * bar_len + '\u2591' * (30 - bar_len)\n",
    "    print(f\"  {r['method']:<8} [{bar}] {r['trainable_params']:,}\")"
   ],
   "metadata": {},
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "\ud83d\udcca VISUAL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "\ud83e\udde0 Memory Usage:\n",
      "  LoRA     [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 1.00 GB\n",
      "  QLoRA    [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 0.75 GB\n",
      "  DoRA     [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588] 1.76 GB\n",
      "\n",
      "\u23f1\ufe0f Training Time:\n",
      "  LoRA     [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 279.2s\n",
      "  QLoRA    [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588] 413.6s\n",
      "  DoRA     [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 293.7s\n",
      "\n",
      "\ud83d\udce6 Trainable Parameters:\n",
      "  LoRA     [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591] 1,081,344\n",
      "  QLoRA    [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591] 1,081,344\n",
      "  DoRA     [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588] 1,130,496\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "\n",
    "QLoRA's 4-bit quantization saves ~25% memory.\n",
    "DoRA being higher is expected \u2014 it stores additional magnitude vectors.\n",
    "\n",
    "QLoRA is slower \u2014 this is expected!\n",
    "The 4-bit weights must be dequantized on every forward pass:\n",
    "\n",
    "\n",
    "    QLoRA Forward Pass:\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502 W (4-bit)   \u2502 \u2500\u2500\u25ba \u2502 Dequantize  \u2502 \u2500\u2500\u25ba \u2502 Compute     \u2502\n",
    "    \u2502 in memory   \u2502     \u2502 to FP16     \u2502     \u2502 W \u00d7 x       \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                              \u2191\n",
    "                        Extra overhead!\n",
    "      \n",
    "Trade-off: QLoRA saves memory but costs time.\n",
    "\n",
    "    LoRA:  1,081,344\n",
    "    QLoRA: 1,081,344  \u2190 Same (just different base precision)\n",
    "    DoRA:  1,130,496  \u2190 +49,152 extra (magnitude vectors)\n",
    "\n",
    "DoRA's extra params:\n",
    "\n",
    "    Extra = 1,130,496 - 1,081,344 = 49,152\n",
    "\n",
    "This is the magnitude vector m for each target layer.\n",
    "\n",
    "    For Qwen 0.5B with QKVO targets:\n",
    "      24 layers \u00d7 4 modules \u00d7 512 dim \u2248 49K \u2713\n",
    "\n",
    "  \n",
    "\n",
    "\u26a0\ufe0f Important Note:\n",
    "\n",
    "These results are from 100 training steps on a small dataset.\n",
    "\n",
    "With longer training:\n",
    "- DoRA's advantage typically grows\n",
    "- QLoRA's gap may narrow (or widen)\n",
    "\n",
    "The relative rankings are meaningful, but exact numbers\n",
    "will vary with more training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Part 4: Key Takeaways\n",
    "\n",
    "## Mathematical Summary\n",
    "\n",
    "| Method | Formula | Key Innovation |\n",
    "|--------|---------|----------------|\n",
    "| **LoRA** | $W' = W + BA$ | Low-rank factorization |\n",
    "| **QLoRA** | $W' = \\text{Dequant}(W_{4b}) + BA$ | 4-bit base + FP16 adapters |\n",
    "| **DoRA** | $W' = m \\cdot \\frac{V+BA}{\\|V+BA\\|}$ | Magnitude/direction split |\n",
    "\n",
    "## When to Use Each\n",
    "\n",
    "| Situation | Recommendation |\n",
    "|-----------|----------------|\n",
    "| Plenty of VRAM, quick experiments | **LoRA** |\n",
    "| Limited VRAM (consumer GPU) | **QLoRA** |\n",
    "| Need best quality, have VRAM | **DoRA** |\n",
    "| Production with memory constraints | **QLoRA** |\n",
    "\n",
    "## Code Summary\n",
    "\n",
    "```python\n",
    "# LoRA\n",
    "LoraConfig(r=8, use_dora=False)\n",
    "\n",
    "# QLoRA  \n",
    "BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\")\n",
    "LoraConfig(r=8, use_dora=False)\n",
    "\n",
    "# DoRA\n",
    "LoraConfig(r=8, use_dora=True)  # Just flip the flag!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **LoRA**: [Hu et al., 2021](https://arxiv.org/abs/2106.09685) - \"LoRA: Low-Rank Adaptation of Large Language Models\"\n",
    "- **QLoRA**: [Dettmers et al., 2023](https://arxiv.org/abs/2305.14314) - \"QLoRA: Efficient Finetuning of Quantized LLMs\"\n",
    "- **DoRA**: [Liu et al., 2024](https://arxiv.org/abs/2402.09353) - \"DoRA: Weight-Decomposed Low-Rank Adaptation\""
   ],
   "metadata": {}
  }
 ]
}