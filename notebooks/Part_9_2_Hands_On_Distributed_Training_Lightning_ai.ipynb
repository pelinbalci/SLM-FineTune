{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9.2: Hands-On Distributed Training with Lightning.ai\n",
    "\n",
    "## From Theory to Practice\n",
    "\n",
    "In Part 9.1, we learned the concepts behind distributed training. Now we'll put them into practice using Lightning.ai's dual GPU environment.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Set up Lightning.ai Studio with 2 GPUs\n",
    "2. Configure HuggingFace Accelerate for distributed training\n",
    "3. Run distributed fine-tuning with DeepSpeed ZeRO\n",
    "4. Compare single GPU vs multi-GPU performance\n",
    "5. Understand practical debugging and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setting Up Lightning.ai\n",
    "\n",
    "### 1.1 Create a Studio\n",
    "\n",
    "1. Go to [lightning.ai](https://lightning.ai)\n",
    "2. Create a new Studio\n",
    "3. Select **2x GPU** configuration (L4, L40S, or A10G recommended)\n",
    "4. Choose a PyTorch template or start fresh\n",
    "\n",
    "### 1.2 Verify GPU Setup\n",
    "\n",
    "Once your Studio is running, verify you have 2 GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 24 13:30:26 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   45C    P8             16W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   42C    P8             16W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Run this in your Lightning.ai Studio\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see two GPUs listed. Note the memory available on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "\n",
      "GPU 0: NVIDIA L4\n",
      "  Memory: 22.3 GB\n",
      "  Compute Capability: 8.9\n",
      "\n",
      "GPU 1: NVIDIA L4\n",
      "  Memory: 22.3 GB\n",
      "  Compute Capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"\\nGPU {i}: {props.name}\")\n",
    "    print(f\"  Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"  Compute Capability: {props.major}.{props.minor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages for distributed training\n",
    "!pip install accelerate transformers datasets peft bitsandbytes -q\n",
    "!pip install deepspeed -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate version: 1.12.0\n",
      "Transformers version: 4.57.6\n",
      "Datasets version: 4.5.0\n",
      "PEFT version: 0.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /teamspace/studios/this_studio/.triton/autotune: No such file or directory\n",
      "/home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeed version: 0.18.4\n"
     ]
    }
   ],
   "source": [
    "# Verify installations\n",
    "import accelerate\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"PEFT version: {peft.__version__}\")\n",
    "\n",
    "# Check DeepSpeed\n",
    "try:\n",
    "    import deepspeed\n",
    "    print(f\"DeepSpeed version: {deepspeed.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"DeepSpeed not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Configure Accelerate for Multi-GPU\n",
    "\n",
    "Accelerate needs configuration to know how to distribute training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Method 1: Configuration File\n",
    "\n",
    "Create a configuration file for Accelerate. We'll create configs for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDP Config:\n",
      "compute_environment: LOCAL_MACHINE\n",
      "distributed_type: MULTI_GPU\n",
      "main_training_function: main\n",
      "mixed_precision: bf16\n",
      "num_machines: 1\n",
      "num_processes: 2\n",
      "use_cpu: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Create config directory\n",
    "os.makedirs(\"configs\", exist_ok=True)\n",
    "\n",
    "# Config 1: Simple Multi-GPU (DDP)\n",
    "ddp_config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"distributed_type\": \"MULTI_GPU\",\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 2,  # Number of GPUs\n",
    "    \"use_cpu\": False,\n",
    "    \"main_training_function\": \"main\",\n",
    "}\n",
    "\n",
    "with open(\"configs/ddp_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(ddp_config, f)\n",
    "\n",
    "print(\"DDP Config:\")\n",
    "print(yaml.dump(ddp_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeed ZeRO-2 Config created!\n"
     ]
    }
   ],
   "source": [
    "# Config 2: DeepSpeed ZeRO-2\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "deepspeed_zero2_config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"distributed_type\": \"DEEPSPEED\",\n",
    "    \"deepspeed_config\": {\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 2,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"none\"  # Set to \"cpu\" for CPU offloading\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"none\"\n",
    "            },\n",
    "            \"allgather_partitions\": True,\n",
    "            \"allgather_bucket_size\": 2e8,\n",
    "            \"reduce_scatter\": True,\n",
    "            \"reduce_bucket_size\": 2e8,\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"gradient_clipping\": 1.0,           \n",
    "        \"train_batch_size\": 32,\n",
    "        \"train_micro_batch_size_per_gpu\": 4,\n",
    "        \"bf16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 2,\n",
    "    \"use_cpu\": False,\n",
    "}\n",
    "\n",
    "with open(\"configs/deepspeed_zero2_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(deepspeed_zero2_config, f)\n",
    "\n",
    "print(\"DeepSpeed ZeRO-2 Config created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeed ZeRO-3 Config created!\n"
     ]
    }
   ],
   "source": [
    "# Config 3: DeepSpeed ZeRO-3 (for larger models)\n",
    "deepspeed_zero3_config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"distributed_type\": \"DEEPSPEED\",\n",
    "    \"deepspeed_config\": {\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"sub_group_size\": 1e9,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"stage3_max_live_parameters\": 1e9,\n",
    "            \"stage3_max_reuse_distance\": 1e9,\n",
    "            \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": 4, \n",
    "        \"gradient_clipping\": 1.0,           \n",
    "        \"train_batch_size\": 32,\n",
    "        \"train_micro_batch_size_per_gpu\": 4,\n",
    "        \"bf16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 2,\n",
    "    \"use_cpu\": False,\n",
    "}\n",
    "\n",
    "with open(\"configs/deepspeed_zero3_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(deepspeed_zero3_config, f)\n",
    "\n",
    "print(\"DeepSpeed ZeRO-3 Config created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding the Configs\n",
    "\n",
    "| Config | Use Case | Memory Savings | Speed |\n",
    "|--------|----------|----------------|-------|\n",
    "| **DDP** | Model fits on GPU, want 2x speed | None | Fastest |\n",
    "| **ZeRO-2** | Need more memory for batches/model | ~8x | Fast |\n",
    "| **ZeRO-3** | Model doesn't fit on single GPU | ~N× | Slower |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Prepare Dataset and Model\n",
    "\n",
    "We'll use a small model and dataset to demonstrate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: Qwen/Qwen2.5-0.5B\n",
      "Vocab size: 151643\n",
      "Pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Use a small model for demonstration\n",
    "# Options: \"Qwen/Qwen2.5-0.5B\", \"microsoft/phi-2\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2000\n",
      "\n",
      "Example:\n",
      "{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'input': '', 'instruction': 'Give three tips for staying healthy.'}\n"
     ]
    }
   ],
   "source": [
    "# Load a small instruction dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:2000]\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted example:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 \n"
     ]
    }
   ],
   "source": [
    "# Format dataset for instruction tuning\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format as instruction-response pair.\"\"\"\n",
    "    if example.get(\"input\", \"\").strip():\n",
    "        text = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "\n",
    "print(\"Formatted example:\")\n",
    "print(formatted_dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset columns: ['input_ids', 'attention_mask', 'labels']\n",
      "Example input_ids length: 512\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None  # Return lists for dataset\n",
    "    )\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Add labels (same as input_ids for causal LM)\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
    "\n",
    "print(f\"Tokenized dataset columns: {tokenized_dataset.column_names}\")\n",
    "print(f\"Example input_ids length: {len(tokenized_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Create Training Script\n",
    "\n",
    "For distributed training, we need a standalone script that can be launched across GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Run Distributed Training\n",
    "\n",
    "Now let's launch training across both GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Option A: Using DDP (Data Distributed Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISTRIBUTED TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Number of GPUs: 2\n",
      "Distributed type: DistributedType.MULTI_GPU\n",
      "Mixed precision: bf16\n",
      "Model: Qwen/Qwen2.5-0.5B\n",
      "Batch size per GPU: 4\n",
      "Gradient accumulation: 4\n",
      "Effective batch size: 32\n",
      "============================================================\n",
      "\n",
      "Loading dataset...\n",
      "Train samples: 1800\n",
      "Eval samples: 200\n",
      "\n",
      "Loading model...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "Applying LoRA...\n",
      "Trainable parameters: 2,162,688 (0.44%)\n",
      "/teamspace/studios/this_studio/train_distributed.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/teamspace/studios/this_studio/train_distributed.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "{'loss': 1.4399, 'grad_norm': 0.489316463470459, 'learning_rate': 0.000192112365311485, 'epoch': 0.18}\n",
      "{'loss': 1.4278, 'grad_norm': 0.4761621057987213, 'learning_rate': 0.00015644432188667695, 'epoch': 0.36}\n",
      "{'loss': 1.3704, 'grad_norm': 0.3827182352542877, 'learning_rate': 0.00010285560507936961, 'epoch': 0.53}\n",
      "{'loss': 1.3852, 'grad_norm': 0.40681394934654236, 'learning_rate': 4.836025383610382e-05, 'epoch': 0.71}\n",
      "{'loss': 1.367, 'grad_norm': 0.36284035444259644, 'learning_rate': 1.026015713086418e-05, 'epoch': 0.89}\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:49<00:06,  1.01it/s]\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001B[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:01, 18.28it/s]\u001B[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:01, 11.67it/s]\u001B[A\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:01, 10.47it/s]\u001B[A\n",
      " 32%|██████████████                              | 8/25 [00:00<00:01, 10.00it/s]\u001B[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:00<00:01,  9.75it/s]\u001B[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  9.64it/s]\u001B[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  9.60it/s]\u001B[A\n",
      " 56%|████████████████████████                   | 14/25 [00:01<00:01,  9.56it/s]\u001B[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:01<00:01,  9.55it/s]\u001B[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:01<00:00,  9.51it/s]\u001B[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:01<00:00,  9.48it/s]\u001B[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:01<00:00,  9.44it/s]\u001B[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:01<00:00,  9.40it/s]\u001B[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  9.40it/s]\u001B[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:02<00:00,  9.39it/s]\u001B[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:02<00:00,  9.39it/s]\u001B[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:02<00:00,  9.37it/s]\u001B[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:02<00:00,  9.39it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 1.3416900634765625, 'eval_runtime': 2.6781, 'eval_samples_per_second': 74.68, 'eval_steps_per_second': 9.335, 'epoch': 0.89}\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:52<00:06,  1.01it/s]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:02<00:00,  9.52it/s]\u001B[A\n",
      "{'train_runtime': 59.2691, 'train_samples_per_second': 30.37, 'train_steps_per_second': 0.962, 'train_loss': 1.4306209631133497, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 57/57 [00:59<00:00,  1.04s/it]\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Total training time: 59.87 seconds (1.00 minutes)\n",
      "\n",
      "Saving model...\n",
      "Model saved to: ./outputs/distributed_finetuned_ddp\n"
     ]
    }
   ],
   "source": [
    "# Launch with DDP (simple multi-GPU)\n",
    "!accelerate launch --config_file configs/ddp_config.yaml train_distributed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Option B: Using DeepSpeed ZeRO-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0124 13:53:07.545000 75133 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] \n",
      "W0124 13:53:07.545000 75133 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] *****************************************\n",
      "W0124 13:53:07.545000 75133 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0124 13:53:07.545000 75133 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] *****************************************\n",
      "============================================================\n",
      "DISTRIBUTED TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Number of GPUs: 2\n",
      "Distributed type: DistributedType.DEEPSPEED\n",
      "Mixed precision: bf16\n",
      "Model: Qwen/Qwen2.5-0.5B\n",
      "Batch size per GPU: 4\n",
      "Gradient accumulation: 4\n",
      "Effective batch size: 32\n",
      "============================================================\n",
      "\n",
      "Loading dataset...\n",
      "Train samples: 1800\n",
      "Eval samples: 200\n",
      "\n",
      "Loading model...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "Applying LoRA...\n",
      "Trainable parameters: 2,162,688 (0.44%)\n",
      "/teamspace/studios/this_studio/train_distributed.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/teamspace/studios/this_studio/train_distributed.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Before initializing optimizer states\n",
      "MA 0.94 GB         Max_MA 0.94 GB         CA 0.99 GB         Max_CA 1 GB \n",
      "CPU Virtual Memory:  used = 12.68 GB, percent = 13.4%\n",
      "After initializing optimizer states\n",
      "MA 0.94 GB         Max_MA 0.94 GB         CA 0.99 GB         Max_CA 1 GB \n",
      "CPU Virtual Memory:  used = 12.68 GB, percent = 13.5%\n",
      "After initializing ZeRO optimizer\n",
      "MA 0.94 GB         Max_MA 0.94 GB         CA 0.99 GB         Max_CA 1 GB \n",
      "CPU Virtual Memory:  used = 12.66 GB, percent = 13.4%\n",
      "{'loss': 1.4397, 'grad_norm': 0.4849773943424225, 'learning_rate': 0.000192112365311485, 'epoch': 0.18}\n",
      "{'loss': 1.4282, 'grad_norm': 0.4735908508300781, 'learning_rate': 0.00015644432188667695, 'epoch': 0.36}\n",
      "{'loss': 1.3706, 'grad_norm': 0.3785214424133301, 'learning_rate': 0.00010285560507936961, 'epoch': 0.53}\n",
      "{'loss': 1.3852, 'grad_norm': 0.4147862195968628, 'learning_rate': 4.836025383610382e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3667, 'grad_norm': 0.3638835549354553, 'learning_rate': 1.026015713086418e-05, 'epoch': 0.89}\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:45<00:06,  1.11it/s]\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 12%|█████▎                                      | 3/25 [00:00<00:01, 16.56it/s]\u001B[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:01, 13.22it/s]\u001B[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:01, 12.20it/s]\u001B[A\n",
      " 36%|███████████████▊                            | 9/25 [00:00<00:01, 11.73it/s]\u001B[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:00<00:01, 11.46it/s]\u001B[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01, 11.28it/s]\u001B[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:01<00:00, 11.13it/s]\u001B[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:01<00:00, 11.09it/s]\u001B[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:01<00:00, 11.05it/s]\u001B[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:01<00:00, 11.03it/s]\u001B[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:02<00:00, 11.03it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 1.341916799545288, 'eval_runtime': 2.2874, 'eval_samples_per_second': 87.436, 'eval_steps_per_second': 10.93, 'epoch': 0.89}\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:47<00:06,  1.11it/s]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:02<00:00, 11.13it/s]\u001B[A\n",
      "{'train_runtime': 55.6217, 'train_samples_per_second': 32.361, 'train_steps_per_second': 1.025, 'train_loss': 1.4305567825049685, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 57/57 [00:55<00:00,  1.02it/s]\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Total training time: 57.18 seconds (0.95 minutes)\n",
      "\n",
      "Saving model...\n",
      "Model saved to: ./outputs/distributed_finetuned\n",
      "[rank0]:[W124 13:54:20.857334104 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Launch with DeepSpeed ZeRO-2\n",
    "!accelerate launch --config_file configs/deepspeed_zero2_config.yaml train_distributed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Option C: Using DeepSpeed ZeRO-3 (for larger models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0124 13:55:21.307000 83788 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] \n",
      "W0124 13:55:21.307000 83788 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] *****************************************\n",
      "W0124 13:55:21.307000 83788 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0124 13:55:21.307000 83788 /system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/distributed/run.py:774] *****************************************\n",
      "============================================================\n",
      "DISTRIBUTED TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Number of GPUs: 2\n",
      "Distributed type: DistributedType.DEEPSPEED\n",
      "Mixed precision: bf16\n",
      "Model: Qwen/Qwen2.5-0.5B\n",
      "Batch size per GPU: 4\n",
      "Gradient accumulation: 4\n",
      "Effective batch size: 32\n",
      "============================================================\n",
      "\n",
      "Loading dataset...\n",
      "Train samples: 1800\n",
      "Eval samples: 200\n",
      "\n",
      "Loading model...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "Applying LoRA...\n",
      "Trainable parameters: 2,162,688 (0.44%)\n",
      "/teamspace/studios/this_studio/train_distributed.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/teamspace/studios/this_studio/train_distributed.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "Before initializing optimizer states\n",
      "MA 0.94 GB         Max_MA 0.94 GB         CA 0.99 GB         Max_CA 1 GB \n",
      "CPU Virtual Memory:  used = 12.91 GB, percent = 13.7%\n",
      "After initializing optimizer states\n",
      "MA 0.94 GB         Max_MA 0.94 GB         CA 0.99 GB         Max_CA 1 GB \n",
      "CPU Virtual Memory:  used = 12.92 GB, percent = 13.7%\n",
      "After initializing ZeRO optimizer\n",
      "MA 0.94 GB         Max_MA 0.94 GB         CA 0.99 GB         Max_CA 1 GB \n",
      "CPU Virtual Memory:  used = 12.93 GB, percent = 13.7%\n",
      "{'loss': 1.4397, 'grad_norm': 0.4849773943424225, 'learning_rate': 0.000192112365311485, 'epoch': 0.18}\n",
      "{'loss': 1.4283, 'grad_norm': 0.47627273201942444, 'learning_rate': 0.00015644432188667695, 'epoch': 0.36}\n",
      "{'loss': 1.3707, 'grad_norm': 0.3821205198764801, 'learning_rate': 0.00010285560507936961, 'epoch': 0.53}\n",
      "{'loss': 1.3853, 'grad_norm': 0.41642600297927856, 'learning_rate': 4.836025383610382e-05, 'epoch': 0.71}\n",
      "{'loss': 1.367, 'grad_norm': 0.3649365305900574, 'learning_rate': 1.026015713086418e-05, 'epoch': 0.89}\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:45<00:06,  1.11it/s]\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001B[A\n",
      " 12%|█████▎                                      | 3/25 [00:00<00:01, 16.64it/s]\u001B[A\n",
      " 20%|████████▊                                   | 5/25 [00:00<00:01, 13.31it/s]\u001B[A\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:01, 12.23it/s]\u001B[A\n",
      " 36%|███████████████▊                            | 9/25 [00:00<00:01, 11.75it/s]\u001B[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:00<00:01, 11.47it/s]\u001B[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01, 11.32it/s]\u001B[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:01<00:00, 11.21it/s]\u001B[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:01<00:00, 11.11it/s]\u001B[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:01<00:00, 11.05it/s]\u001B[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:01<00:00, 11.01it/s]\u001B[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:02<00:00, 10.98it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 1.3418835401535034, 'eval_runtime': 2.2858, 'eval_samples_per_second': 87.497, 'eval_steps_per_second': 10.937, 'epoch': 0.89}\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:48<00:06,  1.11it/s]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:02<00:00, 11.06it/s]\u001B[A\n",
      "{'train_runtime': 55.7404, 'train_samples_per_second': 32.293, 'train_steps_per_second': 1.023, 'train_loss': 1.4306606158875583, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 57/57 [00:55<00:00,  1.02it/s]\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Total training time: 57.33 seconds (0.96 minutes)\n",
      "\n",
      "Saving model...\n",
      "Model saved to: ./outputs/distributed_finetuned_deepspeed_zero3\n",
      "[rank0]:[W124 13:56:33.187304539 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Launch with DeepSpeed ZeRO-3 (for larger models)\n",
    "!accelerate launch --config_file configs/deepspeed_zero3_config.yaml train_distributed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Single GPU Baseline for Comparison\n",
    "\n",
    "Let's also run on a single GPU to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single GPU config created!\n"
     ]
    }
   ],
   "source": [
    "# Create single-GPU config\n",
    "single_gpu_config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"distributed_type\": \"NO\",\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 1,\n",
    "    \"use_cpu\": False,\n",
    "}\n",
    "\n",
    "with open(\"configs/single_gpu_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(single_gpu_config, f)\n",
    "\n",
    "print(\"Single GPU config created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISTRIBUTED TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Number of GPUs: 1\n",
      "Distributed type: DistributedType.NO\n",
      "Mixed precision: bf16\n",
      "Model: Qwen/Qwen2.5-0.5B\n",
      "Batch size per GPU: 4\n",
      "Gradient accumulation: 4\n",
      "Effective batch size: 16\n",
      "============================================================\n",
      "\n",
      "Loading dataset...\n",
      "Train samples: 1800\n",
      "Eval samples: 200\n",
      "\n",
      "Loading model...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "Applying LoRA...\n",
      "Trainable parameters: 2,162,688 (0.44%)\n",
      "/teamspace/studios/this_studio/train_distributed.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "{'loss': 1.4701, 'grad_norm': 0.7258951663970947, 'learning_rate': 0.00019896341474445525, 'epoch': 0.09}\n",
      "{'loss': 1.3557, 'grad_norm': 0.48308053612709045, 'learning_rate': 0.0001907992282510675, 'epoch': 0.18}\n",
      "{'loss': 1.4405, 'grad_norm': 0.43630480766296387, 'learning_rate': 0.0001751443762949772, 'epoch': 0.27}\n",
      "{'loss': 1.3852, 'grad_norm': 0.5262428522109985, 'learning_rate': 0.00015329033746173975, 'epoch': 0.36}\n",
      "{'loss': 1.3566, 'grad_norm': 0.6271294355392456, 'learning_rate': 0.0001270400047611508, 'epoch': 0.44}\n",
      " 44%|██████████████████▌                       | 50/113 [00:49<01:02,  1.02it/s]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001B[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 19.03it/s]\u001B[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.86it/s]\u001B[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.62it/s]\u001B[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.11it/s]\u001B[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04,  9.81it/s]\u001B[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.62it/s]\u001B[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03,  9.56it/s]\u001B[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.51it/s]\u001B[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.36it/s]\u001B[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.34it/s]\u001B[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.32it/s]\u001B[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.31it/s]\u001B[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.31it/s]\u001B[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:02<00:03,  9.29it/s]\u001B[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.28it/s]\u001B[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  9.26it/s]\u001B[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.23it/s]\u001B[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.26it/s]\u001B[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02,  9.28it/s]\u001B[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.29it/s]\u001B[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.28it/s]\u001B[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.28it/s]\u001B[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:03<00:02,  9.25it/s]\u001B[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.27it/s]\u001B[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  9.29it/s]\u001B[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01,  9.30it/s]\u001B[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.30it/s]\u001B[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  9.31it/s]\u001B[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01,  9.31it/s]\u001B[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.31it/s]\u001B[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  9.30it/s]\u001B[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  9.28it/s]\u001B[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  9.29it/s]\u001B[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  9.28it/s]\u001B[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:00,  9.27it/s]\u001B[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  9.28it/s]\u001B[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  9.29it/s]\u001B[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.31it/s]\u001B[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00,  9.32it/s]\u001B[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.31it/s]\u001B[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  9.29it/s]\u001B[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  9.29it/s]\u001B[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:05<00:00,  9.28it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 1.3582266569137573, 'eval_runtime': 5.3852, 'eval_samples_per_second': 37.139, 'eval_steps_per_second': 9.285, 'epoch': 0.44}\n",
      " 44%|██████████████████▌                       | 50/113 [00:55<01:02,  1.02it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  9.39it/s]\u001B[A\n",
      "{'loss': 1.3679, 'grad_norm': 0.4314318895339966, 'learning_rate': 9.85589523322443e-05, 'epoch': 0.53}\n",
      "{'loss': 1.3466, 'grad_norm': 0.5254917144775391, 'learning_rate': 7.019678203706163e-05, 'epoch': 0.62}\n",
      "{'loss': 1.4149, 'grad_norm': 0.5844043493270874, 'learning_rate': 4.4293288316255653e-05, 'epoch': 0.71}\n",
      "{'loss': 1.3818, 'grad_norm': 0.6155463457107544, 'learning_rate': 2.2985432127701946e-05, 'epoch': 0.8}\n",
      "{'loss': 1.3494, 'grad_norm': 0.5243532657623291, 'learning_rate': 8.031048042356392e-06, 'epoch': 0.89}\n",
      " 88%|████████████████████████████████████▎    | 100/113 [01:44<00:12,  1.02it/s]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001B[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.62it/s]\u001B[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.71it/s]\u001B[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.46it/s]\u001B[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.98it/s]\u001B[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04,  9.72it/s]\u001B[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.58it/s]\u001B[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03,  9.52it/s]\u001B[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.43it/s]\u001B[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.37it/s]\u001B[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.35it/s]\u001B[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.31it/s]\u001B[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.32it/s]\u001B[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.28it/s]\u001B[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:02<00:03,  9.26it/s]\u001B[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.28it/s]\u001B[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  9.29it/s]\u001B[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.26it/s]\u001B[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.23it/s]\u001B[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02,  9.25it/s]\u001B[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.25it/s]\u001B[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.26it/s]\u001B[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.27it/s]\u001B[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:03<00:02,  9.28it/s]\u001B[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.27it/s]\u001B[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  9.26it/s]\u001B[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01,  9.28it/s]\u001B[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.28it/s]\u001B[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  9.27it/s]\u001B[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01,  9.27it/s]\u001B[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.27it/s]\u001B[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  9.27it/s]\u001B[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  9.27it/s]\u001B[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  9.28it/s]\u001B[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  9.30it/s]\u001B[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:00,  9.30it/s]\u001B[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  9.28it/s]\u001B[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  9.29it/s]\u001B[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.29it/s]\u001B[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00,  9.28it/s]\u001B[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.29it/s]\u001B[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  9.27it/s]\u001B[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  9.25it/s]\u001B[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:05<00:00,  9.23it/s]\u001B[A\n",
      "                                                                                \u001B[A\n",
      "\u001B[A{'eval_loss': 1.351936936378479, 'eval_runtime': 5.4022, 'eval_samples_per_second': 37.022, 'eval_steps_per_second': 9.256, 'epoch': 0.89}\n",
      " 88%|████████████████████████████████████▎    | 100/113 [01:49<00:12,  1.02it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  9.33it/s]\u001B[A\n",
      "{'loss': 1.4877, 'grad_norm': 0.6669118404388428, 'learning_rate': 6.638281360408339e-07, 'epoch': 0.98}\n",
      "{'train_runtime': 123.0668, 'train_samples_per_second': 14.626, 'train_steps_per_second': 0.918, 'train_loss': 1.3940579363730101, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 113/113 [02:03<00:00,  1.09s/it]\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Total training time: 123.36 seconds (2.06 minutes)\n",
      "\n",
      "Saving model...\n",
      "Model saved to: ./outputs/distributed_finetuned_ddp\n"
     ]
    }
   ],
   "source": [
    "# Run on single GPU for comparison\n",
    "!CUDA_VISIBLE_DEVICES=0 accelerate launch --config_file configs/single_gpu_config.yaml train_distributed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Monitor Training\n",
    "\n",
    "While training is running, you can monitor GPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage (run in separate terminal or before training)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For continuous monitoring, use watch (in terminal):\n",
    "# watch -n 1 nvidia-smi\n",
    "\n",
    "# Or use Python for a quick snapshot\n",
    "import subprocess\n",
    "\n",
    "def gpu_memory_usage():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', \n",
    "         '--format=csv,noheader,nounits'],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    print(\"GPU | Memory Used | Memory Total | Utilization\")\n",
    "    print(\"-\" * 50)\n",
    "    for line in result.stdout.strip().split('\\n'):\n",
    "        idx, used, total, util = line.split(', ')\n",
    "        print(f\"GPU {idx} | {used:>6} MB | {total:>6} MB | {util:>3}%\")\n",
    "\n",
    "gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Analyze Results\n",
    "\n",
    "Compare training metrics between single GPU and multi-GPU runs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T14:36:15.979783Z",
     "start_time": "2026-01-24T14:36:15.968783Z"
    }
   },
   "source": [
    "print('''\n",
    "DISTRIBUTED TRAINING COMPARISON\n",
    "================================\n",
    "\n",
    "| Metric          | Single GPU | 2x GPU (DDP) | 2x GPU (ZeRO-2) | 2x GPU (ZeRO-3) |\n",
    "|-----------------|------------|--------------|-----------------|-----------------|\n",
    "| Training Time   | 123.36 sec | 59.87 sec    | 57.18 sec       | 57.33 sec       |\n",
    "| Speedup         | 1.0x       | 2.06x        | 2.16x           | 2.15x           |\n",
    "| Memory/GPU      | ~1.5 GB    | ~1.5 GB      | 0.94 GB         | 0.94 GB         |\n",
    "| Train Loss      | 1.394      | 1.431        | 1.431           | 1.431           |\n",
    "| Eval Loss       | 1.352      | 1.342        | 1.342           | 1.342           |\n",
    "| Samples/sec     | 14.63      | 30.37        | 32.36           | 32.29           |\n",
    "| Steps/sec       | 0.918      | 0.962        | 1.025           | 1.023           |\n",
    "\n",
    "Effective Batch Size:\n",
    "- Single GPU: batch_size × grad_accum = 4 × 4 = 16\n",
    "- Multi-GPU:  batch_size × num_gpus × grad_accum = 4 × 2 × 4 = 32\n",
    "\n",
    "KEY OBSERVATIONS:\n",
    "\n",
    "1. SPEEDUP: All multi-GPU methods achieved ~2x speedup over single GPU\n",
    "   - DDP: 2.06x | ZeRO-2: 2.16x | ZeRO-3: 2.15x\n",
    "\n",
    "2. MEMORY: ZeRO-2 and ZeRO-3 used 37% less GPU memory (0.94 GB vs ~1.5 GB)\n",
    "   - This matters more for larger models that barely fit in memory\n",
    "\n",
    "3. THROUGHPUT: Doubled from 14.6 to 30-32 samples/second\n",
    "\n",
    "4. LOSS: Multi-GPU runs had slightly better eval loss (1.342 vs 1.352)\n",
    "   - Likely due to larger effective batch size (32 vs 16)\n",
    "\n",
    "5. ZeRO-2 vs ZeRO-3: Nearly identical performance on this small model\n",
    "   - ZeRO-3 benefits appear with larger models that need parameter sharding\n",
    "\n",
    "WHEN TO USE EACH:\n",
    "- DDP:    Model fits in GPU memory, want simplicity and speed\n",
    "- ZeRO-2: Need memory savings for optimizer states, minimal overhead\n",
    "- ZeRO-3: Model too large for single GPU, need parameter sharding + CPU offload\n",
    "''')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DISTRIBUTED TRAINING COMPARISON\n",
      "================================\n",
      "\n",
      "| Metric          | Single GPU | 2x GPU (DDP) | 2x GPU (ZeRO-2) | 2x GPU (ZeRO-3) |\n",
      "|-----------------|------------|--------------|-----------------|-----------------|\n",
      "| Training Time   | 123.36 sec | 59.87 sec    | 57.18 sec       | 57.33 sec       |\n",
      "| Speedup         | 1.0x       | 2.06x        | 2.16x           | 2.15x           |\n",
      "| Memory/GPU      | ~1.5 GB    | ~1.5 GB      | 0.94 GB         | 0.94 GB         |\n",
      "| Train Loss      | 1.394      | 1.431        | 1.431           | 1.431           |\n",
      "| Eval Loss       | 1.352      | 1.342        | 1.342           | 1.342           |\n",
      "| Samples/sec     | 14.63      | 30.37        | 32.36           | 32.29           |\n",
      "| Steps/sec       | 0.918      | 0.962        | 1.025           | 1.023           |\n",
      "\n",
      "Effective Batch Size:\n",
      "- Single GPU: batch_size × grad_accum = 4 × 4 = 16\n",
      "- Multi-GPU:  batch_size × num_gpus × grad_accum = 4 × 2 × 4 = 32\n",
      "\n",
      "KEY OBSERVATIONS:\n",
      "\n",
      "1. SPEEDUP: All multi-GPU methods achieved ~2x speedup over single GPU\n",
      "   - DDP: 2.06x | ZeRO-2: 2.16x | ZeRO-3: 2.15x\n",
      "\n",
      "2. MEMORY: ZeRO-2 and ZeRO-3 used 37% less GPU memory (0.94 GB vs ~1.5 GB)\n",
      "   - This matters more for larger models that barely fit in memory\n",
      "\n",
      "3. THROUGHPUT: Doubled from 14.6 to 30-32 samples/second\n",
      "\n",
      "4. LOSS: Multi-GPU runs had slightly better eval loss (1.342 vs 1.352)\n",
      "   - Likely due to larger effective batch size (32 vs 16)\n",
      "\n",
      "5. ZeRO-2 vs ZeRO-3: Nearly identical performance on this small model\n",
      "   - ZeRO-3 benefits appear with larger models that need parameter sharding\n",
      "\n",
      "WHEN TO USE EACH:\n",
      "- DDP:    Model fits in GPU memory, want simplicity and speed\n",
      "- ZeRO-2: Need memory savings for optimizer states, minimal overhead\n",
      "- ZeRO-3: Model too large for single GPU, need parameter sharding + CPU offload\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Load and Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model\n",
    "OUTPUT_DIR = \"./outputs/distributed_finetuned\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Instruction: Explain what machine learning is in simple terms.\n",
      "============================================================\n",
      "Response: Machine learning is a branch of artificial intelligence that involves the development of algorithms and statistical models that enable computers to learn from and make predictions or decisions without being explicitly programmed. It uses statistical methods to identify patterns and relationships in data, allowing machines to learn from experience and make decisions based on that data. Machine learning algorithms are trained on large amounts of data, and as the data changes, the algorithm can adapt and improve its performance over time. This makes machine learning a powerful tool for automating tasks, such as image recognition, natural language processing, and predictive analytics.\n",
      "\n",
      "============================================================\n",
      "Instruction: Write a haiku about coding.\n",
      "============================================================\n",
      "Response: Programming is like a game, \n",
      "Code is the winning move.\n",
      "\n",
      "============================================================\n",
      "Instruction: What are three tips for learning a new programming language?\n",
      "============================================================\n",
      "Response: 1. Start with a beginner's guide: Before diving into a programming language, it is always a good idea to read a beginner's guide or tutorial that provides an overview of the language and its features. This will give you a solid foundation to build upon and make learning more enjoyable.\n",
      "\n",
      "2. Practice, practice, practice: Learning a new programming language takes time and practice. The more you code, the more comfortable you will become with the language. Try to find resources that offer guided practice and feedback to help you improve.\n",
      "\n",
      "3. Use online resources: There are many online resources available that can help you learn a new programming language. Some\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "def generate_response(instruction, max_new_tokens=128):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "test_instructions = [\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"Write a haiku about coding.\",\n",
    "    \"What are three tips for learning a new programming language?\"\n",
    "]\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    response = generate_response(instruction)\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Troubleshooting Common Issues\n",
    "\n",
    "### 11.1 NCCL Errors\n",
    "\n",
    "```bash\n",
    "# If you see NCCL timeout errors, try:\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_TIMEOUT=1800\n",
    "```\n",
    "\n",
    "### 11.2 Out of Memory\n",
    "\n",
    "```python\n",
    "# Reduce batch size or use more aggressive ZeRO stage\n",
    "BATCH_SIZE_PER_GPU = 2  # Reduce from 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 8  # Increase to maintain effective batch\n",
    "```\n",
    "\n",
    "### 11.3 DeepSpeed Installation Issues\n",
    "\n",
    "```bash\n",
    "# Reinstall DeepSpeed with CUDA support\n",
    "pip uninstall deepspeed\n",
    "DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install deepspeed\n",
    "```\n",
    "\n",
    "### 11.4 Processes Not Starting\n",
    "\n",
    "```bash\n",
    "# Check if previous processes are still running\n",
    "ps aux | grep python\n",
    "# Kill orphaned processes if needed\n",
    "pkill -f train_distributed.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Lightning.ai Setup** — Verified 2-GPU configuration and installed dependencies\n",
    "\n",
    "2. **Accelerate Configuration** — Created configs for DDP, ZeRO-2, and ZeRO-3\n",
    "\n",
    "3. **Training Script** — Minimal changes needed for distributed training:\n",
    "   - Use `Accelerator()` \n",
    "   - Launch with `accelerate launch`\n",
    "   - HuggingFace Trainer handles the rest\n",
    "\n",
    "4. **Performance Comparison:**\n",
    "   - DDP: Best speed, no memory savings\n",
    "   - ZeRO-2: Good speed, memory savings\n",
    "   - ZeRO-3: Maximum memory savings, more overhead\n",
    "\n",
    "5. **Effective Batch Size** — Scales with GPU count, may need LR adjustment\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Model fits, want speed | DDP |\n",
    "| Need slightly more memory | ZeRO-2 |\n",
    "| Training much larger model | ZeRO-3 |\n",
    "| Memory still tight | Add CPU offloading |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You now have the skills to:\n",
    "- Configure multi-GPU training on Lightning.ai\n",
    "- Choose between DDP, ZeRO-2, and ZeRO-3\n",
    "- Fine-tune models across multiple GPUs\n",
    "\n",
    "**Future topics to explore:**\n",
    "- Multi-node training (multiple machines)\n",
    "- FSDP as an alternative to DeepSpeed\n",
    "- Profiling and optimizing distributed training\n",
    "- RLHF with distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [Lightning.ai Documentation](https://lightning.ai/docs/)\n",
    "- [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/)\n",
    "- [DeepSpeed Documentation](https://www.deepspeed.ai/)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
