{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# üöÄ Part 8: Inference & Deployment\n",
    "\n",
    "**You've fine-tuned a model. Now what?**\n",
    "\n",
    "This notebook covers how to actually **use** your fine-tuned model.\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| Quick Training | Train a model with Unsloth (fast!) |\n",
    "| Local Inference | Run the model directly |\n",
    "| Saving Formats | LoRA, merged, GGUF |\n",
    "| HuggingFace Hub | Upload and share |\n",
    "| GGUF + llama.cpp | Run on CPU/edge devices |\n",
    "| Ollama Integration | Easy local deployment |\n",
    "\n",
    "---\n",
    "\n",
    "## Deployment Options Overview\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    YOUR FINE-TUNED MODEL                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                          ‚îÇ\n",
    "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ               ‚îÇ               ‚îÇ\n",
    "          ‚ñº               ‚ñº               ‚ñº\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ   LoRA      ‚îÇ ‚îÇ   Merged    ‚îÇ ‚îÇ    GGUF     ‚îÇ\n",
    "   ‚îÇ  Adapter    ‚îÇ ‚îÇ   Model     ‚îÇ ‚îÇ  (llama.cpp)‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚îÇ               ‚îÇ               ‚îÇ\n",
    "          ‚ñº               ‚ñº               ‚ñº\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ HuggingFace ‚îÇ ‚îÇ   vLLM /    ‚îÇ ‚îÇ   Ollama    ‚îÇ\n",
    "   ‚îÇ Inference   ‚îÇ ‚îÇ   TGI       ‚îÇ ‚îÇ   Local     ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "l3aE1gR3tNf-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part A: Setup & Quick Training\n",
    "\n",
    "Let's train a model quickly with Unsloth so we have something to deploy."
   ],
   "metadata": {
    "id": "gre5Qg_1tNgA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q unsloth\n",
    "!pip install -q --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0cU8YAAtNgB",
    "outputId": "5772f8c5-6b2b-4134-9f2c-d919f43ba0b8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may need to restart session."
   ],
   "metadata": {
    "id": "tIEGssxfxOKq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"UNSLOTH_DISABLE_PATCHING\"] = \"1\"   # TRL patching closed\n",
    "os.environ[\"UNSLOTH_USE_FUSED_LOSS\"] = \"0\"     # fused loss closed\n",
    "os.environ[\"UNSLOTH_DISABLE_COMPILE\"] = \"1\"    # unsloth compile closed\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"        # torch dynamo closed"
   ],
   "metadata": {
    "id": "0nDtQZrzww-W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ot1dOegYw6EM",
    "outputId": "d2912cae-71d1-4ac3-ce4a-ac173b02455f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ak3vhyhFtNgB",
    "outputId": "765f1277-6cb9-44b0-d7ff-3a226b1cead9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ GPU: Tesla T4\n",
      "   Memory: 15.8 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "print(f\"üìã Model: {MODEL_ID}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtwcsIeHtNgB",
    "outputId": "fb3c9b76-d6a6-48d4-b598-1a8428d3604c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìã Model: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load model with Unsloth\n",
    "print(\"üì¶ Loading model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_ID,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "print(\"‚úÖ Model loaded!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjaCHNyVtNgC",
    "outputId": "b79ae78e-5688-47ee-e0c7-96d5e9800852"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì¶ Loading model...\n",
      "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ Model loaded!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "print(\"‚úÖ LoRA applied!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzFydzH1tNgC",
    "outputId": "818b8b11-04a9-40cd-a299-659bdaa414da"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2026.1.3 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ LoRA applied!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
    "print(f\"‚úÖ Dataset: {len(dataset)} examples\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwpQ50w-tNgC",
    "outputId": "992b0951-e419-458c-921d-c8becf262f04",
    "ExecuteTime": {
     "end_time": "2026-01-21T14:56:35.235027Z",
     "start_time": "2026-01-21T14:55:47.051150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "346674c09442469ba20af3a4da46d18a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PyCharmProjects_Backup\\SLM-FineTune\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pelin\\.cache\\huggingface\\hub\\datasets--mlabonne--guanaco-llama2-1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data/train-00000-of-00001-9ad84bb9cf65a4(‚Ä¶):   0%|          | 0.00/967k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82ade92955a047aa9f38aa0b476b8312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac64a0b3f0f04d858a620cdc92dc66c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset: 1000 examples\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Explore the dataset\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DATASET EXPLORATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic info\n",
    "print(f\"\\nüìã Dataset size: {len(dataset)} examples\")\n",
    "print(f\"üìã Features: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Look at structure\n",
    "print(f\"\\nüîç First example:\")\n",
    "print(\"-\"*60)\n",
    "print(dataset[0][\"text\"][:500] + \"...\" if len(dataset[0][\"text\"]) > 500 else dataset[0][\"text\"])\n",
    "\n",
    "# Text length statistics\n",
    "text_lengths = [len(item[\"text\"]) for item in dataset]\n",
    "token_estimates = [len(item[\"text\"].split()) for item in dataset]\n",
    "\n",
    "print(f\"\\nüìà Text Length Statistics:\")\n",
    "print(f\"   Min chars:     {min(text_lengths):,}\")\n",
    "print(f\"   Max chars:     {max(text_lengths):,}\")\n",
    "print(f\"   Avg chars:     {sum(text_lengths)//len(text_lengths):,}\")\n",
    "print(f\"   Avg words:     {sum(token_estimates)//len(token_estimates):,}\")\n",
    "\n",
    "# Check format (Llama2 style)\n",
    "sample = dataset[0][\"text\"]\n",
    "has_inst = \"[INST]\" in sample\n",
    "has_human = \"### Human:\" in sample\n",
    "print(f\"\\nüìù Format detected:\")\n",
    "print(f\"   Llama2 style [INST]: {has_inst}\")\n",
    "print(f\"   Guanaco style ###:   {has_human}\")\n",
    "\n",
    "# Show a few more examples\n",
    "print(f\"\\nüîç Sample prompts (first 100 chars):\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    preview = dataset[i][\"text\"][:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   [{i}] {preview}...\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4rpQqpluB-k",
    "outputId": "1e256a02-cd0f-45c0-f056-e57a2dc124e1",
    "ExecuteTime": {
     "end_time": "2026-01-21T14:59:43.419802Z",
     "start_time": "2026-01-21T14:59:43.269789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DATASET EXPLORATION\n",
      "============================================================\n",
      "\n",
      "üìã Dataset size: 1000 examples\n",
      "üìã Features: ['text']\n",
      "\n",
      "üîç First example:\n",
      "------------------------------------------------------------\n",
      "<s>[INST] Me gradu√© hace poco de la carrera de medicina ¬øMe podr√≠as aconsejar para conseguir r√°pidamente un puesto de trabajo? [/INST] Esto vale tanto para m√©dicos como para cualquier otra profesi√≥n tras finalizar los estudios aniversarios y mi consejo ser√≠a preguntar a cu√°ntas personas haya conocido mejor. En este caso, mi primera opci√≥n ser√≠a hablar con otros profesionales m√©dicos, echar curr√≠culos en hospitales y cualquier centro de salud. En paralelo, trabajar√≠a por mejorar mi marca personal...\n",
      "\n",
      "üìà Text Length Statistics:\n",
      "   Min chars:     58\n",
      "   Max chars:     11,443\n",
      "   Avg chars:     1,499\n",
      "   Avg words:     234\n",
      "\n",
      "üìù Format detected:\n",
      "   Llama2 style [INST]: True\n",
      "   Guanaco style ###:   False\n",
      "\n",
      "üîç Sample prompts (first 100 chars):\n",
      "   [0] <s>[INST] Me gradu√© hace poco de la carrera de medicina ¬øMe podr√≠as aconsejar para conseguir r√°pidam...\n",
      "   [1] <s>[INST] –°–∞–º—ã–π –≤–µ–ª–∏–∫–∏–π —á–µ–ª–æ–≤–µ–∫ –∏–∑ –≤—Å–µ—Ö –∂–∏–≤—à–∏—Ö –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ? [/INST] –î–ª—è –Ω–∞—á–∞–ª–∞ –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏...\n",
      "   [2] <s>[INST] Compose a professional email with the following points:  Me chinese cook 10 years Good goo...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T15:02:26.232530Z",
     "start_time": "2026-01-21T15:02:22.576216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze dataset text lengths (no tokenizer needed)\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset (quick, just metadata)\n",
    "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
    "\n",
    "# Approximate tokens by words (rough estimate: 1 word ‚âà 1.3 tokens)\n",
    "word_lengths = [len(ex[\"text\"].split()) for ex in dataset]\n",
    "approx_tokens = [int(w * 1.3) for w in word_lengths]\n",
    "\n",
    "print(f\"üìä Approximate Token Length Statistics:\")\n",
    "print(f\"   Min: {min(approx_tokens)}\")\n",
    "print(f\"   Max: {max(approx_tokens)}\")\n",
    "print(f\"   Mean: {sum(approx_tokens)//len(approx_tokens)}\")\n",
    "print(f\"   Examples > 512 tokens: {sum(1 for l in approx_tokens if l > 512)} ({100*sum(1 for l in approx_tokens if l > 512)/len(approx_tokens):.1f}%)\")\n",
    "print(f\"   Examples > 1024 tokens: {sum(1 for l in approx_tokens if l > 1024)} ({100*sum(1 for l in approx_tokens if l > 1024)/len(approx_tokens):.1f}%)\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(approx_tokens, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=512, color='r', linestyle='--', linewidth=2, label='MAX_SEQ_LENGTH=512')\n",
    "plt.axvline(x=1024, color='orange', linestyle='--', linewidth=2, label='MAX_SEQ_LENGTH=1024')\n",
    "plt.xlabel(\"Approximate Token Length\")\n",
    "plt.ylabel(\"Number of Examples\")\n",
    "plt.title(\"Dataset Token Length Distribution\\n(Guanaco-Llama2-1k)\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show what percentage of data is kept/lost\n",
    "kept_512 = sum(1 for l in approx_tokens if l <= 512)\n",
    "kept_1024 = sum(1 for l in approx_tokens if l <= 1024)\n",
    "print(f\"\\nüìà Data Retention:\")\n",
    "print(f\"   With max_length=512:  {kept_512}/{len(dataset)} examples fully kept ({100*kept_512/len(dataset):.1f}%)\")\n",
    "print(f\"   With max_length=1024: {kept_1024}/{len(dataset)} examples fully kept ({100*kept_1024/len(dataset):.1f}%)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Approximate Token Length Statistics:\n",
      "   Min: 6\n",
      "   Max: 2245\n",
      "   Mean: 304\n",
      "   Examples > 512 tokens: 155 (15.5%)\n",
      "   Examples > 1024 tokens: 16 (1.6%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Data Retention:\n",
      "   With max_length=512:  845/1000 examples fully kept (84.5%)\n",
      "   With max_length=1024: 984/1000 examples fully kept (98.4%)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## This code gives error. I will fix it.",
   "metadata": {
    "id": "WlcN1JTZygpW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Training arguments - quick training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer ready!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a3nUBIxtNgC",
    "outputId": "0c63983b-8d78-4790-890c-29f9c8ba43a3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Trainer ready!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Train!\n",
    "import time\n",
    "print(\"üöÄ Training...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "train_time = time.time() - start\n",
    "print(f\"\\n‚úÖ Training complete in {train_time:.0f} seconds!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "cj5tqf6ctNgC",
    "outputId": "f4791943-fd51-4ad8-9fdd-508fa0ecf235"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üöÄ Training...\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 125\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 8,798,208 of 502,830,976 (1.75% trained)\n",
      "Unsloth: Input IDs of shape torch.Size([4, 526]) with length 526 > the model's max sequence length of 512.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (2048) to match target batch_size (2104).",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-277144600.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mtrain_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'model'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"for_training\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfor_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muse_gradient_checkpointing\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_gc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m         \u001B[0;31m# Restore previous mode when possible\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'model'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"for_inference\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2323\u001B[0m                 \u001B[0mhf_hub_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_progress_bars\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2324\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2325\u001B[0;31m             return inner_training_loop(\n\u001B[0m\u001B[1;32m   2326\u001B[0m                 \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2327\u001B[0m                 \u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001B[0m in \u001B[0;36m_fast_inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n",
      "\u001B[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1216\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1217\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaybe_activation_offload_context\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1218\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1219\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1220\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mlog\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001B[0m in \u001B[0;36m_unsloth_training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n",
      "\u001B[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001B[0m in \u001B[0;36mcompute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   1205\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_items_in_batch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1206\u001B[0m     ):\n\u001B[0;32m-> 1207\u001B[0;31m         outputs = super().compute_loss(\n\u001B[0m\u001B[1;32m   1208\u001B[0m             \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1209\u001B[0m             \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001B[0m in \u001B[0;36m_unsloth_pre_compute_loss\u001B[0;34m(self, model, inputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1657\u001B[0m             \u001B[0;34m\"Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1658\u001B[0m         )\n\u001B[0;32m-> 1659\u001B[0;31m     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_old_compute_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1660\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1661\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/_utils.py\u001B[0m in \u001B[0;36mcompute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1773\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1774\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1775\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1776\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1777\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1784\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1785\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1786\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1787\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1788\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    818\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 819\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodel_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    820\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    821\u001B[0m     \u001B[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    805\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    806\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 807\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mconvert_to_fp32\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    808\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    809\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getstate__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001B[0m in \u001B[0;36mdecorate_autocast\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_autocast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mautocast_instance\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001B[0m in \u001B[0;36minner\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m                 \u001B[0mfn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__dynamo_disable\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdisable_fn\u001B[0m  \u001B[0;31m# type: ignore[attr-defined]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mdisable_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minner\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001B[0m in \u001B[0;36m_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1042\u001B[0m                 \u001B[0m_maybe_set_eval_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_callback_from_stance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallback\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1043\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1044\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1045\u001B[0m                 \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1046\u001B[0m                     \u001B[0mset_eval_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001B[0m in \u001B[0;36mPeftModel_fast_forward\u001B[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m   1543\u001B[0m         )\n\u001B[1;32m   1544\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1545\u001B[0;31m         return self.base_model(\n\u001B[0m\u001B[1;32m   1546\u001B[0m             \u001B[0minput_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1547\u001B[0m             \u001B[0mcausal_mask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcausal_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1773\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1774\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1775\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1776\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1777\u001B[0m     \u001B[0;31m# torchrec tests the code consistency with the following code\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1784\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1785\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1786\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1787\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1788\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 308\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    309\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_pre_injection_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mModule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mPeftConfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madapter_name\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001B[0m in \u001B[0;36m_CausalLM_fast_forward\u001B[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1419\u001B[0m                 \u001B[0;31m#     logit_softcapping  = logit_softcapping,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1420\u001B[0m                 \u001B[0;31m# )\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1421\u001B[0;31m                 loss = unsloth_fused_ce_loss(\n\u001B[0m\u001B[1;32m   1422\u001B[0m                     \u001B[0mtrainer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1423\u001B[0m                     \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py\u001B[0m in \u001B[0;36munsloth_fused_ce_loss\u001B[0;34m(trainer, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, target_gb, torch_compile, overwrite, **kwargs)\u001B[0m\n\u001B[1;32m    374\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mTARGET_GB\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtarget_gb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTARGET_GB\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    375\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mN_CHUNKS\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"n_chunks\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mN_CHUNKS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 376\u001B[0;31m     return apply_autograd_function(UnslothFusedLoss, dict(\n\u001B[0m\u001B[1;32m    377\u001B[0m         \u001B[0mloss_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_fused_ce_loss\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py\u001B[0m in \u001B[0;36mapply_autograd_function\u001B[0;34m(autograd, mapping)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mapply_autograd_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[0mparameters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefaults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_mapping\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 46\u001B[0;31m     return getattr(autograd, \"apply\")(*(\n\u001B[0m\u001B[1;32m     47\u001B[0m         \u001B[0mmapping\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mold_key\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefault\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mold_key\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefault\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001B[0m in \u001B[0;36mapply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    579\u001B[0m             \u001B[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_functorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munwrap_dead_wrappers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 581\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    582\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    583\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_setup_ctx_defined\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(ctx, loss_function, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, shift_labels, target_gb, torch_compile, overwrite, extra_kwargs)\u001B[0m\n\u001B[1;32m    312\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mgrad_inputs_j\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_states_j\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels_j\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m             \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m__grad_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m__shift_states\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m__shift_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 314\u001B[0;31m             accumulate_chunk(\n\u001B[0m\u001B[1;32m    315\u001B[0m                 \u001B[0mn_chunks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mn_chunks\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    316\u001B[0m                 \u001B[0mgrad_inputs_j\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgrad_inputs_j\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py\u001B[0m in \u001B[0;36maccumulate_chunk\u001B[0;34m(n_chunks, grad_inputs_j, grad_lm_head, grad_lm_head_bias, hidden_states_j, lm_head_weight, lm_head_bias, labels_j, divisor, scaling, shift_labels, **kwargs)\u001B[0m\n\u001B[1;32m    284\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    285\u001B[0m                 \u001B[0;34m(\u001B[0m\u001B[0mchunk_grad_input\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 286\u001B[0;31m                 (chunk_loss, (unscaled_loss,)) = torch.func.grad_and_value(\n\u001B[0m\u001B[1;32m    287\u001B[0m                     \u001B[0mloss_function\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m                     \u001B[0margnums\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/apis.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 449\u001B[0;31m         return eager_transforms.grad_and_value_impl(\n\u001B[0m\u001B[1;32m    450\u001B[0m             \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margnums\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhas_aux\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    451\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/vmap.py\u001B[0m in \u001B[0;36mfn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgraph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisable_saved_tensors_hooks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/eager_transforms.py\u001B[0m in \u001B[0;36mgrad_and_value_impl\u001B[0;34m(func, argnums, has_aux, args, kwargs)\u001B[0m\n\u001B[1;32m   1362\u001B[0m             \u001B[0mtree_map_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_create_differentiable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdiff_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1364\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1365\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mhas_aux\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1366\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py\u001B[0m in \u001B[0;36mcompute_fused_ce_loss\u001B[0;34m(hidden_states, lm_head_weight, lm_head_bias, labels, n_items, scaling, shift_labels, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m     \u001B[0;31m# Calculate cross entropy loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[0mreduction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"sum\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mn_items\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"mean\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 103\u001B[0;31m     loss = torch.nn.functional.cross_entropy(\n\u001B[0m\u001B[1;32m    104\u001B[0m         \u001B[0minput\u001B[0m  \u001B[0;34m=\u001B[0m \u001B[0mlogits\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m         \u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3456\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msize_average\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mreduce\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3457\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3458\u001B[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001B[0m\u001B[1;32m   3459\u001B[0m         \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3460\u001B[0m         \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Expected input batch_size (2048) to match target batch_size (2104)."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FIX"
   ],
   "metadata": {
    "id": "MaD4TiZly2HT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def tok_fn(examples):\n",
    "    out = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,   # ‚úÖ Cut sequences > MAX_SEQ_LENGTH\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,          # important: no padding in dataset\n",
    "    )\n",
    "    return out\n",
    "\n",
    "tokenized_dataset = dataset.map(tok_fn, batched=True, remove_columns=dataset.column_names)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ea1346d8b15e449f899fe24be21bf676",
      "a9c6393578c54e0aa78ffe56206ff665",
      "ff009746dd22485f9747ac997253ec6b",
      "3d476c7b70e6468cbd1a9c8c323b8208",
      "1de9ae94a98841c690f3fc4df5f87022",
      "bd4bd38d57b4437698590651705803d4",
      "06ba35dd705f487a84819868101798b5",
      "be8551f0892d466097f6a8c06e88b6ed",
      "5cd15eb4d92d4e1d88ae90621f48b052",
      "73ae61084f8a42b2968d3dc8b06656f7",
      "2a63313b7e39473aba64d6742d92a2f0"
     ]
    },
    "id": "UDiYceOSzcWk",
    "outputId": "518f2b7e-8714-4790-8468-5132f9a664ff"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What this does:**\n",
    "- Tokenizes all text BEFORE training\n",
    "- Truncates anything over 512 tokens\n",
    "- `padding=False` ‚Äî no padding yet (each example is different length)\n",
    "- `remove_columns` ‚Äî removes original \"text\" column, keeps only token IDs\n",
    "```\n",
    "Before: {\"text\": \"very long string...\"}\n",
    "After:  {\"input_ids\": [1, 234, 567, ...], \"attention_mask\": [1, 1, 1, ...]}\n",
    "         (max 512 tokens)"
   ],
   "metadata": {
    "id": "iNAIzRwf0MjQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Training arguments - quick training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    warmup_ratio=0.03,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Data collator with proper truncation\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,\n",
    "    data_collator=data_collator,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer ready!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phqfApOqylK3",
    "outputId": "8ff0e507-901f-46ba-ecb2-fdfd1185af37"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Trainer ready!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,              # ‚úÖ Pad to longest in batch\n",
    "        pad_to_multiple_of=8,      # ‚úÖ Pad to multiple of 8 (GPU efficiency)\n",
    "    )\n",
    "    ```\n",
    "\n",
    "**What this does:**\n",
    "- When creating a batch, pads all examples to the same length\n",
    "- Only pads to the longest sequence IN THAT BATCH (not globally)\n",
    "- `pad_to_multiple_of=8` ‚Äî GPU tensor operations are faster with dimensions divisible by 8\n",
    "```\n",
    "Batch example:\n",
    "  Example 1: [1, 2, 3, 4, 5]         ‚Üí [1, 2, 3, 4, 5, PAD, PAD, PAD]\n",
    "  Example 2: [1, 2, 3]               ‚Üí [1, 2, 3, PAD, PAD, PAD, PAD, PAD]  \n",
    "  Example 3: [1, 2, 3, 4, 5, 6, 7]   ‚Üí [1, 2, 3, 4, 5, 6, 7, PAD]\n",
    "                                        ‚Üë padded to 8 (multiple of 8)"
   ],
   "metadata": {
    "id": "sAWOXDWn0rBx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train!\n",
    "import time\n",
    "print(\"üöÄ Training...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "train_time = time.time() - start\n",
    "print(f\"\\n‚úÖ Training complete in {train_time:.0f} seconds!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "9PavMFi8yt4e",
    "outputId": "08a58693-f515-4090-aca2-6993bc0c145c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üöÄ Training...\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 125\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 8,798,208 of 502,830,976 (1.75% trained)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.806100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "‚úÖ Training complete in 115 seconds!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Part B: Local Inference\n",
    "\n",
    "The simplest way to use your model: run it directly!"
   ],
   "metadata": {
    "id": "-ShDDDggtNgD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Enable fast inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"‚úÖ Inference mode enabled\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JM-km44gtNgD",
    "outputId": "2be4a623-2019-4967-a335-8c96a8414812"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ Inference mode enabled\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def chat(prompt, max_tokens=256):\n",
    "    \"\"\"Simple chat function.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ chat() function ready\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ly_tPsI6tNgD",
    "outputId": "4e6acc31-f0b9-4749-d4c8-9ad9ae1b1c91"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ chat() function ready\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "print(\"üß™ Testing the fine-tuned model:\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Write a Python function to reverse a string.\",\n",
    "    \"Give me 3 tips for better sleep.\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"üë§ User: {prompt}\")\n",
    "    response = chat(prompt)\n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    print(\"-\" * 50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qv3tzt9tNgD",
    "outputId": "89826ad1-5ffc-467c-eaf2-d39cb244cd14"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üß™ Testing the fine-tuned model:\n",
      "\n",
      "üë§ User: What is machine learning?\n",
      "ü§ñ Assistant: Machine learning (ML) is a subset of artificial intelligence that involves the development of algorithms and models that enable computers to learn from data and make decisions or predictions based on this learning. It has been used in a wide range of applications, including image recognition, speech recognition, natural language processing, predictive analytics, and more.\n",
      "\n",
      "The process of machine learning typically starts with collecting and preprocessing data, which may include cleaning, transforming, and normalizing it. Then, there is an iterative phase where new data is fed into the model to improve its accuracy over time. The final stage involves testing and validating the model's performance using various metrics such as accuracy, precision, recall, F1 score, and others.\n",
      "\n",
      "In summary, machine learning is a powerful technique for enabling machines to automatically learn patterns from data and perform tasks without explicit programming instructions. This field is currently at the forefront of AI research and has numerous applications across different industries and domains.\n",
      "--------------------------------------------------\n",
      "üë§ User: Write a Python function to reverse a string.\n",
      "ü§ñ Assistant: def reverse_string(s):\n",
      "    return s[::-1]\n",
      "--------------------------------------------------\n",
      "üë§ User: Give me 3 tips for better sleep.\n",
      "ü§ñ Assistant: 1. Create a bedtime routine: Try to go to bed and wake up at the same time every day. This will help your body's natural clock to regulate its sleep cycles.\n",
      "2. Avoid naps: Taking short naps in the afternoon or night can make it harder to fall asleep at night. Make sure you don't nap too close to bedtime or if it's already past midnight.\n",
      "3. Reduce screen time before bedtime: The blue light emitted by electronic devices can interfere with your ability to sleep well. It is best to turn off all electronic devices from an hour before going to bed until about one hour before going to sleep.\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Part C: Saving Your Model\n",
    "\n",
    "Multiple formats for different use cases:\n",
    "\n",
    "| Format | Size | Use Case |\n",
    "|--------|------|----------|\n",
    "| LoRA adapter | ~10-50 MB | Share adapters, combine with base |\n",
    "| Merged 16-bit | ~1-14 GB | HuggingFace, full precision |\n",
    "| Merged 4-bit | ~0.5-4 GB | Smaller, quantized |\n",
    "| GGUF | ~0.3-8 GB | llama.cpp, Ollama, CPU inference |"
   ],
   "metadata": {
    "id": "bgaUAyjctNgD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 1: Save LoRA Adapter Only\n",
    "\n",
    "Smallest size. Requires base model to use."
   ],
   "metadata": {
    "id": "qKJAQjMVtNgD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Save LoRA adapter\n",
    "LORA_PATH = \"./my_lora_adapter\"\n",
    "\n",
    "model.save_pretrained(LORA_PATH)\n",
    "tokenizer.save_pretrained(LORA_PATH)\n",
    "\n",
    "# Check size\n",
    "import os\n",
    "lora_size = sum(os.path.getsize(os.path.join(LORA_PATH, f))\n",
    "               for f in os.listdir(LORA_PATH)\n",
    "               if os.path.isfile(os.path.join(LORA_PATH, f)))\n",
    "\n",
    "print(f\"‚úÖ LoRA adapter saved to {LORA_PATH}\")\n",
    "print(f\"   Size: {lora_size / 1e6:.1f} MB\")\n",
    "print(f\"\\nüìÅ Contents:\")\n",
    "for f in os.listdir(LORA_PATH):\n",
    "    size = os.path.getsize(os.path.join(LORA_PATH, f)) / 1e6\n",
    "    print(f\"   {f}: {size:.1f} MB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZjRLVwfXtNgE",
    "outputId": "b1f1fd32-5e08-4013-a278-e6bdd8c619bc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ LoRA adapter saved to ./my_lora_adapter\n",
      "   Size: 51.1 MB\n",
      "\n",
      "üìÅ Contents:\n",
      "   special_tokens_map.json: 0.0 MB\n",
      "   vocab.json: 2.8 MB\n",
      "   chat_template.jinja: 0.0 MB\n",
      "   tokenizer.json: 11.4 MB\n",
      "   README.md: 0.0 MB\n",
      "   added_tokens.json: 0.0 MB\n",
      "   adapter_config.json: 0.0 MB\n",
      "   tokenizer_config.json: 0.0 MB\n",
      "   adapter_model.safetensors: 35.2 MB\n",
      "   merges.txt: 1.7 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 2: Save Merged Model (16-bit)\n",
    "\n",
    "Full model with adapter weights merged in. Ready for HuggingFace."
   ],
   "metadata": {
    "id": "pgmpbfoitNgE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Save merged 16-bit model\n",
    "MERGED_PATH = \"./my_merged_model\"\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    MERGED_PATH,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Merged model saved to {MERGED_PATH}\")\n",
    "\n",
    "# Check size\n",
    "merged_size = sum(os.path.getsize(os.path.join(MERGED_PATH, f))\n",
    "                 for f in os.listdir(MERGED_PATH)\n",
    "                 if os.path.isfile(os.path.join(MERGED_PATH, f)))\n",
    "print(f\"   Size: {merged_size / 1e9:.2f} GB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324,
     "referenced_widgets": [
      "61e902b8d1774d33b78cb0f865bae91f",
      "3506b76dac6c4a42936cc79443e4dd0c",
      "62a1033d42854703aa2d4dbe77ccd85c",
      "50709e46aa34426ab93853a2242de430",
      "b87933f641fb4a99b79b8901d1dcf977",
      "f4fd8d8411164eca8c796ee348a3a041",
      "38001f1df8aa424bad8f58e38234a78d",
      "29dc1d51ea54451e8e4bb68a835147a7",
      "576026d538ad48d7a640baaacd50c45e",
      "25f4bf419c844c84a5a0e248dabe6ff7",
      "9b7ec207b2664193863f752d1c58ee64",
      "923da3ec4c4f4dcc9ce104d9fe762612",
      "9ec03f6deb5447cf9698036f762d3704",
      "40b433bc18f94b148ef0b0326fe08dd7",
      "dc6e5068dd4c4644bce7d490d85cf863",
      "837ac9ff677a43a3a9fdff16da803166",
      "9f7cb191dd49479d921cae4e29ebf393",
      "2251e4a1628644b1820d7bd1e2af0949",
      "cc5f65fe26454379ab22317e4991f94f",
      "ae8bd71ebd7046b6b52335e9d6f91957",
      "3f79ca4cc2c1456a8b25598a7e629a39",
      "d08bfeb4860645dc9e7004cf639787a5"
     ]
    },
    "id": "Vq6tfJ0YtNgE",
    "outputId": "932cd86f-0def-4a02-bfb9-b3f855c43018"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:07<00:00,  7.46s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:14<00:00, 14.55s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/my_merged_model`\n",
      "‚úÖ Merged model saved to ./my_merged_model\n",
      "   Size: 1.00 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 3: Save as GGUF (for llama.cpp / Ollama)\n",
    "\n",
    "GGUF is the format used by:\n",
    "- **llama.cpp** ‚Äî C++ inference engine\n",
    "- **Ollama** ‚Äî Easy local LLM runner\n",
    "- **LM Studio** ‚Äî Desktop app for LLMs\n",
    "- **GPT4All** ‚Äî Local LLM platform\n",
    "\n",
    "### Quantization Options\n",
    "\n",
    "| Method | Bits | Size | Quality | Speed |\n",
    "|--------|------|------|---------|-------|\n",
    "| q8_0 | 8-bit | Large | Best | Slow |\n",
    "| q5_k_m | 5-bit | Medium | Great | Medium |\n",
    "| q4_k_m | 4-bit | Small | Good | Fast |\n",
    "| q3_k_m | 3-bit | Tiny | OK | Fastest |\n",
    "| q2_k | 2-bit | Smallest | Poor | Fastest |"
   ],
   "metadata": {
    "id": "xbdZ_K3htNgF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Save as GGUF with different quantizations\n",
    "GGUF_PATH = \"./my_model_gguf\"\n",
    "\n",
    "# q4_k_m is a good balance of size and quality\n",
    "model.save_pretrained_gguf(\n",
    "    GGUF_PATH,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q8_0\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ GGUF model saved!\")\n",
    "\n",
    "# List GGUF files\n",
    "print(f\"\\nüìÅ GGUF files:\")\n",
    "for f in os.listdir(GGUF_PATH):\n",
    "    if f.endswith('.gguf'):\n",
    "        size = os.path.getsize(os.path.join(GGUF_PATH, f)) / 1e9\n",
    "        print(f\"   {f}: {size:.2f} GB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urgWh1LUtNgF",
    "outputId": "63096de6-7333-4653-e1df-6b64162c198e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7345.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:22<00:00, 22.31s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Unsloth: Merge process complete. Saved to `/content/my_model_gguf`\nUnsloth: All required system packages already installed!\nUnsloth: Initial conversion completed! Files: ['qwen2.5-0.5b-instruct.F16.gguf']\nUnsloth: Model files cleanup...\nUnsloth: All GGUF conversions completed successfully!\nUnsloth: Saved Ollama Modelfile to current directory\nUnsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n‚úÖ GGUF model saved!\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Part D: Upload to HuggingFace Hub\n",
    "\n",
    "Share your model with the world!"
   ],
   "metadata": {
    "id": "C5yvcUQrtNgF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Login to HuggingFace (you'll need a token)\n",
    "# Get your token at: https://huggingface.co/settings/tokens\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Login interactively\n",
    "login()\n",
    "\n",
    "# Option 2: Use token directly (uncomment and add your token)\n",
    "# login(token=\"hf_your_token_here\")\n",
    "\n",
    "print(\"üí° To upload, run: login() or login(token='your_token')\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "0e2b25e78b0d4b0aa6a8f7b7d448b04d",
      "0a4758f8d10f45f5b170e1ebde464f5e",
      "feeca3d2fccf44d7bf2e580707e85015",
      "b83e4a5c7bcd44ee8932205dd639a8ae",
      "8dd4a1f0753f4f6394be2732023da8b4",
      "6294beeb55494e5ca3dc74a0653ddf36",
      "ac2a98551b3c483da9c90a634c0c839c",
      "02c3a767c09546acac36414b9b262a62",
      "09f07c0b480d4dbaba312a7e2e0de0ca",
      "324e81ed1f3043d08fe4ff0592fa0845",
      "78c5f73a198b46edadc07b276274b9c4",
      "180faa1f329544b8901e118b11b22747",
      "00a144546f814db98321d88c0756ca58",
      "f13c9d817e3c4085a8fe67894212b7ec",
      "7b91f1080a9940f988f420122143e013",
      "575fcf49d5d149cd9d157db2296cc5a5",
      "de42d13260ce4cbc826afe8dd0f46eb1",
      "43cd6c0d35d840f99ac520f89d1938f6",
      "1bf244824fa34d7ca88b4c9e3bbc95cc",
      "f0ce8125b3c2490d8bef0d8786e098dc"
     ]
    },
    "id": "pfRX6wqFtNgF",
    "outputId": "08ddc28b-9b7a-4465-e4a6-debca761d568"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üí° To upload, run: login() or login(token='your_token')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Upload options (uncomment to use)\n",
    "\n",
    "HUB_USERNAME = \"pelinbalci\"  # Change this!\n",
    "MODEL_NAME = \"my-qwen-finetuned\"\n",
    "\n",
    "print(\"üì§ Upload commands (uncomment to use):\")\n",
    "\n",
    "# Upload LoRA adapter only\n",
    "model.push_to_hub(\n",
    "    f\"{HUB_USERNAME}/{MODEL_NAME}-lora\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Upload merged 16-bit model\n",
    "model.push_to_hub_merged(\n",
    "    f\"{HUB_USERNAME}/{MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "# Upload GGUF (for Ollama users)\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{HUB_USERNAME}/{MODEL_NAME}-gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q8_0\",\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f92cea80b5ba4e519ef7a1dda52b7310",
      "80f66b9b36f9464aaa0b8a2f5c4a8d21",
      "176e733ffac24c56a91a9d194021451f",
      "3a2ce1a93cb34257a6a28c3aee2db216",
      "5adfae02303b46588458f29f2e105665",
      "a4a2b537bc01496f9e29b0eafea31619",
      "2f38fa55150a40aea1fe13dda3b96def",
      "fcba0d3cdd2a41999d723f0724d238dc",
      "4dabd985516e4cbaac3b11eb18f406c8",
      "9ccb80ef8671412c90e17b54060f13f9",
      "4b7b1d013e2f4a2cb590e35da4e1cacd",
      "666b136f7bba4a8ea69afc096d034853",
      "077d2423340f49aab96750070d0b6fe1",
      "8698f4a8f95f4192b9367e4291501415",
      "6a08168fed5c48d4aeed274a229735f1",
      "4e3733cbf7134e26a2e4d2f5742bfc4f",
      "3cb5730714674d08be978a938f4f4a47",
      "33d8ba2cafb8410ba112e9eea9d2f439",
      "c8555998d977417dad6fe4f1b43c1f76",
      "c97f9c46cb7645c9a5ca0e858e791314",
      "3fee8b0853b84a77acbf5887323cfebd",
      "80cb0cd4234e4c1ca9cd8f8951590ba9",
      "2d8ecee93ecc4acf9151d4b8d4ba2c71",
      "08cfd1f6b2a2400abdbf07a4779c8cb1",
      "095737f597b341b28262d2f289036276",
      "16bd479f8a3b402e97adb204b2f0bf59",
      "46f62126d77f411698d1e86a2f2dfbd1",
      "6582b8d83e0b467e8d88165b8726ccea",
      "abd418556184419dbfb5281a3d26a62f",
      "01a6fbeed88c46e1b1b6a239e596cf45",
      "389e55d2f391492aabdb61e06108a40d",
      "94933597c659461f82185c0a8a52dd14",
      "92b15ef11af44f158b9d2c727cf85413",
      "e8961fc344bd4b19965095c358daf7c8",
      "7a81bdb63a12483dbf0ab3c018b4ace5",
      "c2da782500754e27b4f218e557303e33",
      "e6773023f68b475cb9efbed348ee2b25",
      "c1f257f97b6c4f469159dc4670323bbf",
      "d0bfe9f3fad641d7898d6afd1c4d238f",
      "204c8b78d6ba4fd989b1ed95a823af6e",
      "f89db30e79db4a32a52e1e60dc291a57",
      "4bb226f56783499caeb6a2230b2a01ee",
      "aa33da1dc5d74d889d8c84187d224b9d",
      "f83d846678e5441b840e91ae049eb7cb",
      "10b8f96b04b14592b4439ee1a07bc59b",
      "fcfb59f9fee641588af84a24cfdf5bbe",
      "8933e50844034f7387b8fcc67773e2af",
      "36a952ab9b83422f8d057bb2c20bd2c1",
      "b4cbd7617b254a89a17df341948658b7",
      "90b401550fa6443db75e918ad1babeab",
      "5818ada705ce4a778341aee9fea03aec",
      "1dfb2ad3d7ad43e6b5fb672c917cc293",
      "77f98b190a0c42a9b62c7ef88fe71af9",
      "e23a945b9cfe48f5b1dfefea033dd858",
      "737e0e2ef693444a97a81f495e77da7a",
      "ab0bde2199c84de6b55bdc99de552394",
      "4121d3ce51334fe6ac9dbd630eb7d4e1",
      "695d78484f3f4e0aa186fccd47828bc8",
      "42748aef9e7c4d608de439486ba7d7fe",
      "6843729f8aa540a5be0a7d568db19a10",
      "78648ad1f60d48589999fad6eefe53be",
      "d60e5ed0e2fc42868b2784eda51d0293",
      "b89ad627f6794ab08d36af0b04f3c24a",
      "a097111700bf419ea6924e7c373c7c38",
      "605e68b32cc74939a4667694131e3d1b",
      "62e77c2167c9425ead3060ac83a91fa6",
      "0d582439d4e44686a9dc0f8b05b99d29",
      "7a588ad3ce4c4d099cb2c85d27f3572d",
      "d861d3e06d974afe9b6f184050f880cd",
      "22f2fba8da1b45cd9497307ebc8a0f04",
      "e61a7e09f70344d2a277ae791a15d241",
      "fed413f0aea3414098d22e0d5cb9b523",
      "676c05a9cda3412199692c72fa81707e",
      "4d3fd1b7474d44879af54abedda23f71",
      "a4771f35b1fb4fb69398cc7b0a1ff43a",
      "79dedff0e4384e798a8f335900fd8693",
      "f6177cd2756545b29f446d77e7f300ff",
      "2dbf8d3a761847b1bbc8da116782212e",
      "d7afa5d00c6d49eb96e633217ac7dde0",
      "64f300d6c8d64d7ea36c8bd7a9c851d0",
      "481e8a6cf0e44d3fa0d41202dd1ebd68",
      "dcd5ca1b45d14f8491a99fd38464fccf",
      "4626f4d5ad9e467799791ff02af183d5",
      "d5c3128ab8544b53baabd805cf143f48",
      "5a0b001919db4d57b4f220c0289a5705",
      "1ea5f2ab460f4bb3a0b33272d4ce908c",
      "38af85b048544dd1a1e51c57fdfe80e7",
      "c5c0b80963ab46c4be88d3f73d0f9c39",
      "4f60686f8ca04528a3d65808f5a98eee",
      "50d66b101ad642dd8965f10d80991da5",
      "9d0b64a44031480e85654807758d9165",
      "9c23fc28744846b9ae5a39d5596fafda",
      "9fcdec060e424d05afefb76a473d9637",
      "dc59e5d14335434485a00c312eedb756",
      "8e8faab6ec394351baf68ee8a7e72359",
      "80526167f2934be6a3d4d28343b61d77",
      "23d4f1fd1c81478fae25bd572dd441ad",
      "77ae4a5779574dcc8c910b94ff0f8eca",
      "45a214d606d14208b432204bc2bf26d0",
      "766ef98522df460a8e392c496c0ba58b",
      "ba450dc047e94a3498f857ef35a7f5d3",
      "a5cda565fccc48528657b9b21b4c8154",
      "06d2c0d3b4a64619b1cfbf63941182a2",
      "51850ede1c6c411980c7834085614196",
      "1c0de19c80d249a6808941e8a6cb2e82",
      "246a05095f6b4ac0aeb39c344d062986",
      "7adc1c10117e40aaa62cf2e81eaa45c5",
      "58e55558abf74547bcf60406be605106",
      "f9a9241ecbcb44a4be6d6f7b3a49733e",
      "07bf004641c94c1daa158b43f15d0501",
      "e06ce86d009d42b483e9957446d83d09",
      "de5f729e05294d619f5af0174d824de4",
      "108a0c2b192b458f934d5c08f34706a9",
      "fd14ef32f753465f8bd1344a217b19b0",
      "d4ff79d2f1a74ac5b5534a409dbdfcec",
      "27d72753d4d444d3b376aa5a8d02ef1c",
      "ce3d8ae7aeb5452b9c06f4c7eeb3e168",
      "3bda20c2196f47618d31c29ca63f0960",
      "3740e2f01b83462e89996f28c5001815",
      "b1337166460040c79376473a353ade05",
      "32f34c59ef3941bbb2a807ff07dea099",
      "ff5e1efec5da4b3dab94785507186dc1",
      "14eb80dfed424aed80d7f167dca363fc",
      "538da45acb2f416e99236513709be46b",
      "a41d7dba33ec4c97b96df2e33b38ffd8",
      "2010da5513174e98adb7fd8920d0de02",
      "ad85479b3664440d8c74404a3c2722be",
      "e60e8905da834e09a6b8ec156e204792",
      "9fd45909276f48b99d1c5cb9c5146a90",
      "bedc0fc411fa444d82a3189e87de901a",
      "dd43673d6ac64fbc9cfcb61b66563eb8",
      "397015903d184892928d6e10a280ab79",
      "c8178bdaad214131b05c4e4b8c93bea2",
      "0120f0edbe214367a62c438f9216b459",
      "2d982a73e0b742c684f40d40f180d6fc",
      "17784e681d4d420f9f9ff2f47e55b3fa",
      "c6a99ce97db9497c979bad12be79dd36",
      "e97c1d195f1949ed8c8fe7fb5200744c",
      "6e1d035d21c04b76b716387b35b3ebab",
      "7b946fda4b4b4489ab7a379164c528eb",
      "6bbb1409987847f786ef0ab436ffba5a",
      "7bc984cc0593408a98291f367bb8049c",
      "d8c6e3b2214d40bdb2f3de6a530eaf92",
      "d11f72269da64568ba51c64b01e02d66",
      "f65bbbc229914bf798097c24e5bb787e",
      "e93437666d894f69ba9414d287a8662f",
      "8c69c0d608114e158ae203be718e2579",
      "ad1db1fbde204154b632843888ace650",
      "9c912458d0af4a6c9a8135dd5e980595",
      "ba3e88ff491845ce9fae485839ee6569",
      "16ee4604044e4ab7be080ad1e3ac2574",
      "960e8bc2943f4ea384359ca04e6e2b09",
      "77172327cbc24e1e8e662998c85e5cf2",
      "227f4632b15246ceb4e221b5f419fdce",
      "ddfa4af04ade425582e88b236f7ba5ac",
      "b34165b26a244b1c9e6b39ecf66a698c",
      "ddd9147ee6594391b035e74cd5fac408",
      "3359000d3f26448e98fd6d0a5633b5d7",
      "588d176f05944a099fd89129d2c600fa",
      "05c00f4313874e179934546b6a7a110d",
      "75c01f80998940b18504be22076ad20f",
      "5feaefe767784084b5bdbfc894d16db9",
      "dc6613cd4be243238ce128df1fd5b8a2",
      "6cdbaa382c1a4529adb4bc14be30dd5c",
      "ef83c503cb7141c7bca1ccd31af2c0a2"
     ]
    },
    "id": "XgLx-LV2tNgG",
    "outputId": "50faa0e0-5de7-418e-feaf-5da09df69895"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì§ Upload commands (uncomment to use):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved model to https://huggingface.co/pelinbalci/my-qwen-finetuned-lora\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:22<00:00, 22.14s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Merging weights into 16bit:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:48<00:00, 48.41s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/pelinbalci/my-qwen-finetuned`\n",
      "Unsloth: Converting model to GGUF format...\n",
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\rUnsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:16<00:00, 16.17s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:49<00:00, 49.96s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Unsloth: Merge process complete. Saved to `/tmp/unsloth_gguf_ggroe1i7`\nUnsloth: Initial conversion completed! Files: ['qwen2.5-0.5b-instruct.F16.gguf']\nUnsloth: Model files cleanup...\nUnsloth: All GGUF conversions completed successfully!\nUnsloth: Saved Ollama Modelfile to current directory\nUnsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Uploading config.json...\n",
      "Uploading Ollama Modelfile...\n",
      "Unsloth: Successfully uploaded GGUF to https://huggingface.co/pelinbalci/my-qwen-finetuned-gguf\n",
      "Unsloth: Cleaning up temporary files...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'pelinbalci/my-qwen-finetuned-gguf'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Part E: Using Your Model\n",
    "\n",
    "## Option 1: Load LoRA Adapter Later"
   ],
   "metadata": {
    "id": "C1pXF_YstNgG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# How to load your LoRA adapter later\n",
    "print(\"üìù Loading LoRA adapter:\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load base model + your adapter\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./my_lora_adapter\",  # or \"username/model-lora\" from Hub\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Enable inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# ‚úÖ USE CHAT TEMPLATE\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# ‚úÖ DECODE ONLY NEW TOKENS\n",
    "response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "print(\"-------\")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmww-6UytNgG",
    "outputId": "351607ab-6997-4b3b-ae97-a5d9913b89fd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìù Loading LoRA adapter:\n",
      "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "-------\n",
      "Hello! How can I assist you today? Please let me know if there is anything specific or urgent that needs attention.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# How to load your LoRA adapter later\n",
    "print(\"üìù Loading LoRA adapter:\")\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load base model + your adapter\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"pelinbalci/my-qwen-finetuned-lora\",  # or \"username/model-lora\" from Hub\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Enable inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# ‚úÖ USE CHAT TEMPLATE\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# ‚úÖ DECODE ONLY NEW TOKENS\n",
    "response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "print(\"-------\")\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFST-IZ97-lf",
    "outputId": "7e725c53-2d62-4bc8-b863-b2999b6ec81a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìù Loading LoRA adapter:\n",
      "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "-------\n",
      "Hello! How can I assist you today? Please let me know if there is anything specific you would like to ask or discuss.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 2: Use with Ollama\n",
    "\n",
    "Ollama makes it super easy to run models locally."
   ],
   "metadata": {
    "id": "YBD0TU2VtNgG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "ü¶ô Ollama Deployment Explained\n",
    "\n",
    "First, an important note: That cell mixes bash commands with Python, which won't work directly in a notebook. It's meant as documentation/instructions for local deployment.\n",
    "\n",
    "\n",
    "**Step-by-Step Breakdown**\n",
    "\n",
    "1. Install Ollama\n",
    "\n",
    "    bashcurl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "Downloads and installs Ollama on your local machine (Mac, Linux, or Windows WSL). This is a one-time setup.\n",
    "\n",
    "2. Create a Modelfile\n",
    "\n",
    "bash\n",
    "    \n",
    "    cat > Modelfile << 'EOF'\n",
    "    ...\n",
    "    EOF\n",
    "    ```\n",
    "\n",
    "This creates a configuration file that tells Ollama how to use your model:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| `FROM ./path/to/model.gguf` | Points to your GGUF model file |\n",
    "| `TEMPLATE` | Defines the chat format (must match your model's training format) |\n",
    "| `PARAMETER stop` | Tokens that signal end of response |\n",
    "| `PARAMETER temperature` | Controls randomness (0.7 = balanced) |\n",
    "\n",
    "**3. The Template Explained**\n",
    "\n",
    "For Qwen2.5, the chat format uses ChatML style:\n",
    "\n",
    "\n",
    "    ```\n",
    "    <|im_start|>system\n",
    "    You are a helpful assistant.<|im_end|>\n",
    "    <|im_start|>user\n",
    "    Hello!<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    Hi there!<|im_end|>\n",
    "\n",
    "The template in the Modelfile maps Ollama's variables to this format.\n",
    "\n",
    "4. Create & Run\n",
    "\n",
    "bash\n",
    "\n",
    "    ollama create my-qwen -f Modelfile  # Registers the model\n",
    "    ollama run my-qwen \"What is Python?\"  # Runs inference\n"
   ],
   "metadata": {
    "id": "uZgVDKLLAWtn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìù NOTE: These are terminal commands for LOCAL deployment.\n",
    "   Copy and run them on your own machine, not in Colab.\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "ü¶ô OLLAMA DEPLOYMENT (run these commands in your terminal, not in notebook)\n",
    "\n",
    "# 1. Install Ollama (one-time)\n",
    "curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# 2. Download your GGUF from HuggingFace\n",
    "# Either manually or:\n",
    "huggingface-cli download pelinbalci/my-qwen-finetuned-gguf --local-dir ./my_model\n",
    "\n",
    "# 3. Create Modelfile (save this as a file named 'Modelfile')\n",
    "\"\"\")\n",
    "\n",
    "modelfile_content = '''FROM ./qwen2.5-0.5b-instruct.Q8_0.gguf\n",
    "\n",
    "TEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "{{ end }}{{ if .Prompt }}<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "{{ end }}<|im_start|>assistant\n",
    "{{ .Response }}<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "PARAMETER stop \"<|im_start|>\"\n",
    "PARAMETER stop \"<|im_end|>\"\n",
    "PARAMETER temperature 0.7\n",
    "'''\n",
    "\n",
    "print(modelfile_content)\n",
    "\n",
    "print(\"\"\"\n",
    "# 4. Create and run the model\n",
    "ollama create my-qwen -f Modelfile\n",
    "ollama run my-qwen \"What is Python?\"\n",
    "\n",
    "# 5. Or use the API\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"my-qwen\",\n",
    "  \"prompt\": \"What is machine learning?\"\n",
    "}'\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "Bv8nIYz2tNgG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 3: Use with llama.cpp Directly"
   ],
   "metadata": {
    "id": "s1iDE04RtNgH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "üîß llama.cpp Explained\n",
    "\n",
    "llama.cpp is a C/C++ inference engine for running GGUF models - it's actually what powers Ollama under the hood. It's lower-level but gives you more control.\n",
    "\n",
    "\n",
    "When to Use What\n",
    "| Tool | BEst For |\n",
    "|-----------|---------|\n",
    "|Ollama   |Easy local deployment, quick setup|\n",
    "|llama.cpp  |Maximum control, custom integrations, embedded systems|\n",
    "|vLLM/TGI |Production servers with high throughput|\n",
    "\n",
    "\n",
    "\n",
    "The Commands Explained\n",
    "\n",
    "1. Build llama.cpp\n",
    "\n",
    "bash\n",
    "\n",
    "    git clone https://github.com/ggerganov/llama.cpp\n",
    "    cd llama.cpp\n",
    "    make -j  # Compiles the C++ code\n",
    "\n",
    "2. Direct inference (one-shot)\n",
    "\n",
    "\n",
    "    ./llama-cli -m /path/to/model.gguf \\\n",
    "          -p \"What is machine learning?\" \\\n",
    "          -n 256 \\      # max tokens to generate\n",
    "          --temp 0.7    # temperature\n",
    "\n",
    "\n",
    "3. Server mode (API endpoint)\n",
    "\n",
    "\n",
    "    ./llama-server -m /path/to/model.gguf --port 8080\n"
   ],
   "metadata": {
    "id": "GEz2kwDOB4lK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìù NOTE: These are terminal commands for LOCAL deployment.\n",
    "   Copy and run them on your own machine, not in Colab.\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\")\n",
    "\n",
    "print(\"üîß Using with llama.cpp:\")\n",
    "print(\"\"\"\n",
    "# 1. Clone and build llama.cpp\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "make -j\n",
    "\n",
    "# 2. Run inference (note: 'main' is now 'llama-cli')\n",
    "./llama-cli -m ./qwen2.5-0.5b-instruct.Q8_0.gguf \\\\\n",
    "       -p \"What is machine learning?\" \\\\\n",
    "       -n 256 \\\\\n",
    "       --temp 0.7\n",
    "\n",
    "# 3. Or start a server (note: 'server' is now 'llama-server')\n",
    "./llama-server -m ./qwen2.5-0.5b-instruct.Q8_0.gguf --port 8080\n",
    "\n",
    "# Then call via API:\n",
    "curl http://localhost:8080/completion \\\\\n",
    "     -H \"Content-Type: application/json\" \\\\\n",
    "     -d '{\"prompt\": \"Hello!\", \"n_predict\": 100}'\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "PJ1pfZj4tNgH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Option 4: Python API Server (Simple)"
   ],
   "metadata": {
    "id": "UsYLM3XwtNgH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"üêç Simple Python API Server:\")\n",
    "\n",
    "# server.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model once at startup\n",
    "# Option 1: From local adapter\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     \"./my_lora_adapter\",\n",
    "#     max_seq_length=512,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "\n",
    "# Option 2: From HuggingFace Hub ‚úÖ\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"pelinbalci/my-qwen-finetuned-lora\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    max_tokens: int = 256\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(request: ChatRequest):\n",
    "    messages = [{\"role\": \"user\", \"content\": request.message}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True,\n",
    "        add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=request.max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return {\"response\": response}\n",
    "\n",
    "# Run with: uvicorn server:app --host 0.0.0.0 --port 8000\n",
    "\n",
    "# Test with:\n",
    "# curl -X POST http://localhost:8000/chat \\\\\n",
    "#      -H \"Content-Type: application/json\" \\\\\n",
    "#      -d '{\"message\": \"What is Python?\"}'"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Y0jBYP9tNgH",
    "outputId": "b4e4239f-c8cc-4f8e-f30e-f789183a9488"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üêç Simple Python API Server:\n",
      "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's use it this notebook :)"
   ],
   "metadata": {
    "id": "B3Mcv179DlO8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install fastapi uvicorn nest-asyncio pyngrok -q"
   ],
   "metadata": {
    "id": "s9z9nDCoDnza"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 2: Create and run the server\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from unsloth import FastLanguageModel\n",
    "import threading\n",
    "\n",
    "# Allow nested event loops (needed for Colab)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Load model (use already loaded model if available, or load fresh)\n",
    "print(\"üì¶ Loading model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"pelinbalci/my-qwen-finetuned-lora\",\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"‚úÖ Model ready!\")\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    max_tokens: int = 256\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(request: ChatRequest):\n",
    "    messages = [{\"role\": \"user\", \"content\": request.message}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True,\n",
    "        add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=request.max_tokens,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return {\"response\": response}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"ok\"}\n",
    "\n",
    "# Start server in background thread\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run_server, daemon=True)\n",
    "thread.start()\n",
    "print(\"üöÄ Server running on port 8000\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2ucVJzNDsOj",
    "outputId": "8d39b608-71eb-4dee-fa79-9f4c7ab6f001"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üì¶ Loading model...\n",
      "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ Model ready!\n",
      "üöÄ Server running on port 8000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: Test the endpoint\n",
    "import requests\n",
    "\n",
    "# Test health\n",
    "print(requests.get(\"http://localhost:8000/health\").json())\n",
    "\n",
    "# Test chat\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/chat\",\n",
    "    json={\"message\": \"What is machine learning?\", \"max_tokens\": 100}\n",
    ")\n",
    "print(response.json())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx6T4gtPDus8",
    "outputId": "e646ef47-bf9e-4809-8b77-235c3b4b2c1b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:     127.0.0.1:50386 - \"GET /health HTTP/1.1\" 200 OK\n",
      "{'status': 'ok'}\n",
      "INFO:     127.0.0.1:50396 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "{'response': 'Machine learning is a subfield of artificial intelligence (AI) that involves the development and application of algorithms to enable computers to learn from data and make decisions based on new information. In other words, it is a method of creating intelligent machines that can learn from experience and adapt to new situations.\\n\\nThere are several types of machine learning, including supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model using labeled data, while unsupervised learning involves finding patterns in unl'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 4: Interactive testing\n",
    "test_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Write a haiku about coding\",\n",
    "    \"Explain AI in one sentence\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/chat\",\n",
    "        json={\"message\": prompt, \"max_tokens\": 100}\n",
    "    )\n",
    "    print(f\"üë§ {prompt}\")\n",
    "    print(f\"ü§ñ {response.json()['response']}\")\n",
    "    print(\"-\" * 50)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrepRlESDxBH",
    "outputId": "531d5079-432d-4343-89bf-3e5d1c0680dd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:     127.0.0.1:50410 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "üë§ What is Python?\n",
      "ü§ñ Python is a high-level, general-purpose programming language that was developed in 1991 by Guido van Rossum and first released as an open-source software project in 1995. It has since become one of the most popular programming languages in the world, with over 20 million active users and over 370,000 open source projects.\n",
      "\n",
      "Python is known for its simplicity and readability, making it easy to learn and use for beginners. It has a\n",
      "--------------------------------------------------\n",
      "INFO:     127.0.0.1:41824 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "üë§ Write a haiku about coding\n",
      "ü§ñ Code flows like water,\n",
      "Syntax and logic blend together,\n",
      "Life's adventures in the digital realm.\n",
      "--------------------------------------------------\n",
      "INFO:     127.0.0.1:41826 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "üë§ Explain AI in one sentence\n",
      "ü§ñ Artificial intelligence (AI) refers to the simulation of human intelligence in machines that improve over time through experience and learning, without being explicitly programmed.\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "\n",
    "# Cell 5: Create public URL (optional)\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Get auth token from https://ngrok.com (free account)\n",
    "# ngrok.set_auth_token(\"your_token_here\")\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"üåê Public URL: {public_url}\")\n",
    "print(f\"   Test with: curl -X POST {public_url}/chat -H 'Content-Type: application/json' -d '{{\\\"message\\\": \\\"Hello!\\\"}}'\")\n",
    "\n",
    "\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "254ufCvyD5BI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Part F: Deployment Checklist\n",
    "\n",
    "## Before Deploying\n",
    "\n",
    "```\n",
    "‚ñ° Test model with various inputs\n",
    "‚ñ° Check for hallucinations / errors\n",
    "‚ñ° Evaluate on held-out test set\n",
    "‚ñ° Test edge cases and adversarial inputs\n",
    "‚ñ° Measure latency and throughput\n",
    "‚ñ° Estimate costs (GPU/CPU, memory)\n",
    "```\n",
    "\n",
    "## Choosing a Format\n",
    "\n",
    "| Need | Format | Why |\n",
    "|------|--------|-----|\n",
    "| Share adapter only | LoRA | Tiny, needs base model |\n",
    "| HuggingFace deployment | Merged 16-bit | Full compatibility |\n",
    "| Local/edge deployment | GGUF q4_k_m | Small, fast, CPU-friendly |\n",
    "| Maximum quality | GGUF q8_0 | Best quality, larger |\n",
    "| Minimum size | GGUF q2_k | Smallest, lower quality |\n",
    "\n",
    "## Production Considerations\n",
    "\n",
    "| Aspect | Options |\n",
    "|--------|--------|\n",
    "| Serving | vLLM, TGI, Ollama, llama.cpp |\n",
    "| Scaling | Kubernetes, replicas, load balancing |\n",
    "| Monitoring | Latency, errors, token usage |\n",
    "| Safety | Input filtering, output moderation |"
   ],
   "metadata": {
    "id": "G4IEUxLTtNgH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "1. **Quick Training** with Unsloth (~2 min)\n",
    "2. **Local Inference** ‚Äî simple chat function\n",
    "3. **Saving Formats** ‚Äî LoRA, merged, GGUF\n",
    "4. **HuggingFace Hub** ‚Äî sharing your model\n",
    "5. **Deployment Options** ‚Äî Ollama, llama.cpp, API server\n",
    "\n",
    "## Key Commands\n",
    "\n",
    "```python\n",
    "# Save LoRA\n",
    "model.save_pretrained(\"./adapter\")\n",
    "\n",
    "# Save merged\n",
    "model.save_pretrained_merged(\"./merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "# Save GGUF\n",
    "model.save_pretrained_gguf(\"./gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "# Upload to Hub\n",
    "model.push_to_hub_merged(\"username/model\", tokenizer, save_method=\"merged_16bit\")\n",
    "```\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "WKPbqqtXtNgH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detailed Explanation\n",
    "\n",
    "\n",
    "### 1. LoRA Adapter Only (save_pretrained)\n",
    "\n",
    "\n",
    "\n",
    "        Base Model (494MB) + LoRA Adapter (35MB) = Working Model\n",
    "            ‚Üë Downloaded                ‚Üë Your upload\n",
    "            separately                  (tiny!)\n",
    "\n",
    "**What it saves:** Only the trained adapter weights (the \"diff\" from base model)\n",
    "Size: ~35 MB (your case)\n",
    "**Pros:**\n",
    "- Tiny file size\n",
    "- Fast to upload/download\n",
    "- Can share multiple adapters for same base model\n",
    "\n",
    "**Cons:**\n",
    "- Requires base model at inference time\n",
    "- Slightly more complex loading\n",
    "\n",
    "**Best for:** Sharing on HuggingFace, version control, multiple fine-tunes\n",
    "\n",
    "\n",
    "### 2. Merged 16-bit (save_pretrained_merged)\n",
    "\n",
    "\n",
    "        Base Model + LoRA = Single Complete Model\n",
    "                            ‚Üë Your upload (~1GB)\n",
    "\n",
    "\n",
    "**What it saves:** Base model weights + LoRA merged together as one model\n",
    "\n",
    "**Pros:**\n",
    "- Self-contained, no dependencies\n",
    "- Standard HuggingFace format\n",
    "- Works with any HF-compatible tool model\n",
    "\n",
    "**Cons:**\n",
    "- Much larger file\n",
    "- Loses ability to swap adapters\n",
    "\n",
    "**Best for:**  HuggingFace Inference Endpoints, sharing complete models\n",
    "\n",
    "### 3. GGUF Quantized (save_pretrained_gguf)\n",
    "\n",
    "      Merged Model ‚Üí Quantized & Optimized for CPU/Edge\n",
    "                    ‚Üë Your upload (~531MB for Q8_0)\n",
    "\n",
    "\n",
    "**What it saves:** Quantized model in llama.cpp format\n",
    "\n",
    "**Size:** Depends on quantization method\n",
    "\n",
    "**Pros:**\n",
    "- Runs on CPU (no GPU needed!)\n",
    "- Works with Ollama, llama.cpp\n",
    "- Great for local/edge deployment\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Slight quality loss from quantization\n",
    "- Different ecosystem (not HuggingFace native)\n",
    "\n",
    "**Best for:** Local deployment, Ollama, mobile/edge devices\n",
    "\n",
    "\n",
    "\n",
    "        Want to share fine-tune efficiently?     ‚Üí LoRA\n",
    "        Want full HuggingFace compatibility?     ‚Üí Merged 16-bit  \n",
    "        Want to run locally without GPU?         ‚Üí GGUF\n",
    "        Want all options for users?              ‚Üí Upload all three! ‚úÖ (what you did)\n",
    "\n",
    "\n",
    "\n",
    "| Format | Size |Needs Base Model |GPU Required |Use Case|\n",
    "|--------|--------|--------|--------|--------|\n",
    "|LoRA |35 MB‚úÖ  |Yes | ‚úÖ Yes| Sharing, HF Hub|\n",
    "|Merged| 1 GB| ‚ùå No| ‚úÖ Yes| HF Inference, Cloud|\n",
    "|GGUF |300-530 MB |‚ùå No |‚ùå No| Local, Ollama, Edge|"
   ],
   "metadata": {
    "id": "Q122_V88Fxcc"
   }
  }
 ]
}
