{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3dOLbVjVoJd"
      },
      "source": [
        "# Part 8.5: ExecuTorch Fundamentals\n",
        "\n",
        "## Deploying PyTorch Models to Edge Devices\n",
        "\n",
        "In previous sections, we covered inference and deployment methods for servers and desktops:\n",
        "- **Ollama** — Local desktop/server deployment\n",
        "- **llama.cpp / GGUF** — CPU-optimized, cross-platform\n",
        "- **FastAPI** — API server deployment\n",
        "\n",
        "Now we explore **ExecuTorch** — Meta's solution for deploying PyTorch models to edge devices like mobile phones, smartwatches, IoT sensors, and embedded systems.\n",
        "\n",
        "**Learning Objectives:**\n",
        "1. Understand what ExecuTorch is and the problem it solves\n",
        "2. Learn how ExecuTorch differs from standard PyTorch\n",
        "3. Master the export pipeline: PyTorch → ExecuTorch\n",
        "4. Run inference with the ExecuTorch runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B127PDaVoJl"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. What is ExecuTorch?\n",
        "\n",
        "ExecuTorch is Meta's framework for deploying PyTorch models on **edge devices** — mobile phones, embedded systems, IoT devices, and wearables. It's designed for environments where resources are severely constrained compared to server deployments.\n",
        "\n",
        "### The Problem ExecuTorch Solves\n",
        "\n",
        "You've deployed models to servers and desktops. But what about running inference on:\n",
        "- A smartphone with 4GB RAM\n",
        "- A smartwatch with 512MB RAM\n",
        "- An IoT sensor with 128MB RAM\n",
        "- A drone or robot with no internet connection\n",
        "\n",
        "These environments have constraints you haven't dealt with yet:\n",
        "\n",
        "| Constraint | Description |\n",
        "|------------|-------------|\n",
        "| **Limited memory** | Can't load a 4GB model |\n",
        "| **Limited compute** | No powerful GPU |\n",
        "| **No network** | Must run fully on-device |\n",
        "| **Battery concerns** | Efficiency matters |\n",
        "| **Binary size** | App size limits on mobile stores |\n",
        "\n",
        "Standard PyTorch wasn't designed for this. It carries dependencies and overhead that work fine on servers but are too heavy for edge devices.\n",
        "\n",
        "### Think of ExecuTorch as:\n",
        "\n",
        "> \"A compiler and runtime that transforms PyTorch models into efficient, self-contained programs that can run on resource-constrained devices without Python.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqVnFuVvVoJm"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. How ExecuTorch Differs from Standard PyTorch\n",
        "\n",
        "### Standard PyTorch Execution\n",
        "\n",
        "When you run a PyTorch model normally:\n",
        "- Python interpreter runs your code line by line\n",
        "- PyTorch dispatches operations dynamically\n",
        "- You can use if/else, loops, dynamic shapes\n",
        "- Requires Python runtime + PyTorch library (~hundreds of MB)\n",
        "\n",
        "### ExecuTorch Execution\n",
        "\n",
        "With ExecuTorch:\n",
        "- Model is compiled ahead-of-time (AOT) to a static graph\n",
        "- Lightweight C++ runtime executes the graph\n",
        "- No Python needed on the target device\n",
        "- Minimal runtime (~1MB possible)\n",
        "\n",
        "### Comparison Table\n",
        "\n",
        "| Aspect | Server/Desktop (Ollama, FastAPI) | Edge (ExecuTorch) |\n",
        "|--------|----------------------------------|-------------------|\n",
        "| Runtime size | Hundreds of MB | ~1MB possible |\n",
        "| Dependencies | Python, CUDA, libraries | Minimal C++ runtime |\n",
        "| Compilation | JIT or eager execution | Ahead-of-time (AOT) |\n",
        "| Optimization | General purpose | Device-specific |\n",
        "| Target | x86/CUDA servers | ARM, NPU, DSP, custom silicon |\n",
        "\n",
        "### The Key Trade-off\n",
        "\n",
        "**You're trading flexibility for efficiency.** A server can JIT compile and adapt; an edge device needs everything pre-decided and optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_0LyXEpVoJo"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. The ExecuTorch Pipeline\n",
        "\n",
        "Here's the journey a model takes from PyTorch to edge deployment:\n",
        "\n",
        "```\n",
        "PyTorch Model (nn.Module)\n",
        "        ↓\n",
        "   torch.export()\n",
        "        ↓\n",
        "  Exported Program (ATen dialect)\n",
        "        ↓\n",
        "   to_edge()\n",
        "        ↓\n",
        "  Edge Program (Edge dialect)\n",
        "        ↓\n",
        "   to_executorch()\n",
        "        ↓\n",
        "  .pte file (ExecuTorch Program)\n",
        "        ↓\n",
        "  ExecuTorch Runtime (C++)\n",
        "```\n",
        "\n",
        "### Step-by-Step Explanation\n",
        "\n",
        "**1. torch.export()** — Captures your model as a static graph. Unlike regular PyTorch which executes dynamically, this freezes the computation into a fixed structure. This is necessary because edge devices can't do dynamic Python execution.\n",
        "\n",
        "**2. ATen Dialect** — The exported program uses PyTorch's ATen operators (~2000 operations). Too many for lean edge deployment.\n",
        "\n",
        "**3. to_edge()** — Converts to Edge dialect, a smaller standardized set of operators designed for portability across edge devices.\n",
        "\n",
        "**4. to_executorch()** — Final compilation step. Produces a `.pte` file (PyTorch ExecuTorch) — a serialized binary ready for the runtime.\n",
        "\n",
        "**5. ExecuTorch Runtime** — A lightweight C++ runtime that loads and executes `.pte` files. No Python needed on the device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9__uLan9VoJo"
      },
      "source": [
        "### Key Concept: Delegates\n",
        "\n",
        "ExecuTorch can delegate operations to specialized hardware:\n",
        "\n",
        "| Delegate | Target Hardware |\n",
        "|----------|----------------|\n",
        "| **XNNPACK** | Optimized CPU kernels for ARM/x86 |\n",
        "| **CoreML** | Apple's Neural Engine (iOS/macOS) |\n",
        "| **Qualcomm QNN** | Snapdragon NPUs |\n",
        "| **Vulkan** | Mobile GPU compute |\n",
        "| **Custom backends** | Your own hardware |\n",
        "\n",
        "This means the same `.pte` file can route different operations to different accelerators for maximum efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KYOI_G7VoJp"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Code Comparison: PyTorch vs ExecuTorch\n",
        "\n",
        "Let's see the difference between standard PyTorch execution and ExecuTorch compilation with a simple example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5-Zd634VoJq"
      },
      "source": [
        "### Standard PyTorch: How You Normally Run Models\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(10, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(32, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Create model and input\n",
        "model = SimpleModel()\n",
        "model.eval()\n",
        "sample_input = torch.randn(1, 10)\n",
        "\n",
        "# Run inference — PyTorch executes dynamically\n",
        "output = model(sample_input)\n",
        "print(output.shape)  # torch.Size([1, 5])\n",
        "```\n",
        "\n",
        "**What happens under the hood:**\n",
        "- Python interpreter runs your code line by line\n",
        "- PyTorch dispatches operations dynamically\n",
        "- Flexible — you can use if/else, loops, dynamic shapes\n",
        "- Requires Python runtime + PyTorch library (~hundreds of MB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnreZikxVoJs"
      },
      "source": [
        "### ExecuTorch: Compiled for Edge Deployment\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.export import export\n",
        "from executorch.exir import to_edge\n",
        "\n",
        "# Same model as before\n",
        "class SimpleModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(10, 32)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(32, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleModel()\n",
        "model.eval()\n",
        "\n",
        "# Step 1: Export — capture the model as a static graph\n",
        "sample_input = (torch.randn(1, 10),)\n",
        "exported_program = export(model, sample_input)\n",
        "\n",
        "# Step 2: Convert to Edge dialect\n",
        "edge_program = to_edge(exported_program)\n",
        "\n",
        "# Step 3: Convert to ExecuTorch format\n",
        "executorch_program = edge_program.to_executorch()\n",
        "\n",
        "# Step 4: Save as .pte file\n",
        "with open(\"simple_model.pte\", \"wb\") as f:\n",
        "    f.write(executorch_program.buffer)\n",
        "\n",
        "print(\"Model exported to simple_model.pte\")\n",
        "```\n",
        "\n",
        "**What happens:**\n",
        "- `export()` traces your model and captures a fixed computation graph\n",
        "- `to_edge()` converts operations to a portable edge-friendly format\n",
        "- `to_executorch()` compiles everything into a binary blob\n",
        "- The `.pte` file contains everything needed — no Python required to run it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftfBaiOVoJs"
      },
      "source": [
        "### Running the .pte File\n",
        "\n",
        "**On your development machine (Python, for testing):**\n",
        "\n",
        "```python\n",
        "from executorch.runtime import Runtime\n",
        "\n",
        "# Load the compiled model\n",
        "runtime = Runtime.get()\n",
        "program = runtime.load_program(\"simple_model.pte\")\n",
        "method = program.load_method(\"forward\")\n",
        "\n",
        "# Run inference\n",
        "input_tensor = torch.randn(1, 10)\n",
        "output = method.execute([input_tensor])\n",
        "print(output[0].shape)  # Same result as PyTorch\n",
        "```\n",
        "\n",
        "**On an edge device (C++, no Python):**\n",
        "\n",
        "```cpp\n",
        "#include <executorch/runtime/executor/program.h>\n",
        "\n",
        "// Load .pte file\n",
        "auto program = Program::load(\"simple_model.pte\");\n",
        "auto method = program->load_method(\"forward\");\n",
        "\n",
        "// Prepare input and run\n",
        "method->execute();\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnyOQc_KVoJt"
      },
      "source": [
        "### Visual Summary\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────┐\n",
        "│                   STANDARD PYTORCH                       │\n",
        "│                                                         │\n",
        "│   Python Script → PyTorch Runtime → Dynamic Execution   │\n",
        "│                                                         │\n",
        "│   Requires: Python + PyTorch (~500MB+)                  │\n",
        "│   Runs on: Servers, desktops, laptops                   │\n",
        "└─────────────────────────────────────────────────────────┘\n",
        "\n",
        "┌─────────────────────────────────────────────────────────┐\n",
        "│                     EXECUTORCH                           │\n",
        "│                                                         │\n",
        "│   Python Script                                         │\n",
        "│        ↓ export()                                       │\n",
        "│   Exported Program                                      │\n",
        "│        ↓ to_edge()                                      │\n",
        "│   Edge Program                                          │\n",
        "│        ↓ to_executorch()                                │\n",
        "│   .pte file ──→ Lightweight C++ Runtime → Execution    │\n",
        "│                                                         │\n",
        "│   Requires: Just the .pte file + tiny runtime (~1MB)   │\n",
        "│   Runs on: Phones, watches, IoT, embedded devices      │\n",
        "└─────────────────────────────────────────────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRxxzYY2VoJu"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Hands-On: Complete ExecuTorch Pipeline\n",
        "\n",
        "Now let's run through the complete pipeline with working code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW8JjsMxVoJv"
      },
      "source": [
        "### 5.1 Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coV6GWh5VoJv",
        "outputId": "042967e9-1280-492e-841c-a959a0364ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.7/542.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install ExecuTorch\n",
        "# Note: This may take a few minutes\n",
        "!pip install executorch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6th1V3mVoJx",
        "outputId": "7a7f8212-74ad-4c65-ba7c-192a20b7a1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cpu for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ExecuTorch imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Verify installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "try:\n",
        "    from torch.export import export\n",
        "    from executorch.exir import to_edge\n",
        "    print(\"✓ ExecuTorch imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Import error: {e}\")\n",
        "    print(\"\\nTroubleshooting: ExecuTorch requires specific PyTorch versions.\")\n",
        "    print(\"Try: pip install executorch torch==2.4.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSz7XamVVoJx"
      },
      "source": [
        "### 5.2 Create a Simple Model\n",
        "\n",
        "We'll use a small classifier — complex enough to be meaningful, simple enough to understand completely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfGfgiMrVoJy",
        "outputId": "8f03355d-c442-4c4f-c7fb-2ea3564e0d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created successfully!\n",
            "Input shape:  torch.Size([1, 20])\n",
            "Output shape: torch.Size([1, 4])\n",
            "Output:       tensor([[ 0.0738, -0.0361,  0.0600,  0.1681]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A small classifier for demonstration.\n",
        "    Input: 20 features\n",
        "    Output: 4 classes\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Create and set to eval mode\n",
        "model = TinyClassifier()\n",
        "model.eval()\n",
        "\n",
        "# Test with sample input\n",
        "sample_input = torch.randn(1, 20)\n",
        "output = model(sample_input)\n",
        "\n",
        "print(f\"Model created successfully!\")\n",
        "print(f\"Input shape:  {sample_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output:       {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvjzuy-9VoJy"
      },
      "source": [
        "### 5.3 Export the Model\n",
        "\n",
        "This is where we transition from dynamic PyTorch to a static graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TROKTo2YVoJz",
        "outputId": "727c98ad-0497-4b4f-c0c8-f0109a9206d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Export successful!\n",
            "Type: ExportedProgram\n"
          ]
        }
      ],
      "source": [
        "from torch.export import export\n",
        "\n",
        "# Define example input for tracing\n",
        "# Note: Input must be a tuple\n",
        "example_inputs = (torch.randn(1, 20),)\n",
        "\n",
        "# Export the model\n",
        "exported_program = export(model, example_inputs)\n",
        "\n",
        "print(\"✓ Export successful!\")\n",
        "print(f\"Type: {type(exported_program).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cSMsU90VoJz"
      },
      "source": [
        "**What just happened?**\n",
        "\n",
        "`torch.export()` traced through your model with the example input and captured every operation as a static graph. No more dynamic Python — just a fixed sequence of tensor operations.\n",
        "\n",
        "Let's inspect what was captured:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkyotYzGVoJ0",
        "outputId": "e41751a4-78bf-4e6c-e97d-b41409c0b547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported Graph:\n",
            "==================================================\n",
            "graph():\n",
            "    %p_layers_0_weight : [num_users=1] = placeholder[target=p_layers_0_weight]\n",
            "    %p_layers_0_bias : [num_users=1] = placeholder[target=p_layers_0_bias]\n",
            "    %p_layers_2_weight : [num_users=1] = placeholder[target=p_layers_2_weight]\n",
            "    %p_layers_2_bias : [num_users=1] = placeholder[target=p_layers_2_bias]\n",
            "    %p_layers_4_weight : [num_users=1] = placeholder[target=p_layers_4_weight]\n",
            "    %p_layers_4_bias : [num_users=1] = placeholder[target=p_layers_4_bias]\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_layers_0_weight, %p_layers_0_bias), kwargs = {})\n",
            "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%linear,), kwargs = {})\n",
            "    %linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%relu, %p_layers_2_weight, %p_layers_2_bias), kwargs = {})\n",
            "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%linear_1,), kwargs = {})\n",
            "    %linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%relu_1, %p_layers_4_weight, %p_layers_4_bias), kwargs = {})\n",
            "    return (linear_2,)\n"
          ]
        }
      ],
      "source": [
        "# View the exported graph\n",
        "print(\"Exported Graph:\")\n",
        "print(\"=\" * 50)\n",
        "print(exported_program.graph_module.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHrfFx4aVoJ0"
      },
      "source": [
        "You'll see operations like `torch.ops.aten.linear.default` and `torch.ops.aten.relu.default`. This is the **ATen dialect** — PyTorch's internal operator representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L6496nGVoJ1"
      },
      "source": [
        "### 5.4 Convert to Edge Dialect\n",
        "\n",
        "Now we convert to a more portable format optimized for edge devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFCywDS2VoJ1",
        "outputId": "7b65957e-4607-47ea-a4e5-e5674dc711cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Edge conversion successful!\n",
            "Type: EdgeProgramManager\n"
          ]
        }
      ],
      "source": [
        "from executorch.exir import to_edge\n",
        "\n",
        "# Convert to edge program\n",
        "edge_program = to_edge(exported_program)\n",
        "\n",
        "print(\"✓ Edge conversion successful!\")\n",
        "print(f\"Type: {type(edge_program).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h00KZxppVoJ1"
      },
      "source": [
        "**Why this step?**\n",
        "\n",
        "ATen has ~2000 operators. Edge dialect standardizes these into a smaller, portable set that can run consistently across different edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10qXAzfVVoJ2"
      },
      "source": [
        "### 5.5 Compile to ExecuTorch Format\n",
        "\n",
        "Final compilation step — creating the `.pte` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oX1T6EjVoJ2",
        "outputId": "6c634951-dfab-44a5-8cf2-55136a232ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ExecuTorch compilation successful!\n",
            "Buffer size: 17,040 bytes\n",
            "Buffer size: 16.64 KB\n"
          ]
        }
      ],
      "source": [
        "# Compile to ExecuTorch\n",
        "executorch_program = edge_program.to_executorch()\n",
        "\n",
        "print(\"✓ ExecuTorch compilation successful!\")\n",
        "print(f\"Buffer size: {len(executorch_program.buffer):,} bytes\")\n",
        "print(f\"Buffer size: {len(executorch_program.buffer)/1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByFWjcOuVoJ3"
      },
      "source": [
        "Notice the size — this tiny model compiles to just a few kilobytes. Compare that to the full PyTorch model which requires the entire PyTorch runtime (~hundreds of MB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1tyAdN0VoJ3"
      },
      "source": [
        "### 5.6 Save the .pte File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gme3zKp3VoJ4",
        "outputId": "92016da5-ddc1-4785-cd70-9f96cb0514de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved to: tiny_classifier.pte\n",
            "  File size: 17,040 bytes (16.64 KB)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Save to file\n",
        "pte_path = \"tiny_classifier.pte\"\n",
        "\n",
        "with open(pte_path, \"wb\") as f:\n",
        "    f.write(executorch_program.buffer)\n",
        "\n",
        "# Verify file\n",
        "file_size = os.path.getsize(pte_path)\n",
        "print(f\"✓ Saved to: {pte_path}\")\n",
        "print(f\"  File size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF--x6x4VoJ5"
      },
      "source": [
        "### 5.7 Run Inference with ExecuTorch Runtime\n",
        "\n",
        "Now let's load and run our compiled model using the ExecuTorch runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4QtdPpBVoJ5",
        "outputId": "a2c4255d-24d5-46a6-cea0-58f2b2a0b464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[program.cpp:153] InternalConsistency verification requested but not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded successfully!\n",
            "\n",
            "Inference Results:\n",
            "Input shape:  torch.Size([1, 20])\n",
            "Output shape: torch.Size([1, 4])\n",
            "Output:       tensor([[-0.0303,  0.1302,  0.0904,  0.1285]])\n"
          ]
        }
      ],
      "source": [
        "from executorch.runtime import Runtime\n",
        "\n",
        "# Load the program\n",
        "runtime = Runtime.get()\n",
        "program = runtime.load_program(pte_path)\n",
        "\n",
        "# Load the forward method\n",
        "method = program.load_method(\"forward\")\n",
        "\n",
        "print(\"✓ Model loaded successfully!\")\n",
        "\n",
        "# Create test input\n",
        "test_input = torch.randn(1, 20)\n",
        "\n",
        "# Run inference\n",
        "outputs = method.execute([test_input])\n",
        "\n",
        "print(f\"\\nInference Results:\")\n",
        "print(f\"Input shape:  {test_input.shape}\")\n",
        "print(f\"Output shape: {outputs[0].shape}\")\n",
        "print(f\"Output:       {outputs[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygFsOG6TVoJ6"
      },
      "source": [
        "### 5.8 Verify Results Match\n",
        "\n",
        "Critical check — does ExecuTorch produce the same results as PyTorch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqLyCE5fVoJ7",
        "outputId": "78120230-de68-44c0-a74d-2b1e9cc22f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison: PyTorch vs ExecuTorch\n",
            "==================================================\n",
            "PyTorch output:     [0.00625398 0.0099101  0.14916544 0.15452597]\n",
            "ExecuTorch output:  [0.00625396 0.0099101  0.14916542 0.15452595]\n",
            "\n",
            "✓ Outputs match: True\n",
            "The ExecuTorch model produces identical results to PyTorch!\n"
          ]
        }
      ],
      "source": [
        "# Run same input through both\n",
        "test_input = torch.randn(1, 20)\n",
        "\n",
        "# PyTorch inference\n",
        "with torch.no_grad():\n",
        "    pytorch_output = model(test_input)\n",
        "\n",
        "# ExecuTorch inference\n",
        "executorch_output = method.execute([test_input])[0]\n",
        "\n",
        "# Compare\n",
        "print(\"Comparison: PyTorch vs ExecuTorch\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"PyTorch output:     {pytorch_output.numpy().flatten()}\")\n",
        "print(f\"ExecuTorch output:  {executorch_output.numpy().flatten()}\")\n",
        "\n",
        "# Check if close (small numerical differences are normal due to floating point)\n",
        "are_close = torch.allclose(pytorch_output, executorch_output, atol=1e-5)\n",
        "print(f\"\\n✓ Outputs match: {are_close}\")\n",
        "\n",
        "if are_close:\n",
        "    print(\"The ExecuTorch model produces identical results to PyTorch!\")\n",
        "else:\n",
        "    max_diff = torch.max(torch.abs(pytorch_output - executorch_output)).item()\n",
        "    print(f\"Maximum difference: {max_diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK3ViBWPVoJ7"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Complete Pipeline Summary\n",
        "\n",
        "Here's the entire ExecuTorch workflow in one place:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfJkks8lVoJ7",
        "outputId": "4a0e38e1-31e2-48c8-f567-1f9eae558f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Model created\n",
            "Step 2: Model exported (ATen dialect)\n",
            "Step 3: Converted to Edge dialect\n",
            "Step 4: Compiled to ExecuTorch format\n",
            "Step 5: Saved to model.pte (16.64 KB)\n",
            "\n",
            "==================================================\n",
            "✓ Model ready for edge deployment!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPLETE EXECUTORCH PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.export import export\n",
        "from executorch.exir import to_edge\n",
        "import os\n",
        "\n",
        "# 1. DEFINE MODEL\n",
        "class TinyClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# 2. PREPARE MODEL\n",
        "model = TinyClassifier()\n",
        "model.eval()\n",
        "print(\"Step 1: Model created\")\n",
        "\n",
        "# 3. EXPORT → EDGE → EXECUTORCH\n",
        "example_inputs = (torch.randn(1, 20),)\n",
        "\n",
        "exported = export(model, example_inputs)\n",
        "print(\"Step 2: Model exported (ATen dialect)\")\n",
        "\n",
        "edge = to_edge(exported)\n",
        "print(\"Step 3: Converted to Edge dialect\")\n",
        "\n",
        "et_program = edge.to_executorch()\n",
        "print(\"Step 4: Compiled to ExecuTorch format\")\n",
        "\n",
        "# 4. SAVE\n",
        "output_path = \"model.pte\"\n",
        "with open(output_path, \"wb\") as f:\n",
        "    f.write(et_program.buffer)\n",
        "\n",
        "file_size = os.path.getsize(output_path)\n",
        "print(f\"Step 5: Saved to {output_path} ({file_size/1024:.2f} KB)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"✓ Model ready for edge deployment!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJQYUDZAVoJ8"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **ExecuTorch** is Meta's framework for deploying PyTorch models to edge devices (phones, IoT, embedded systems)\n",
        "\n",
        "2. **The Pipeline:**\n",
        "   - `export()` — Captures dynamic PyTorch as a static graph\n",
        "   - `to_edge()` — Converts to portable edge operators\n",
        "   - `to_executorch()` — Compiles to final binary format\n",
        "   - `.pte` file — Self-contained, ready for edge runtime\n",
        "\n",
        "3. **Key Trade-off:** You sacrifice flexibility (dynamic execution) for efficiency (small runtime, optimized for constrained devices)\n",
        "\n",
        "4. **Runtime:** No Python needed on the target device — just the lightweight C++ runtime\n",
        "\n",
        "### Comparison with Other Deployment Methods\n",
        "\n",
        "| Method | Target | Runtime Size | Use Case |\n",
        "|--------|--------|--------------|----------|\n",
        "| Ollama | Desktop/Server | ~100MB+ | Local chatbots |\n",
        "| GGUF/llama.cpp | Desktop | ~10-50MB | CPU inference |\n",
        "| FastAPI | Cloud | Variable | API endpoints |\n",
        "| **ExecuTorch** | **Edge** | **~1MB** | **Mobile apps, IoT** |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "In Part 8.6, we'll:\n",
        "- Compare GGUF vs ExecuTorch in detail\n",
        "- Apply ExecuTorch to a real SLM (Small Language Model)\n",
        "- Explore quantization for edge deployment\n",
        "- Handle LLM-specific challenges (tokenization, generation loops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvnM3kkIVoJ9"
      },
      "source": [
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- [ExecuTorch Documentation](https://pytorch.org/executorch/stable/index.html)\n",
        "- [PyTorch Export Documentation](https://pytorch.org/docs/stable/export.html)\n",
        "- [ExecuTorch GitHub Repository](https://github.com/pytorch/executorch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}