{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3dOLbVjVoJd"
      },
      "source": [
        "# Part 8.5: ExecuTorch Fundamentals\n",
        "\n",
        "## Deploying PyTorch Models to Edge Devices\n",
        "\n",
        "In previous sections, we covered inference and deployment methods for servers and desktops:\n",
        "- **Ollama** â€” Local desktop/server deployment\n",
        "- **llama.cpp / GGUF** â€” CPU-optimized, cross-platform\n",
        "- **FastAPI** â€” API server deployment\n",
        "\n",
        "Now we explore **ExecuTorch** â€” Meta's solution for deploying PyTorch models to edge devices like mobile phones, smartwatches, IoT sensors, and embedded systems.\n",
        "\n",
        "**Learning Objectives:**\n",
        "1. Understand what ExecuTorch is and the problem it solves\n",
        "2. Learn how ExecuTorch differs from standard PyTorch\n",
        "3. Master the export pipeline: PyTorch â†’ ExecuTorch\n",
        "4. Run inference with the ExecuTorch runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B127PDaVoJl"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. What is ExecuTorch?\n",
        "\n",
        "ExecuTorch is an on-device (edge) inference runtime developed by Meta.\n",
        "\n",
        "It is designed to run PyTorch models efficiently on mobile and edge devices, such as:\n",
        "\n",
        "ğŸ“± Phones (Android / iOS)\n",
        "\n",
        "ğŸ’» Laptops\n",
        "\n",
        "ğŸ§  Embedded & edge hardware\n",
        "\n",
        "ğŸ§© Custom accelerators\n",
        "\n",
        "ğŸ‘‰ Think of it as â€œPyTorch for deployment on-deviceâ€, not for training.\n",
        "\n",
        "\n",
        "### The Problem ExecuTorch Solves\n",
        "\n",
        "You've deployed models to servers and desktops. But what about running inference on:\n",
        "- A smartphone with 4GB RAM\n",
        "- A smartwatch with 512MB RAM\n",
        "- An IoT sensor with 128MB RAM\n",
        "- A drone or robot with no internet connection\n",
        "\n",
        "These environments have constraints you haven't dealt with yet:\n",
        "\n",
        "| Constraint | Description |\n",
        "|------------|-------------|\n",
        "| **Limited memory** | Can't load a 4GB model |\n",
        "| **Limited compute** | No powerful GPU |\n",
        "| **No network** | Must run fully on-device |\n",
        "| **Battery concerns** | Efficiency matters |\n",
        "| **Binary size** | App size limits on mobile stores |\n",
        "\n",
        "Standard PyTorch wasn't designed for this. It carries dependencies and overhead that work fine on servers but are too heavy for edge devices.\n",
        "\n",
        "PyTorch is amazing for training, but:\n",
        "\n",
        "âŒ Heavy for mobile\n",
        "\n",
        "âŒ Large runtime\n",
        "\n",
        "âŒ Not optimized for on-device inference\n",
        "\n",
        "âŒ Hard to ship models inside apps\n",
        "\n",
        "**ExecuTorch solves this by:**\n",
        "\n",
        "Exporting models from PyTorch\n",
        "\n",
        "Running them in a lightweight, optimized runtime\n",
        "\n",
        "Supporting quantization & operator fusion\n",
        "\n",
        "Enabling private, offline inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqVnFuVvVoJm"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. How ExecuTorch Differs from Standard PyTorch\n",
        "\n",
        "### Standard PyTorch Execution\n",
        "\n",
        "When you run a PyTorch model normally:\n",
        "- Python interpreter runs your code line by line\n",
        "- PyTorch dispatches operations dynamically\n",
        "- You can use if/else, loops, dynamic shapes\n",
        "- Requires Python runtime + PyTorch library (~hundreds of MB)\n",
        "\n",
        "### ExecuTorch Execution\n",
        "\n",
        "With ExecuTorch:\n",
        "- Model is compiled ahead-of-time (AOT) to a static graph\n",
        "- Lightweight C++ runtime executes the graph\n",
        "- No Python needed on the target device\n",
        "- Minimal runtime (~1MB possible)\n",
        "\n",
        "### Comparison Table\n",
        "\n",
        "| Aspect | Server/Desktop (Ollama, FastAPI) | Edge (ExecuTorch) |\n",
        "|--------|----------------------------------|-------------------|\n",
        "| Runtime size | Hundreds of MB | ~1MB possible |\n",
        "| Dependencies | Python, CUDA, libraries | Minimal C++ runtime |\n",
        "| Compilation | JIT or eager execution | Ahead-of-time (AOT) |\n",
        "| Optimization | General purpose | Device-specific |\n",
        "| Target | x86/CUDA servers | ARM, NPU, DSP, custom silicon |\n",
        "\n",
        "### The Key Trade-off\n",
        "\n",
        "**You're trading flexibility for efficiency.** A server can JIT compile and adapt; an edge device needs everything pre-decided and optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_0LyXEpVoJo"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. The ExecuTorch Pipeline\n",
        "\n",
        "ExecuTorch does NOT train models.\n",
        "It only runs already-trained models efficiently.\n",
        "\n",
        "Here's the journey a model takes from PyTorch to edge deployment:\n",
        "\n",
        "```\n",
        "PyTorch Model (nn.Module)\n",
        "        â†“\n",
        "   torch.export()\n",
        "        â†“\n",
        "  Exported Program (ATen dialect)\n",
        "        â†“\n",
        "   to_edge()\n",
        "        â†“\n",
        "  Edge Program (Edge dialect)\n",
        "        â†“\n",
        "   to_executorch()\n",
        "        â†“\n",
        "  .pte file (ExecuTorch Program)\n",
        "        â†“\n",
        "  ExecuTorch Runtime (C++)\n",
        "```\n",
        "\n",
        "### Step-by-Step Explanation\n",
        "\n",
        "**1. torch.export()** â€” Captures your model as a static graph. Unlike regular PyTorch which executes dynamically, this freezes the computation into a fixed structure. This is necessary because edge devices can't do dynamic Python execution.\n",
        "\n",
        "**2. ATen Dialect** â€” The exported program uses PyTorch's ATen operators (~2000 operations). Too many for lean edge deployment.\n",
        "\n",
        "**3. to_edge()** â€” Converts to Edge dialect, a smaller standardized set of operators designed for portability across edge devices.\n",
        "\n",
        "**4. to_executorch()** â€” Final compilation step. Produces a `.pte` file (PyTorch ExecuTorch) â€” a serialized binary ready for the runtime.\n",
        "\n",
        "**5. ExecuTorch Runtime** â€” A lightweight C++ runtime that loads and executes `.pte` files. No Python needed on the device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9__uLan9VoJo"
      },
      "source": [
        "### Key Concept: Delegates\n",
        "\n",
        "ExecuTorch can delegate operations to specialized hardware:\n",
        "\n",
        "| Delegate | Target Hardware |\n",
        "|----------|----------------|\n",
        "| **XNNPACK** | Optimized CPU kernels for ARM/x86 |\n",
        "| **CoreML** | Apple's Neural Engine (iOS/macOS) |\n",
        "| **Qualcomm QNN** | Snapdragon NPUs |\n",
        "| **Vulkan** | Mobile GPU compute |\n",
        "| **Custom backends** | Your own hardware |\n",
        "\n",
        "This means the same `.pte` file can route different operations to different accelerators for maximum efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KYOI_G7VoJp"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Code Comparison: PyTorch vs ExecuTorch\n",
        "\n",
        "Let's see the difference between standard PyTorch execution and ExecuTorch compilation with a simple example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5-Zd634VoJq"
      },
      "source": [
        "### Standard PyTorch: How You Normally Run Models\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(10, 32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(32, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Create model and input\n",
        "model = SimpleModel()\n",
        "model.eval()\n",
        "sample_input = torch.randn(1, 10)\n",
        "\n",
        "# Run inference â€” PyTorch executes dynamically\n",
        "output = model(sample_input)\n",
        "print(output.shape)  # torch.Size([1, 5])\n",
        "```\n",
        "\n",
        "**What happens under the hood:**\n",
        "- Python interpreter runs your code line by line\n",
        "- PyTorch dispatches operations dynamically\n",
        "- Flexible â€” you can use if/else, loops, dynamic shapes\n",
        "- Requires Python runtime + PyTorch library (~hundreds of MB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnreZikxVoJs"
      },
      "source": [
        "### ExecuTorch: Compiled for Edge Deployment\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.export import export\n",
        "from executorch.exir import to_edge\n",
        "\n",
        "# Same model as before\n",
        "class SimpleModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(10, 32)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(32, 5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleModel()\n",
        "model.eval()\n",
        "\n",
        "# Step 1: Export â€” capture the model as a static graph\n",
        "sample_input = (torch.randn(1, 10),)\n",
        "exported_program = export(model, sample_input)\n",
        "\n",
        "# Step 2: Convert to Edge dialect\n",
        "edge_program = to_edge(exported_program)\n",
        "\n",
        "# Step 3: Convert to ExecuTorch format\n",
        "executorch_program = edge_program.to_executorch()\n",
        "\n",
        "# Step 4: Save as .pte file\n",
        "with open(\"simple_model.pte\", \"wb\") as f:\n",
        "    f.write(executorch_program.buffer)\n",
        "\n",
        "print(\"Model exported to simple_model.pte\")\n",
        "```\n",
        "\n",
        "**What happens:**\n",
        "- `export()` traces your model and captures a fixed computation graph\n",
        "- `to_edge()` converts operations to a portable edge-friendly format\n",
        "- `to_executorch()` compiles everything into a binary blob\n",
        "- The `.pte` file contains everything needed â€” no Python required to run it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vftfBaiOVoJs"
      },
      "source": [
        "### Running the .pte File\n",
        "\n",
        "**On your development machine (Python, for testing):**\n",
        "\n",
        "```python\n",
        "from executorch.runtime import Runtime\n",
        "\n",
        "# Load the compiled model\n",
        "runtime = Runtime.get()\n",
        "program = runtime.load_program(\"simple_model.pte\")\n",
        "method = program.load_method(\"forward\")\n",
        "\n",
        "# Run inference\n",
        "input_tensor = torch.randn(1, 10)\n",
        "output = method.execute([input_tensor])\n",
        "print(output[0].shape)  # Same result as PyTorch\n",
        "```\n",
        "\n",
        "**On an edge device (C++, no Python):**\n",
        "\n",
        "```cpp\n",
        "#include <executorch/runtime/executor/program.h>\n",
        "\n",
        "// Load .pte file\n",
        "auto program = Program::load(\"simple_model.pte\");\n",
        "auto method = program->load_method(\"forward\");\n",
        "\n",
        "// Prepare input and run\n",
        "method->execute();\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnyOQc_KVoJt"
      },
      "source": [
        "### Visual Summary\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                   STANDARD PYTORCH                       â”‚\n",
        "â”‚                                                         â”‚\n",
        "â”‚   Python Script â†’ PyTorch Runtime â†’ Dynamic Execution   â”‚\n",
        "â”‚                                                         â”‚\n",
        "â”‚   Requires: Python + PyTorch (~500MB+)                  â”‚\n",
        "â”‚   Runs on: Servers, desktops, laptops                   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     EXECUTORCH                           â”‚\n",
        "â”‚                                                         â”‚\n",
        "â”‚   Python Script                                         â”‚\n",
        "â”‚        â†“ export()                                       â”‚\n",
        "â”‚   Exported Program                                      â”‚\n",
        "â”‚        â†“ to_edge()                                      â”‚\n",
        "â”‚   Edge Program                                          â”‚\n",
        "â”‚        â†“ to_executorch()                                â”‚\n",
        "â”‚   .pte file â”€â”€â†’ Lightweight C++ Runtime â†’ Execution    â”‚\n",
        "â”‚                                                         â”‚\n",
        "â”‚   Requires: Just the .pte file + tiny runtime (~1MB)   â”‚\n",
        "â”‚   Runs on: Phones, watches, IoT, embedded devices      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRxxzYY2VoJu"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Hands-On: Complete ExecuTorch Pipeline\n",
        "\n",
        "Now let's run through the complete pipeline with working code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW8JjsMxVoJv"
      },
      "source": [
        "### 5.1 Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coV6GWh5VoJv",
        "outputId": "a0416de9-2e0b-4862-8760-7f3b89329c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m543.0/543.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install ExecuTorch\n",
        "# Note: This may take a few minutes\n",
        "!pip install executorch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6th1V3mVoJx",
        "outputId": "2f5de011-855b-46fd-8393-0d2087ca8489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cpu for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ ExecuTorch imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Verify installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "try:\n",
        "    from torch.export import export\n",
        "    from executorch.exir import to_edge\n",
        "    print(\"âœ“ ExecuTorch imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"âœ— Import error: {e}\")\n",
        "    print(\"\\nTroubleshooting: ExecuTorch requires specific PyTorch versions.\")\n",
        "    print(\"Try: pip install executorch torch==2.4.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSz7XamVVoJx"
      },
      "source": [
        "### 5.2 Create a Simple Model\n",
        "\n",
        "We'll use a small classifier â€” complex enough to be meaningful, simple enough to understand completely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfGfgiMrVoJy",
        "outputId": "e52ffd67-c782-4169-f0f1-cf772c88dfb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyClassifier(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A small classifier for demonstration.\n",
        "    Input: 20 features\n",
        "    Output: 4 classes\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Create and set to eval mode\n",
        "model = TinyClassifier()\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš ï¸ eval() is important â€” ExecuTorch is inference-only."
      ],
      "metadata": {
        "id": "kSfc9TGcKKyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with sample input\n",
        "sample_input = torch.randn(1, 20)"
      ],
      "metadata": {
        "id": "a-OtRRzUKJbj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(sample_input)\n",
        "\n",
        "print(f\"Model created successfully!\")\n",
        "print(f\"Input shape:  {sample_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output:       {output}\")"
      ],
      "metadata": {
        "id": "2-M2ULG-KXaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7506136b-e27c-4850-babb-c2400f7c8e15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created successfully!\n",
            "Input shape:  torch.Size([1, 20])\n",
            "Output shape: torch.Size([1, 4])\n",
            "Output:       tensor([[-0.2175, -0.1721,  0.2850,  0.2179]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvjzuy-9VoJy"
      },
      "source": [
        "### 5.3 Export the Model\n",
        "\n",
        "This is where we transition from dynamic PyTorch to a static graph."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create example inputs (VERY important)\n",
        "\n",
        "ExecuTorch needs a static graph."
      ],
      "metadata": {
        "id": "WCd7JvSVKiFd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TROKTo2YVoJz"
      },
      "outputs": [],
      "source": [
        "# Define example input for tracing\n",
        "# Note: Input must be a tuple\n",
        "example_inputs = (torch.randn(1, 20),)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells PyTorch:\n",
        "\n",
        "- input shape\n",
        "- dtype\n",
        "- graph structure"
      ],
      "metadata": {
        "id": "Jpl6fIdWKylQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the model\n",
        "from torch.export import export\n",
        "\n",
        "exported_program = export(model, example_inputs)\n",
        "\n",
        "print(\"âœ“ Export successful!\")\n",
        "print(f\"Type: {type(exported_program).__name__}\")"
      ],
      "metadata": {
        "id": "di4yNidzKonX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "841aa0d1-e736-4992-c382-45c5ae425c7e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Export successful!\n",
            "Type: ExportedProgram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This produces an ExportedProgram:\n",
        "\n",
        "- No Python\n",
        "- Static graph\n",
        "- Ready for lowering"
      ],
      "metadata": {
        "id": "Em0c7l2YKmFk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cSMsU90VoJz"
      },
      "source": [
        "**What just happened?**\n",
        "\n",
        "`torch.export()` traced through your model with the example input and captured every operation as a static graph. No more dynamic Python â€” just a fixed sequence of tensor operations.\n",
        "\n",
        "Let's inspect what was captured:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkyotYzGVoJ0",
        "outputId": "b542b0ef-f7b0-4ba5-8fa9-19249030452c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported Graph:\n",
            "==================================================\n",
            "graph():\n",
            "    %p_layers_0_weight : [num_users=1] = placeholder[target=p_layers_0_weight]\n",
            "    %p_layers_0_bias : [num_users=1] = placeholder[target=p_layers_0_bias]\n",
            "    %p_layers_2_weight : [num_users=1] = placeholder[target=p_layers_2_weight]\n",
            "    %p_layers_2_bias : [num_users=1] = placeholder[target=p_layers_2_bias]\n",
            "    %p_layers_4_weight : [num_users=1] = placeholder[target=p_layers_4_weight]\n",
            "    %p_layers_4_bias : [num_users=1] = placeholder[target=p_layers_4_bias]\n",
            "    %x : [num_users=1] = placeholder[target=x]\n",
            "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%x, %p_layers_0_weight, %p_layers_0_bias), kwargs = {})\n",
            "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%linear,), kwargs = {})\n",
            "    %linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%relu, %p_layers_2_weight, %p_layers_2_bias), kwargs = {})\n",
            "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%linear_1,), kwargs = {})\n",
            "    %linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%relu_1, %p_layers_4_weight, %p_layers_4_bias), kwargs = {})\n",
            "    return (linear_2,)\n"
          ]
        }
      ],
      "source": [
        "# View the exported graph\n",
        "print(\"Exported Graph:\")\n",
        "print(\"=\" * 50)\n",
        "print(exported_program.graph_module.graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHrfFx4aVoJ0"
      },
      "source": [
        "You'll see operations like `torch.ops.aten.linear.default` and `torch.ops.aten.relu.default`. This is the **ATen dialect** â€” PyTorch's internal operator representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L6496nGVoJ1"
      },
      "source": [
        "### 5.4 Convert to Edge Dialect\n",
        "\n",
        "Now we convert to a more portable format optimized for edge devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFCywDS2VoJ1",
        "outputId": "2c1bd82c-8a21-4a65-df08-e9f4ede44328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Edge conversion successful!\n",
            "Type: EdgeProgramManager\n"
          ]
        }
      ],
      "source": [
        "from executorch.exir import to_edge\n",
        "\n",
        "# Convert to edge program\n",
        "edge_program = to_edge(exported_program)\n",
        "\n",
        "print(\"âœ“ Edge conversion successful!\")\n",
        "print(f\"Type: {type(edge_program).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h00KZxppVoJ1"
      },
      "source": [
        "**Why this step?**\n",
        "\n",
        "ATen has ~2000 operators. Edge dialect standardizes these into a smaller, portable set that can run consistently across different edge devices.\n",
        "\n",
        "\n",
        "- Lowers ops\n",
        "- Prepares for mobile / edge backends\n",
        "- Applies graph optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10qXAzfVVoJ2"
      },
      "source": [
        "### 5.5 Compile to ExecuTorch Format\n",
        "\n",
        "Final compilation step â€” creating the `.pte` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oX1T6EjVoJ2",
        "outputId": "a3b457ef-66de-4f5b-fc18-f5094a6c346c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ ExecuTorch compilation successful!\n",
            "Buffer size: 17,040 bytes\n",
            "Buffer size: 16.64 KB\n"
          ]
        }
      ],
      "source": [
        "# Compile to ExecuTorch\n",
        "executorch_program = edge_program.to_executorch()\n",
        "\n",
        "print(\"âœ“ ExecuTorch compilation successful!\")\n",
        "print(f\"Buffer size: {len(executorch_program.buffer):,} bytes\")\n",
        "print(f\"Buffer size: {len(executorch_program.buffer)/1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByFWjcOuVoJ3"
      },
      "source": [
        "Notice the size â€” this tiny model compiles to just a few kilobytes. Compare that to the full PyTorch model which requires the entire PyTorch runtime (~hundreds of MB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1tyAdN0VoJ3"
      },
      "source": [
        "### 5.6 Save the .pte File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gme3zKp3VoJ4",
        "outputId": "827afb89-a5c9-4a9f-8945-c81a40a6c673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Saved to: tiny_classifier.pte\n",
            "  File size: 17,040 bytes (16.64 KB)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Save to file\n",
        "pte_path = \"tiny_classifier.pte\"\n",
        "\n",
        "with open(pte_path, \"wb\") as f:\n",
        "    f.write(executorch_program.buffer)\n",
        "\n",
        "# Verify file\n",
        "file_size = os.path.getsize(pte_path)\n",
        "print(f\"âœ“ Saved to: {pte_path}\")\n",
        "print(f\"  File size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF--x6x4VoJ5"
      },
      "source": [
        "### 5.7 Run Inference with ExecuTorch Runtime\n",
        "\n",
        "Now let's load and run our compiled model using the ExecuTorch runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4QtdPpBVoJ5",
        "outputId": "a61fdf32-a506-4992-a4db-ac8eb4775ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[program.cpp:153] InternalConsistency verification requested but not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Model loaded successfully!\n",
            "\n",
            "Inference Results:\n",
            "Input shape:  torch.Size([1, 20])\n",
            "Output shape: torch.Size([1, 4])\n",
            "Output:       tensor([[-0.1746,  0.0170,  0.0333,  0.1112]])\n"
          ]
        }
      ],
      "source": [
        "from executorch.runtime import Runtime\n",
        "\n",
        "# Load the program\n",
        "runtime = Runtime.get()\n",
        "program = runtime.load_program(pte_path)\n",
        "\n",
        "# Load the forward method\n",
        "method = program.load_method(\"forward\")\n",
        "\n",
        "print(\"âœ“ Model loaded successfully!\")\n",
        "\n",
        "# Create test input\n",
        "test_input = torch.randn(1, 20)\n",
        "\n",
        "# Run inference\n",
        "outputs = method.execute([test_input])\n",
        "\n",
        "print(f\"\\nInference Results:\")\n",
        "print(f\"Input shape:  {test_input.shape}\")\n",
        "print(f\"Output shape: {outputs[0].shape}\")\n",
        "print(f\"Output:       {outputs[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygFsOG6TVoJ6"
      },
      "source": [
        "### 5.8 Verify Results Match\n",
        "\n",
        "Critical check â€” does ExecuTorch produce the same results as PyTorch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqLyCE5fVoJ7",
        "outputId": "3ff8baa1-8d8a-4534-b6b5-31c04d08dd7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison: PyTorch vs ExecuTorch\n",
            "==================================================\n",
            "PyTorch output:     [-0.1961719   0.00400118  0.14360346  0.19159152]\n",
            "ExecuTorch output:  [-0.1961719   0.00400117  0.14360347  0.19159153]\n",
            "\n",
            "âœ“ Outputs match: True\n",
            "The ExecuTorch model produces identical results to PyTorch!\n"
          ]
        }
      ],
      "source": [
        "# Run same input through both\n",
        "test_input = torch.randn(1, 20)\n",
        "\n",
        "# PyTorch inference\n",
        "with torch.no_grad():\n",
        "    pytorch_output = model(test_input)\n",
        "\n",
        "# ExecuTorch inference\n",
        "executorch_output = method.execute([test_input])[0]\n",
        "\n",
        "# Compare\n",
        "print(\"Comparison: PyTorch vs ExecuTorch\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"PyTorch output:     {pytorch_output.numpy().flatten()}\")\n",
        "print(f\"ExecuTorch output:  {executorch_output.numpy().flatten()}\")\n",
        "\n",
        "# Check if close (small numerical differences are normal due to floating point)\n",
        "are_close = torch.allclose(pytorch_output, executorch_output, atol=1e-5)\n",
        "print(f\"\\nâœ“ Outputs match: {are_close}\")\n",
        "\n",
        "if are_close:\n",
        "    print(\"The ExecuTorch model produces identical results to PyTorch!\")\n",
        "else:\n",
        "    max_diff = torch.max(torch.abs(pytorch_output - executorch_output)).item()\n",
        "    print(f\"Maximum difference: {max_diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK3ViBWPVoJ7"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Complete Pipeline Summary\n",
        "\n",
        "Here's the entire ExecuTorch workflow in one place:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfJkks8lVoJ7",
        "outputId": "e87c0260-1b74-4e45-d7e0-77ba4506a475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Model created\n",
            "Step 2: Model exported (ATen dialect)\n",
            "Step 3: Converted to Edge dialect\n",
            "Step 4: Compiled to ExecuTorch format\n",
            "Step 5: Saved to model.pte (16.64 KB)\n",
            "\n",
            "==================================================\n",
            "âœ“ Model ready for edge deployment!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPLETE EXECUTORCH PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.export import export\n",
        "from executorch.exir import to_edge\n",
        "import os\n",
        "\n",
        "# 1. DEFINE MODEL\n",
        "class TinyClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(20, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# 2. PREPARE MODEL\n",
        "model = TinyClassifier()\n",
        "model.eval()\n",
        "print(\"Step 1: Model created\")\n",
        "\n",
        "# 3. EXPORT â†’ EDGE â†’ EXECUTORCH\n",
        "example_inputs = (torch.randn(1, 20),)\n",
        "\n",
        "exported = export(model, example_inputs)\n",
        "print(\"Step 2: Model exported (ATen dialect)\")\n",
        "\n",
        "edge = to_edge(exported)\n",
        "print(\"Step 3: Converted to Edge dialect\")\n",
        "\n",
        "et_program = edge.to_executorch()\n",
        "print(\"Step 4: Compiled to ExecuTorch format\")\n",
        "\n",
        "# 4. SAVE\n",
        "output_path = \"model.pte\"\n",
        "with open(output_path, \"wb\") as f:\n",
        "    f.write(et_program.buffer)\n",
        "\n",
        "file_size = os.path.getsize(output_path)\n",
        "print(f\"Step 5: Saved to {output_path} ({file_size/1024:.2f} KB)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"âœ“ Model ready for edge deployment!\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJQYUDZAVoJ8"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Key Takeaways\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **ExecuTorch** is Meta's framework for deploying PyTorch models to edge devices (phones, IoT, embedded systems)\n",
        "\n",
        "2. **The Pipeline:**\n",
        "   - `export()` â€” Captures dynamic PyTorch as a static graph\n",
        "   - `to_edge()` â€” Converts to portable edge operators\n",
        "   - `to_executorch()` â€” Compiles to final binary format\n",
        "   - `.pte` file â€” Self-contained, ready for edge runtime\n",
        "\n",
        "3. **Key Trade-off:** You sacrifice flexibility (dynamic execution) for efficiency (small runtime, optimized for constrained devices)\n",
        "\n",
        "4. **Runtime:** No Python needed on the target device â€” just the lightweight C++ runtime\n",
        "\n",
        "### Comparison with Other Deployment Methods\n",
        "\n",
        "| Method | Target | Runtime Size | Use Case |\n",
        "|--------|--------|--------------|----------|\n",
        "| Ollama | Desktop/Server | ~100MB+ | Local chatbots |\n",
        "| GGUF/llama.cpp | Desktop | ~10-50MB | CPU inference |\n",
        "| FastAPI | Cloud | Variable | API endpoints |\n",
        "| **ExecuTorch** | **Edge** | **~1MB** | **Mobile apps, IoT** |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "In Part 8.6, we'll:\n",
        "- Compare GGUF vs ExecuTorch in detail\n",
        "- Apply ExecuTorch to a real SLM (Small Language Model)\n",
        "- Explore quantization for edge deployment\n",
        "- Handle LLM-specific challenges (tokenization, generation loops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvnM3kkIVoJ9"
      },
      "source": [
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- [ExecuTorch Documentation](https://pytorch.org/executorch/stable/index.html)\n",
        "- [PyTorch Export Documentation](https://pytorch.org/docs/stable/export.html)\n",
        "- [ExecuTorch GitHub Repository](https://github.com/pytorch/executorch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}