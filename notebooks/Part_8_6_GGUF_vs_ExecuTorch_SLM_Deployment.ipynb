{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 8.6: GGUF vs ExecuTorch \u2014 Edge Deployment for SLMs\n",
        "\n",
        "## Choosing the Right Deployment Format\n",
        "\n",
        "In Part 8.5, we learned the fundamentals of ExecuTorch. Now we'll:\n",
        "\n",
        "1. **Compare GGUF and ExecuTorch** \u2014 Both are for local inference, but serve different purposes\n",
        "2. **Apply ExecuTorch to an SLM** \u2014 Export a real language model for edge deployment\n",
        "3. **Understand the trade-offs** \u2014 When to use each format\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand the key differences between GGUF and ExecuTorch\n",
        "- Know when to choose each deployment format\n",
        "- Export a Small Language Model to ExecuTorch format\n",
        "- Handle LLM-specific challenges in edge deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1. GGUF vs ExecuTorch: Understanding the Difference\n",
        "\n",
        "Both GGUF and ExecuTorch enable local, on-device inference without cloud dependencies. But they target **fundamentally different environments**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 GGUF (llama.cpp)\n",
        "\n",
        "**Target:** Desktops, laptops, servers \u2014 machines with a full operating system\n",
        "\n",
        "**How it works:**\n",
        "- Quantized model weights stored in a single file\n",
        "- llama.cpp runtime (C/C++) loads and executes\n",
        "- Optimized for CPU inference on x86/ARM with SIMD instructions\n",
        "- Can use GPU acceleration via Metal, CUDA, Vulkan\n",
        "\n",
        "**Runtime requirements:**\n",
        "- Needs llama.cpp binary (~10-50MB compiled)\n",
        "- Runs as a standalone process on Linux/macOS/Windows\n",
        "- Full OS environment assumed\n",
        "\n",
        "**Typical use cases:**\n",
        "- Running LLMs on your MacBook\n",
        "- Local chatbots on desktop\n",
        "- Privacy-focused deployments\n",
        "- Hobbyist/developer experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 ExecuTorch\n",
        "\n",
        "**Target:** Edge devices \u2014 phones, watches, IoT, embedded systems\n",
        "\n",
        "**How it works:**\n",
        "- Model compiled to `.pte` format with AOT optimization\n",
        "- Minimal C++ runtime (~1MB or less)\n",
        "- Designed to embed inside apps (iOS/Android) or run on bare-metal\n",
        "- Hardware delegation to NPUs, DSPs, neural accelerators\n",
        "\n",
        "**Runtime requirements:**\n",
        "- No OS required in some cases (bare-metal possible)\n",
        "- Integrates into mobile app binaries\n",
        "- Designed for extreme resource constraints\n",
        "\n",
        "**Typical use cases:**\n",
        "- On-device AI in mobile apps (keyboard prediction, photo enhancement)\n",
        "- Wearables (smartwatch health monitoring)\n",
        "- IoT sensors (anomaly detection)\n",
        "- Automotive (driver assistance)\n",
        "- Anywhere internet connectivity isn't guaranteed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Side-by-Side Comparison\n",
        "\n",
        "| Aspect | GGUF / llama.cpp | ExecuTorch |\n",
        "|--------|------------------|------------|\n",
        "| **Primary target** | Desktop/laptop/server | Mobile/embedded/IoT |\n",
        "| **Minimum RAM** | ~2GB+ realistic | Can work with 128MB |\n",
        "| **Binary size** | ~10-50MB runtime | ~1MB runtime possible |\n",
        "| **OS required** | Yes (Linux/macOS/Windows) | No (can run bare-metal) |\n",
        "| **Hardware acceleration** | CPU SIMD, optional GPU | NPU, DSP, Neural Engine |\n",
        "| **Integration model** | Standalone process | Embedded in app |\n",
        "| **Python needed** | No (at runtime) | No (at runtime) |\n",
        "| **Quantization** | GGUF formats (Q4_K_M, etc.) | PTQ, QAT, dynamic/static |\n",
        "| **Primary use case** | LLMs on personal computers | AI features in apps/devices |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 The Key Insight\n",
        "\n",
        "**GGUF assumes you have a computer.** It's optimized for the kind of machine where you'd run Ollama or a local server.\n",
        "\n",
        "**ExecuTorch assumes you might not.** It's optimized for the kind of device where every megabyte matters and you're shipping inside an app binary.\n",
        "\n",
        "### Real-World Example\n",
        "\n",
        "Imagine you fine-tuned a small assistant model:\n",
        "\n",
        "| Deployment Path | User Experience |\n",
        "|-----------------|----------------|\n",
        "| **GGUF** | User downloads Ollama, pulls your model, runs it in terminal or via API. Works great on a MacBook. |\n",
        "| **ExecuTorch** | You embed the model inside an iOS/Android app. User installs from App Store. Model runs entirely on-device, no setup required, works offline, uses the phone's neural accelerator. |\n",
        "\n",
        "Same model, completely different deployment story."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5 Decision Framework\n",
        "\n",
        "**Choose GGUF when:**\n",
        "- Users have desktops/laptops\n",
        "- RAM is 4GB+ available\n",
        "- Running as a standalone application or server\n",
        "- You want quick experimentation with different quantization levels\n",
        "- CPU inference is acceptable\n",
        "\n",
        "**Choose ExecuTorch when:**\n",
        "- Deploying to mobile apps (iOS/Android)\n",
        "- Target device has <2GB RAM\n",
        "- Need to leverage NPU/Neural Engine\n",
        "- Model must be embedded in app binary\n",
        "- Offline functionality is critical\n",
        "- Battery efficiency matters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. ExecuTorch for LLMs: Special Considerations\n",
        "\n",
        "Deploying language models to edge devices presents unique challenges compared to simple classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 LLM-Specific Challenges\n",
        "\n",
        "| Challenge | Description | Solution |\n",
        "|-----------|-------------|----------|\n",
        "| **Model Size** | Even small LLMs are 100MB-1GB+ | Aggressive quantization (4-bit, 8-bit) |\n",
        "| **KV Cache** | Grows with sequence length | Fixed cache size, sliding window |\n",
        "| **Autoregressive Generation** | Token-by-token loop | Optimized generation kernels |\n",
        "| **Tokenization** | Needs tokenizer on device | Export tokenizer alongside model |\n",
        "| **Dynamic Shapes** | Variable sequence lengths | Use dynamic shape export or pad |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 ExecuTorch LLM Solutions\n",
        "\n",
        "Meta has developed specific optimizations for LLMs on ExecuTorch:\n",
        "\n",
        "**1. Custom Attention Kernels**\n",
        "- Optimized SDPA (Scaled Dot-Product Attention) for mobile\n",
        "- Efficient KV cache management\n",
        "\n",
        "**2. Quantization Support**\n",
        "- 8-bit dynamic quantization\n",
        "- 4-bit weight quantization\n",
        "- Mixed precision strategies\n",
        "\n",
        "**3. Hardware Delegation**\n",
        "- CoreML for Apple Neural Engine\n",
        "- Qualcomm QNN for Snapdragon NPUs\n",
        "- XNNPACK for optimized CPU execution\n",
        "\n",
        "**4. Memory Optimization**\n",
        "- Tensor memory planning\n",
        "- Efficient buffer reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Hands-On: Exporting an SLM to ExecuTorch\n",
        "\n",
        "Let's export a real Small Language Model. We'll use a small model to demonstrate the workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfdb17e-bb11-4903-c837-acb751a3e0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/11.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.2/11.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.5/11.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m543.0/543.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install executorch transformers torch accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30361971-5142-4eec-f344-4f80f0c1827e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu126 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 ExecuTorch imported successfully!\n",
            "\u2713 Transformers imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Verify imports\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "try:\n",
        "    from torch.export import export\n",
        "    from executorch.exir import to_edge\n",
        "    print(\"\u2713 ExecuTorch imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u2717 ExecuTorch import error: {e}\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    print(\"\u2713 Transformers imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u2717 Transformers import error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Approach 1: Export a Minimal GPT-2 Style Model\n",
        "\n",
        "For demonstration purposes, we'll first create a minimal transformer-style model that's easy to export. This helps understand the process before tackling full LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5213f9-0f53-4463-d033-28b8e7f9d695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created: MiniLM\n",
            "Parameters: 230,144 (0.23M)\n",
            "Input shape:  torch.Size([1, 16])\n",
            "Output shape: torch.Size([1, 16, 1000])  (batch, seq_len, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MiniTransformerBlock(nn.Module):\n",
        "    \"\"\"A minimal transformer block for demonstration.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim=64, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual\n",
        "        attn_out, _ = self.attention(x, x, x, need_weights=False)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # FFN with residual\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MiniLM(nn.Module):\n",
        "    \"\"\"\n",
        "    A minimal language model for ExecuTorch demonstration.\n",
        "\n",
        "    This is NOT a real LLM - it's a simplified architecture\n",
        "    to demonstrate the export process.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=1000, embed_dim=64, num_heads=4, num_layers=2, max_seq_len=32):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Token embedding\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Positional embedding\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            MiniTransformerBlock(embed_dim, num_heads)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # Get embeddings\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Output projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = MiniLM(\n",
        "    vocab_size=1000,\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    max_seq_len=32\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Count parameters\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model created: MiniLM\")\n",
        "print(f\"Parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
        "\n",
        "# Test forward pass\n",
        "sample_input = torch.randint(0, 1000, (1, 16))  # batch=1, seq_len=16\n",
        "with torch.no_grad():\n",
        "    output = model(sample_input)\n",
        "print(f\"Input shape:  {sample_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}  (batch, seq_len, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Export MiniLM to ExecuTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fb39a1-b81b-4de5-c404-1eeedd2d06a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Exporting model...\n",
            "\u2713 Export successful!\n"
          ]
        }
      ],
      "source": [
        "from torch.export import export\n",
        "from executorch.exir import to_edge\n",
        "\n",
        "# Prepare example input\n",
        "# Note: We use a fixed sequence length for export\n",
        "example_input = (torch.randint(0, 1000, (1, 16)),)\n",
        "\n",
        "print(\"Step 1: Exporting model...\")\n",
        "try:\n",
        "    exported_program = export(model, example_input)\n",
        "    print(\"\u2713 Export successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u2717 Export failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c57a0f-a59d-4caf-f41e-766174bd9d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: Converting to Edge dialect...\n",
            "\u2713 Edge conversion successful!\n"
          ]
        }
      ],
      "source": [
        "# Convert to Edge dialect\n",
        "print(\"Step 2: Converting to Edge dialect...\")\n",
        "try:\n",
        "    edge_program = to_edge(exported_program)\n",
        "    print(\"\u2713 Edge conversion successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u2717 Edge conversion failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d999eace-7eb4-40bd-cd9d-da5cf8fcad1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3: Compiling to ExecuTorch...\n",
            "\u2713 ExecuTorch compilation successful!\n",
            "  Buffer size: 945,664 bytes (923.50 KB)\n"
          ]
        }
      ],
      "source": [
        "# Compile to ExecuTorch\n",
        "print(\"Step 3: Compiling to ExecuTorch...\")\n",
        "try:\n",
        "    executorch_program = edge_program.to_executorch()\n",
        "    print(\"\u2713 ExecuTorch compilation successful!\")\n",
        "    print(f\"  Buffer size: {len(executorch_program.buffer):,} bytes ({len(executorch_program.buffer)/1024:.2f} KB)\")\n",
        "except Exception as e:\n",
        "    print(f\"\u2717 Compilation failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d421af15-f0ac-474f-9ae9-d72837960233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4: Saved to mini_lm.pte\n",
            "  File size: 945,664 bytes (923.50 KB)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Save the .pte file\n",
        "pte_path = \"mini_lm.pte\"\n",
        "\n",
        "with open(pte_path, \"wb\") as f:\n",
        "    f.write(executorch_program.buffer)\n",
        "\n",
        "file_size = os.path.getsize(pte_path)\n",
        "print(f\"Step 4: Saved to {pte_path}\")\n",
        "print(f\"  File size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Run Inference with ExecuTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b593b6-907d-443a-f4e7-34cf48d04a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[program.cpp:153] InternalConsistency verification requested but not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Model loaded!\n",
            "\n",
            "Inference Results:\n",
            "Input shape:  torch.Size([1, 16])\n",
            "Output shape: torch.Size([1, 16, 1000])\n"
          ]
        }
      ],
      "source": [
        "from executorch.runtime import Runtime\n",
        "\n",
        "# Load the compiled model\n",
        "runtime = Runtime.get()\n",
        "program = runtime.load_program(pte_path)\n",
        "method = program.load_method(\"forward\")\n",
        "\n",
        "print(\"\u2713 Model loaded!\")\n",
        "\n",
        "# Run inference\n",
        "test_input = torch.randint(0, 1000, (1, 16))\n",
        "outputs = method.execute([test_input])\n",
        "\n",
        "print(f\"\\nInference Results:\")\n",
        "print(f\"Input shape:  {test_input.shape}\")\n",
        "print(f\"Output shape: {outputs[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Verify Results Match PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e3c835-8533-47a0-c643-3597e3678394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison: PyTorch vs ExecuTorch\n",
            "==================================================\n",
            "Outputs match (atol=1e-4): True\n",
            "Maximum difference: 0.000001\n",
            "\n",
            "\u2713 ExecuTorch produces equivalent results!\n"
          ]
        }
      ],
      "source": [
        "# Compare PyTorch vs ExecuTorch outputs\n",
        "test_input = torch.randint(0, 1000, (1, 16))\n",
        "\n",
        "# PyTorch\n",
        "with torch.no_grad():\n",
        "    pytorch_output = model(test_input)\n",
        "\n",
        "# ExecuTorch\n",
        "executorch_output = method.execute([test_input])[0]\n",
        "\n",
        "# Compare\n",
        "are_close = torch.allclose(pytorch_output, executorch_output, atol=1e-4)\n",
        "max_diff = torch.max(torch.abs(pytorch_output - executorch_output)).item()\n",
        "\n",
        "print(\"Comparison: PyTorch vs ExecuTorch\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Outputs match (atol=1e-4): {are_close}\")\n",
        "print(f\"Maximum difference: {max_diff:.6f}\")\n",
        "\n",
        "if are_close:\n",
        "    print(\"\\n\u2713 ExecuTorch produces equivalent results!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Approach 2: Exporting a HuggingFace Model\n",
        "\n",
        "Now let's try exporting a real (small) model from HuggingFace. This is more complex due to the model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 The Challenge with HuggingFace Models\n",
        "\n",
        "HuggingFace models have complexities that make direct export challenging:\n",
        "\n",
        "1. **Dynamic control flow** \u2014 KV caching, variable attention masks\n",
        "2. **Custom operations** \u2014 Rotary embeddings, specialized attention\n",
        "3. **Large size** \u2014 Even small models are 100MB+\n",
        "4. **Generation loop** \u2014 Autoregressive decoding is outside the model\n",
        "\n",
        "**Solutions:**\n",
        "- Use `torch.export` with strict=False for more flexibility\n",
        "- Wrap the model to handle specific inputs\n",
        "- Use ExecuTorch's LLM-specific utilities (when available)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Export Strategy: Wrapper Approach\n",
        "\n",
        "We'll wrap a HuggingFace model to make it export-friendly.\n",
        "\n",
        "\n",
        "AutoTokenizer loads GPT-2 tokenizer.\n",
        "\n",
        "GPT-2 often has no pad token, so you set pad_token = eos_token to avoid padding errors.\n",
        "\n",
        "AutoModelForCausalLM loads a causal language model (next-token prediction).\n",
        "\n",
        "use_cache=False disables KV-cache (past key/values). That\u2019s important because:\n",
        "\n",
        "KV-cache introduces extra outputs /\n",
        "state that complicate export.\n",
        "\n",
        "eval() puts the model into inference mode (no dropout etc.).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "324c0c888183483da897f4a1dc141466",
            "0638cb54ce324be285421621d81ed23e",
            "5c0fec1ac346454cb0f2b4b857dc69c4",
            "9625f45c1c274ddba9ccfc5d1302419f",
            "fef26acced444c9abc7c953a6e2d00fa",
            "1daa33341eec4bdd8382bcaebbf363ff",
            "53814221cf524d028ace8e2eb534c93e",
            "b4207c11e52e4f669993418d40537043",
            "de4c7fa6f8b54f74af329a78c32ff35d",
            "520d1a9ec8474c07bab96126f37a62fc",
            "8f5045fce7f2495c9648f0374f25fbab",
            "03f81a78c47646039671fa0323664409",
            "4f238da0cda4462195365b10590eb608",
            "de4b949e8dbd4bba98fa90cfd69d73b4",
            "b0612bb9a40e464697e462e6269ffd79",
            "7b95836106a54362ba1d2a0a45592c2c",
            "67d8393eb388411b99d23a82cd1aa943",
            "9d8705bec0fc43abbd7975e971c107e9",
            "e6ae38238d054f7686fb6e97ce4e6476",
            "b42e64c5634941c59ebb8551e92d5f6b",
            "d64b9e562fe248d696c7baa9f7bf8006",
            "811e5347a75e40f08952a2e07ccd5f9c",
            "a7e97af4bf754882a59d3870cd398847",
            "f479e25916b8455b8bed49658dce34e6",
            "3f42b9b10c424938b030c5b28a3c73ba",
            "10f0ca5703a24d68b944e3dd46b8d758",
            "74365b7ec31d4af296ea43c65ad02e78",
            "46c95306ff594ff1b0f29a57dc31cc25",
            "e0294352ec6442b0932a58eb809624d1",
            "a4584892473c4e218e30574d24125d4a",
            "c5603104b81040788c8a6b875d0ebf98",
            "f7dec5476f044abdabbe0a6bc054ea0b",
            "8eb063795efc452ca028862a609d3025",
            "cc37e5b37635455aac57eb82ba2415cb",
            "d160f838b8a04a559f45b04340098020",
            "ec72dd5f94b74eed985f0f98f920c1ed",
            "d1b9d65940604f7382228c3b9cc184f1",
            "6dee6ffe12dc47a7b816a15a44d7840d",
            "7f7e6fa0b6424708b3fe95694ac28c53",
            "d1d6ce4cfbb14c978c537f112569df5b",
            "525d2e4dd7c14abcaa5307baa06736d8",
            "ec1e4e8916a44a1ba4be7dbb01ef7ca6",
            "5b88a947308b4480bcddfc420bb88c52",
            "d4f861517d174f838f7ed8c540d2e37a",
            "74e2b6b01d594849ae431e36574aead5",
            "f4e0b2fde28f45c3be9b3cb605bbe0cf",
            "85f543a6829140e993ed35bef6f7d158",
            "61881ef7af874afd89aac4eb6bd80b4d",
            "9b337a7a02da4122bd390c6029330c8a",
            "45d9132afef2446ba72a0dbd44844e4e",
            "b0f56dd12ca049e7a6bbe9a682fdc5d8",
            "4a02747092614c69a88564cf48956954",
            "90623c93e73e461da31eeefe0efe36f2",
            "118fac6d5df549479e718f5c89903447",
            "77bd855994f443ad8e5a16c3b0a97dd4",
            "eae3a9681dfb43f58e084fb84946f645",
            "eca602b4272044e0be1d04d3ab3a2d5c",
            "8cbc42f9dbf24468aef01cbb1bb299f2",
            "cbfda685bcc54dfb9d21304653cf676e",
            "758ae158211c4e3ba91e73fdb1e2dcfe",
            "b3e752fa794349819fbc0ec5c72e4785",
            "b0427ceb60ff443c8b05e11e4b5125f2",
            "efe3241a5da44342893fe96c1fc29036",
            "28bc6e4533a344cb903761ae731e9aa7",
            "05f61fb5c9c94d9083fe4c7db01b0dc4",
            "7d59496559224be0beec31aace345a5a"
          ]
        },
        "outputId": "2a7277c4-0964-4682-e5dd-a29c54d7bb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: sshleifer/tiny-gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "324c0c888183483da897f4a1dc141466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03f81a78c47646039671fa0323664409"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7e97af4bf754882a59d3870cd398847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc37e5b37635455aac57eb82ba2415cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74e2b6b01d594849ae431e36574aead5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eae3a9681dfb43f58e084fb84946f645"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Model loaded!\n",
            "  Parameters: 102,714\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "\n",
        "# Use a very small model for demonstration\n",
        "# Options: \"sshleifer/tiny-gpt2\", \"hf-internal-testing/tiny-random-gpt2\"\n",
        "model_name = \"sshleifer/tiny-gpt2\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    use_cache=False  # Disable KV cache for simpler export\n",
        ")\n",
        "hf_model.eval()\n",
        "\n",
        "# Count parameters\n",
        "num_params = sum(p.numel() for p in hf_model.parameters())\n",
        "print(f\"\u2713 Model loaded!\")\n",
        "print(f\"  Parameters: {num_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bbb7655-6823-4a2c-a266-70f5e8f99ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'Hello world'\n",
            "Input IDs shape: torch.Size([1, 2])\n",
            "Output logits shape: torch.Size([1, 2, 50257])\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "test_text = \"Hello world\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = hf_model(**inputs)\n",
        "\n",
        "print(f\"Input: '{test_text}'\")\n",
        "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
        "print(f\"Output logits shape: {outputs.logits.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Create Export-Friendly Wrapper\n",
        "\n",
        "\n",
        "Hugging Face models usually return a complex object (ModelOutput) with many fields (logits, hidden states, cache, etc.). Export tools prefer:\n",
        "\n",
        "- simple tensor inputs\n",
        "- simple tensor outputs\n",
        "\n",
        "So this wrapper forces:\n",
        "\n",
        "- input: input_ids only\n",
        "- output: logits tensor only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6885aa-ee41-4634-ca1d-5c47918c5b35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExportableGPT2(\n",
              "  (model): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(50257, 2)\n",
              "      (wpe): Embedding(1024, 2)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-1): 2 x GPT2Block(\n",
              "          (ln_1): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D(nf=6, nx=2)\n",
              "            (c_proj): Conv1D(nf=2, nx=2)\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D(nf=8, nx=2)\n",
              "            (c_proj): Conv1D(nf=2, nx=8)\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=2, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "class ExportableGPT2(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper to make GPT-2 style models export-friendly.\n",
        "\n",
        "    Key changes:\n",
        "    - Takes only input_ids (no attention_mask for simplicity)\n",
        "    - Returns only logits tensor\n",
        "    - Disables KV cache\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Create attention mask (all ones for simplicity)\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=False\n",
        "        )\n",
        "\n",
        "        # Return only logits\n",
        "        return outputs.logits\n",
        "\n",
        "\n",
        "# Create wrapper\n",
        "exportable_model = ExportableGPT2(hf_model)\n",
        "exportable_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You create attention_mask = ones so the model treats all tokens as real tokens (no padding). That\u2019s fine for demo/export, but in real usage you usually want a real attention mask.\n",
        "\n",
        "Then you test it:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Test wrapper\n",
        "test_ids = torch.randint(0, tokenizer.vocab_size, (1, 8))\n",
        "with torch.no_grad():\n",
        "    output = exportable_model(test_ids)\n",
        "print(f\"Wrapper test - Input: {test_ids.shape}\")\n",
        "print(f\"Output: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989b10d4-639d-45cf-d55d-730057ce7574"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrapper test - Input: torch.Size([1, 8])\n",
            "Output: torch.Size([1, 8, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it prints shapes correctly, the wrapper works."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Export the Wrapped Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "64ecf7986ebb4e01b55038746d0dc24e",
            "09bb7f3ebe9c488f9c11c232ab19183a",
            "17b7feeec6eb49d29d6d170b2c9b3566",
            "b86311cb655d4794ad59e20fc5b9226a",
            "83e86873e8a4455588c78eaa6b3ae281",
            "da9249ded9134a8c9ae81de9228e16b2",
            "32929490649e4cb6a0bc2f9a8a1ce64d",
            "89413d15a0974f6096b198bfd9bfd222",
            "1a9039580cb54b75b7ba0b8201f493d7",
            "cc18479d168c4b3487a304614fca1535",
            "f892cb6da31d4d5f8550479568d39fc0"
          ]
        },
        "outputId": "f3f35cf4-c65b-44ee-f675-ec7199bd7956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting with sequence length: 16\n",
            "This may take a moment...\n",
            "\n",
            "Step 1: Exporting...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.51M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64ecf7986ebb4e01b55038746d0dc24e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Export successful!\n",
            "Step 2: Converting to Edge...\n",
            "\u2713 Edge conversion successful!\n",
            "Step 3: Compiling to ExecuTorch...\n",
            "\u2713 Compilation successful!\n"
          ]
        }
      ],
      "source": [
        "from torch.export import export\n",
        "from executorch.exir import to_edge\n",
        "\n",
        "# Prepare example input with fixed sequence length\n",
        "seq_length = 16\n",
        "example_input = (torch.randint(0, tokenizer.vocab_size, (1, seq_length)),)\n",
        "\n",
        "print(f\"Exporting with sequence length: {seq_length}\")\n",
        "print(\"This may take a moment...\\n\")\n",
        "\n",
        "# Step 1: Export\n",
        "print(\"Step 1: Exporting...\")\n",
        "exported = export(exportable_model, example_input)\n",
        "print(\"\u2713 Export successful!\")\n",
        "\n",
        "# Step 2: Convert to Edge\n",
        "print(\"Step 2: Converting to Edge...\")\n",
        "edge = to_edge(exported)\n",
        "print(\"\u2713 Edge conversion successful!\")\n",
        "\n",
        "# Step 3: Compile to ExecuTorch\n",
        "print(\"Step 3: Compiling to ExecuTorch...\")\n",
        "et_program = edge.to_executorch()\n",
        "print(\"\u2713 Compilation successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.export.export(...)\n",
        "- Captures a static computation graph from your model given example_input.\n",
        "- This is why you pick a fixed seq_length here.\n",
        "\n",
        "to_edge(exported)\n",
        "- Converts the graph into an \u201cedge-safe\u201d representation (EXIR edge dialect).\n",
        "- Prepares it for mobile/runtime constraints.\n",
        "\n",
        "to_executorch()\n",
        "- Compiles it into an ExecuTorch runnable artifact."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Save\n",
        "pte_path = \"tiny_gpt2.pte\"\n",
        "with open(pte_path, \"wb\") as f:\n",
        "  f.write(et_program.buffer)\n",
        "\n",
        "file_size = os.path.getsize(pte_path)\n",
        "print(f\"\\n\u2713 Saved to {pte_path}\")\n",
        "print(f\"  File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce3ea4c-5ee4-4956-b662-7c2b08696527"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2713 Saved to tiny_gpt2.pte\n",
            "  File size: 1,490,996 bytes (1.42 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\u2705 Now tiny_gpt2.pte is the file you\u2019d ship into a mobile/edge app."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Alternative: Using ExecuTorch's LLM Utilities\n",
        "\n",
        "For production LLM deployment, ExecuTorch provides specialized utilities. Here's the conceptual approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9ea94b-28e3-447e-bde7-24fb23a15679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Production ExecuTorch LLM Export Approach:\n",
            "==================================================\n",
            "\n",
            "# Production ExecuTorch LLM Export (Conceptual)\n",
            "# =============================================\n",
            "\n",
            "from executorch.examples.models.llama2 import Llama2Model\n",
            "from executorch.exir import to_edge\n",
            "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
            "\n",
            "# 1. Load model with ExecuTorch's optimized wrapper\n",
            "model = Llama2Model.from_pretrained(\"path/to/model\")\n",
            "\n",
            "# 2. Export with LLM-specific settings\n",
            "exported = torch.export.export(model, example_inputs)\n",
            "\n",
            "# 3. Convert to edge with hardware delegation\n",
            "edge = to_edge(exported)\n",
            "\n",
            "# 4. Apply XNNPACK optimization for CPU\n",
            "edge = edge.to_backend(XnnpackPartitioner())\n",
            "\n",
            "# 5. Compile with quantization\n",
            "from executorch.exir import EdgeCompileConfig\n",
            "config = EdgeCompileConfig(_check_ir_validity=False)\n",
            "et_program = edge.to_executorch(config)\n",
            "\n",
            "# 6. Save\n",
            "with open(\"llama2.pte\", \"wb\") as f:\n",
            "    f.write(et_program.buffer)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This is conceptual code showing the production approach\n",
        "# Actual implementation requires additional ExecuTorch LLM packages\n",
        "\n",
        "CONCEPTUAL_CODE = \"\"\"\n",
        "# Production ExecuTorch LLM Export (Conceptual)\n",
        "# =============================================\n",
        "\n",
        "from executorch.examples.models.llama2 import Llama2Model\n",
        "from executorch.exir import to_edge\n",
        "from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner\n",
        "\n",
        "# 1. Load model with ExecuTorch's optimized wrapper\n",
        "model = Llama2Model.from_pretrained(\"path/to/model\")\n",
        "\n",
        "# 2. Export with LLM-specific settings\n",
        "exported = torch.export.export(model, example_inputs)\n",
        "\n",
        "# 3. Convert to edge with hardware delegation\n",
        "edge = to_edge(exported)\n",
        "\n",
        "# 4. Apply XNNPACK optimization for CPU\n",
        "edge = edge.to_backend(XnnpackPartitioner())\n",
        "\n",
        "# 5. Compile with quantization\n",
        "from executorch.exir import EdgeCompileConfig\n",
        "config = EdgeCompileConfig(_check_ir_validity=False)\n",
        "et_program = edge.to_executorch(config)\n",
        "\n",
        "# 6. Save\n",
        "with open(\"llama2.pte\", \"wb\") as f:\n",
        "    f.write(et_program.buffer)\n",
        "\"\"\"\n",
        "\n",
        "print(\"Production ExecuTorch LLM Export Approach:\")\n",
        "print(\"=\" * 50)\n",
        "print(CONCEPTUAL_CODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Quantization for Edge Deployment\n",
        "\n",
        "Quantization is critical for edge deployment. Let's explore the options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Quantization Options Comparison\n",
        "\n",
        "| Method | Precision | Size Reduction | Accuracy Impact | Use Case |\n",
        "|--------|-----------|----------------|-----------------|----------|\n",
        "| **FP32** | 32-bit | 1x (baseline) | None | Development |\n",
        "| **FP16** | 16-bit | 2x | Minimal | GPU inference |\n",
        "| **INT8 Dynamic** | 8-bit | 4x | Low | CPU inference |\n",
        "| **INT8 Static** | 8-bit | 4x | Low | Calibrated inference |\n",
        "| **INT4** | 4-bit | 8x | Moderate | Extreme compression |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Applying Dynamic Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4247b92b-bee1-42e3-baeb-5e06eab6e79d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic Quantization Applied!\n",
            "\n",
            "Original model (FP32):\n",
            "  Size estimate: 920,576 bytes\n",
            "\n",
            "Quantized model (INT8):\n",
            "  Size estimate: ~230,144 bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2049492293.py:15: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = quantize_dynamic(\n"
          ]
        }
      ],
      "source": [
        "# Dynamic quantization example with our MiniLM\n",
        "from torch.ao.quantization import quantize_dynamic\n",
        "\n",
        "# Create a fresh model\n",
        "model_fp32 = MiniLM(\n",
        "    vocab_size=1000,\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    max_seq_len=32\n",
        ")\n",
        "model_fp32.eval()\n",
        "\n",
        "# Apply dynamic quantization\n",
        "model_int8 = quantize_dynamic(\n",
        "    model_fp32,\n",
        "    {nn.Linear},  # Quantize Linear layers\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "print(\"Dynamic Quantization Applied!\")\n",
        "print(\"\\nOriginal model (FP32):\")\n",
        "print(f\"  Size estimate: {sum(p.numel() * 4 for p in model_fp32.parameters()):,} bytes\")\n",
        "\n",
        "print(\"\\nQuantized model (INT8):\")\n",
        "print(f\"  Size estimate: ~{sum(p.numel() for p in model_fp32.parameters()):,} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309b9393-99a6-49a7-abcd-6d4a35eceb48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantization Impact:\n",
            "  Max difference:  0.033581\n",
            "  Mean difference: 0.007226\n",
            "\n",
            "Small differences are expected and usually acceptable!\n"
          ]
        }
      ],
      "source": [
        "# Compare outputs\n",
        "test_input = torch.randint(0, 1000, (1, 16))\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_fp32 = model_fp32(test_input)\n",
        "    output_int8 = model_int8(test_input)\n",
        "\n",
        "# Check difference\n",
        "max_diff = torch.max(torch.abs(output_fp32 - output_int8)).item()\n",
        "mean_diff = torch.mean(torch.abs(output_fp32 - output_int8)).item()\n",
        "\n",
        "print(\"Quantization Impact:\")\n",
        "print(f\"  Max difference:  {max_diff:.6f}\")\n",
        "print(f\"  Mean difference: {mean_diff:.6f}\")\n",
        "print(\"\\nSmall differences are expected and usually acceptable!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Complete Workflow Summary\n",
        "\n",
        "Here's the complete workflow for deploying an SLM to edge devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20c9c78-095b-4bcc-c9ee-c52b6aea9178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################################################\n",
            "# EXAMPLE: Deploy MiniLM to Edge\n",
            "############################################################\n",
            "\n",
            "Deploying MiniLM to edge...\n",
            "==================================================\n",
            "[1/4] Exporting model...\n",
            "      \u2713 Export complete\n",
            "[2/4] Converting to Edge dialect...\n",
            "      \u2713 Edge conversion complete\n",
            "[3/4] Compiling to ExecuTorch...\n",
            "      \u2713 Compilation complete\n",
            "[4/4] Saving .pte file...\n",
            "      \u2713 Saved to mini_lm_edge.pte\n",
            "\n",
            "==================================================\n",
            "DEPLOYMENT COMPLETE\n",
            "==================================================\n",
            "Model: MiniLM\n",
            "Output: mini_lm_edge.pte\n",
            "Size: 923.50 KB (0.90 MB)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPLETE SLM TO EDGE DEPLOYMENT WORKFLOW\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.export import export\n",
        "from executorch.exir import to_edge\n",
        "import os\n",
        "\n",
        "def deploy_model_to_edge(model, example_input, output_path, model_name=\"model\"):\n",
        "    \"\"\"\n",
        "    Complete pipeline to deploy a PyTorch model to edge devices.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch nn.Module (must be in eval mode)\n",
        "        example_input: Tuple of example input tensors\n",
        "        output_path: Path to save the .pte file\n",
        "        model_name: Name for logging\n",
        "\n",
        "    Returns:\n",
        "        dict with deployment info\n",
        "    \"\"\"\n",
        "    print(f\"Deploying {model_name} to edge...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Ensure eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Step 1: Export\n",
        "    print(\"[1/4] Exporting model...\")\n",
        "    exported_program = export(model, example_input)\n",
        "    print(\"      \u2713 Export complete\")\n",
        "\n",
        "    # Step 2: Convert to Edge\n",
        "    print(\"[2/4] Converting to Edge dialect...\")\n",
        "    edge_program = to_edge(exported_program)\n",
        "    print(\"      \u2713 Edge conversion complete\")\n",
        "\n",
        "    # Step 3: Compile to ExecuTorch\n",
        "    print(\"[3/4] Compiling to ExecuTorch...\")\n",
        "    et_program = edge_program.to_executorch()\n",
        "    print(\"      \u2713 Compilation complete\")\n",
        "\n",
        "    # Step 4: Save\n",
        "    print(\"[4/4] Saving .pte file...\")\n",
        "    with open(output_path, \"wb\") as f:\n",
        "        f.write(et_program.buffer)\n",
        "\n",
        "    file_size = os.path.getsize(output_path)\n",
        "    print(f\"      \u2713 Saved to {output_path}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"DEPLOYMENT COMPLETE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    info = {\n",
        "        \"model_name\": model_name,\n",
        "        \"output_path\": output_path,\n",
        "        \"file_size_bytes\": file_size,\n",
        "        \"file_size_kb\": file_size / 1024,\n",
        "        \"file_size_mb\": file_size / 1024 / 1024\n",
        "    }\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Output: {output_path}\")\n",
        "    print(f\"Size: {info['file_size_kb']:.2f} KB ({info['file_size_mb']:.2f} MB)\")\n",
        "\n",
        "    return info\n",
        "\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n\" + \"#\" * 60)\n",
        "print(\"# EXAMPLE: Deploy MiniLM to Edge\")\n",
        "print(\"#\" * 60 + \"\\n\")\n",
        "\n",
        "# Create model\n",
        "model = MiniLM(vocab_size=1000, embed_dim=64, num_heads=4, num_layers=2)\n",
        "model.eval()\n",
        "\n",
        "# Deploy\n",
        "info = deploy_model_to_edge(\n",
        "    model=model,\n",
        "    example_input=(torch.randint(0, 1000, (1, 16)),),\n",
        "    output_path=\"mini_lm_edge.pte\",\n",
        "    model_name=\"MiniLM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Key Takeaways\n",
        "\n",
        "### GGUF vs ExecuTorch\n",
        "\n",
        "| When to Use | GGUF | ExecuTorch |\n",
        "|-------------|------|------------|\n",
        "| Target device | Desktop/laptop | Mobile/embedded |\n",
        "| User setup | Downloads app/binary | Installs from app store |\n",
        "| Integration | Standalone process | Embedded in app |\n",
        "| Hardware | CPU (+ optional GPU) | NPU, Neural Engine |\n",
        "\n",
        "### LLM Edge Deployment Challenges\n",
        "\n",
        "1. **Model size** \u2192 Quantization (INT8, INT4)\n",
        "2. **Dynamic shapes** \u2192 Fixed sequence lengths or padding\n",
        "3. **KV cache** \u2192 Specialized handling or disabled\n",
        "4. **Tokenization** \u2192 Bundle tokenizer with model\n",
        "\n",
        "### The Export Pipeline\n",
        "\n",
        "```\n",
        "PyTorch Model\n",
        "     \u2193 torch.export()\n",
        "Exported Program (ATen)\n",
        "     \u2193 to_edge()\n",
        "Edge Program\n",
        "     \u2193 to_executorch()\n",
        ".pte File \u2192 Edge Device\n",
        "```\n",
        "\n",
        "### Production Considerations\n",
        "\n",
        "- Use ExecuTorch's LLM-specific utilities for production\n",
        "- Apply hardware delegates (XNNPACK, CoreML, QNN) for acceleration\n",
        "- Test thoroughly on target devices\n",
        "- Consider memory constraints at every step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- [ExecuTorch Documentation](https://pytorch.org/executorch/stable/index.html)\n",
        "- [ExecuTorch LLM Examples](https://github.com/pytorch/executorch/tree/main/examples/models)\n",
        "- [llama.cpp / GGUF](https://github.com/ggerganov/llama.cpp)\n",
        "- [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html)\n",
        "- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}