{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9.1: Multi-GPU & Distributed Training Fundamentals\n",
    "\n",
    "## Scaling Beyond Single GPU\n",
    "\n",
    "Throughout this course, we've optimized training on a single GPU using techniques like LoRA, QLoRA, and Unsloth. Now we'll learn how to **scale training across multiple GPUs** — essential for larger models and faster iteration.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand why and when to distribute training\n",
    "2. Learn the three main parallelism strategies\n",
    "3. Understand DeepSpeed ZeRO stages\n",
    "4. Compare FSDP vs DeepSpeed\n",
    "5. Get introduced to HuggingFace Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why Distribute Training?\n",
    "\n",
    "When training on a single GPU, you eventually hit one of two walls:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The Two Problems\n",
    "\n",
    "**Problem 1: Memory Limit — Model Doesn't Fit**\n",
    "\n",
    "GPU memory must hold:\n",
    "- Model parameters\n",
    "- Optimizer states (Adam has 2x parameters)\n",
    "- Gradients\n",
    "- Activations (for backprop)\n",
    "\n",
    "For a 7B parameter model in FP32:\n",
    "```\n",
    "Parameters:      7B × 4 bytes = 28 GB\n",
    "Optimizer (Adam): 7B × 8 bytes = 56 GB\n",
    "Gradients:       7B × 4 bytes = 28 GB\n",
    "─────────────────────────────────────\n",
    "Total:                         112 GB (without activations!)\n",
    "```\n",
    "\n",
    "Even an A100 (80GB) can't fit this!\n",
    "\n",
    "**Problem 2: Speed Limit — Training is Too Slow**\n",
    "\n",
    "Even if the model fits, you might want:\n",
    "- Faster experimentation cycles\n",
    "- Larger batch sizes for better convergence\n",
    "- Quicker hyperparameter searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 What Distribution Solves\n",
    "\n",
    "| Problem | Solution | How It Helps |\n",
    "|---------|----------|-------------|\n",
    "| Model too large | Shard model across GPUs | Each GPU holds fraction of memory |\n",
    "| Training too slow | Process batches in parallel | N GPUs ≈ N× throughput |\n",
    "| Batch size limited | Accumulate across GPUs | Effective batch = N × local batch |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Your Context\n",
    "\n",
    "You've been training on:\n",
    "- **Colab T4**: 16GB VRAM, single GPU\n",
    "- **Models**: 0.5B - 3B parameters\n",
    "- **Techniques**: QLoRA, Unsloth optimizations\n",
    "\n",
    "With **Lightning.ai's 2 GPUs**, you can:\n",
    "- Train ~2× faster with data parallelism\n",
    "- Fit larger models with memory sharding\n",
    "- Use larger batch sizes for better gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Three Parallelism Strategies\n",
    "\n",
    "There are three fundamental ways to distribute training across GPUs.\n",
    "\n",
    "1. Data Parallelism\n",
    "2. Model/Tensor Parallelism\n",
    "3. Fully Sharded Data Parallelism -> FSDP Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Parallelism (DP)\n",
    "\n",
    "**Concept:** Each GPU has a complete copy of the model, but processes different data.\n",
    "\n",
    "```\n",
    "                    Training Data\n",
    "                         │\n",
    "              ┌──────────┴──────────┐\n",
    "              ▼                     ▼\n",
    "         Batch 1               Batch 2\n",
    "              │                     │\n",
    "              ▼                     ▼\n",
    "    ┌─────────────────┐   ┌─────────────────┐\n",
    "    │     GPU 0       │   │     GPU 1       │\n",
    "    │  Full Model     │   │  Full Model     │\n",
    "    │    Copy         │   │    Copy         │\n",
    "    └────────┬────────┘   └────────┬────────┘\n",
    "             │                     │\n",
    "             ▼                     ▼\n",
    "        Gradients 1           Gradients 2\n",
    "             │                     │\n",
    "             └──────────┬──────────┘\n",
    "                        ▼\n",
    "              All-Reduce (Average)\n",
    "                        │\n",
    "                        ▼\n",
    "                 Update Weights\n",
    "                 (synchronized)\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simple to implement\n",
    "- Near-linear speedup with more GPUs\n",
    "- No code changes to model\n",
    "\n",
    "**Cons:**\n",
    "- Model must fit on each GPU\n",
    "- Memory not reduced (actually slightly increased due to gradient buffers)\n",
    "\n",
    "**Best for:** Models that fit on one GPU, but you want faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Parallelism (MP) / Tensor Parallelism (TP)\n",
    "\n",
    "**Concept:** Split the model itself across GPUs.\n",
    "\n",
    "```\n",
    "                    Input\n",
    "                      │\n",
    "                      ▼\n",
    "    ┌─────────────────────────────────┐\n",
    "    │            GPU 0                │\n",
    "    │       Layers 1 - 12             │\n",
    "    └────────────────┬────────────────┘\n",
    "                     │ activations\n",
    "                     ▼\n",
    "    ┌─────────────────────────────────┐\n",
    "    │            GPU 1                │\n",
    "    │       Layers 13 - 24            │\n",
    "    └────────────────┬────────────────┘\n",
    "                     │\n",
    "                     ▼\n",
    "                  Output\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Can train models larger than single GPU memory\n",
    "- Each GPU only holds part of model\n",
    "\n",
    "**Cons:**\n",
    "- Complex to implement\n",
    "- GPUs wait for each other (pipeline bubbles)\n",
    "- Communication overhead between GPUs\n",
    "- Doesn't speed up training much\n",
    "\n",
    "**Best for:** Very large models that can't fit any other way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fully Sharded Data Parallelism (FSDP / ZeRO)\n",
    "\n",
    "**Concept:** Shard parameters, gradients, and optimizer states across GPUs. Gather when needed.\n",
    "\n",
    "```\n",
    "              Before: Data Parallelism\n",
    "    ┌─────────────────┐   ┌─────────────────┐\n",
    "    │     GPU 0       │   │     GPU 1       │\n",
    "    │ Full Params     │   │ Full Params     │\n",
    "    │ Full Optimizer  │   │ Full Optimizer  │\n",
    "    │ Full Gradients  │   │ Full Gradients  │\n",
    "    └─────────────────┘   └─────────────────┘\n",
    "         Memory: 100%          Memory: 100%\n",
    "\n",
    "\n",
    "               After: FSDP / ZeRO-3\n",
    "    ┌─────────────────┐   ┌─────────────────┐\n",
    "    │     GPU 0       │   │     GPU 1       │\n",
    "    │ Params Shard 1  │   │ Params Shard 2  │\n",
    "    │ Optim Shard 1   │   │ Optim Shard 2   │\n",
    "    │ Grads Shard 1   │   │ Grads Shard 2   │\n",
    "    └─────────────────┘   └─────────────────┘\n",
    "         Memory: ~50%          Memory: ~50%\n",
    "\n",
    "\n",
    "    During Forward/Backward:\n",
    "    - All-gather params when needed\n",
    "    - Compute\n",
    "    - Reduce-scatter gradients\n",
    "    - Discard non-local params\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Massive memory reduction (scales with GPU count)\n",
    "- Can train very large models\n",
    "- Combines benefits of DP and MP\n",
    "\n",
    "**Cons:**\n",
    "- More communication overhead\n",
    "- Slightly more complex setup\n",
    "\n",
    "**Best for:** Training large models efficiently across multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparison Summary\n",
    "\n",
    "| Strategy | Memory Reduction | Speedup | Complexity | Use Case |\n",
    "|----------|-----------------|---------|------------|----------|\n",
    "| **Data Parallel** | None | ~N× | Low | Model fits, want speed |\n",
    "| **Model Parallel** | ~N× | Minimal | High | Huge models only |\n",
    "| **FSDP / ZeRO** | ~N× | Good | Medium | Large models + speed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. DeepSpeed ZeRO Stages\n",
    "\n",
    "DeepSpeed's ZeRO (Zero Redundancy Optimizer) has three stages, each sharding more aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What Gets Sharded at Each Stage\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    GPU MEMORY BREAKDOWN                         │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│   ┌─────────────┐                                               │\n",
    "│   │ Activations │  ← Stored for backward pass                   │\n",
    "│   ├─────────────┤                                               │\n",
    "│   │  Gradients  │  ← ZeRO-2+ shards these                       │\n",
    "│   ├─────────────┤                                               │\n",
    "│   │  Optimizer  │  ← ZeRO-1+ shards these (largest!)            │\n",
    "│   │   States    │                                               │\n",
    "│   ├─────────────┤                                               │\n",
    "│   │ Parameters  │  ← ZeRO-3 shards these                        │\n",
    "│   └─────────────┘                                               │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ZeRO Stage Comparison\n",
    "\n",
    "| Stage | What's Sharded | Memory Savings | Communication | Speed |\n",
    "|-------|---------------|----------------|---------------|-------|\n",
    "| **ZeRO-1** | Optimizer states | ~4× | Low | Fast |\n",
    "| **ZeRO-2** | + Gradients | ~8× | Medium | Good |\n",
    "| **ZeRO-3** | + Parameters | ~N× (linear with GPUs) | High | Slower |\n",
    "\n",
    "**Memory formula (approximate):**\n",
    "```\n",
    "Standard DP:  Model × (1 + 1 + optimizer_multiplier) per GPU\n",
    "ZeRO-1:       Model × (1 + 1 + optimizer_multiplier/N) per GPU\n",
    "ZeRO-2:       Model × (1 + (1 + optimizer_multiplier)/N) per GPU\n",
    "ZeRO-3:       Model × (1 + 1 + optimizer_multiplier)/N per GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 When to Use Each Stage\n",
    "\n",
    "**ZeRO-1:**\n",
    "- Model fits on GPU with some room\n",
    "- Want memory savings with minimal overhead\n",
    "- Good default starting point\n",
    "\n",
    "**ZeRO-2:**\n",
    "- Model barely fits on GPU\n",
    "- Need more memory for larger batches\n",
    "- Acceptable communication overhead\n",
    "\n",
    "**ZeRO-3:**\n",
    "- Model doesn't fit on single GPU at all\n",
    "- Maximum memory efficiency needed\n",
    "- Willing to trade speed for capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visual: Memory Reduction with ZeRO\n",
    "\n",
    "For a 7B parameter model with 2 GPUs:\n",
    "\n",
    "```\n",
    "Standard Data Parallel (per GPU):\n",
    "├─────────────────────────────────────────────────────┤ 112 GB\n",
    "│ Params (28GB) │ Optimizer (56GB) │ Gradients (28GB) │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "\n",
    "ZeRO-1 (per GPU):\n",
    "├─────────────────────────────────────────┤ 84 GB\n",
    "│ Params (28GB) │ Optim (28GB) │ Grads (28GB) │\n",
    "└─────────────────────────────────────────┘\n",
    "\n",
    "ZeRO-2 (per GPU):\n",
    "├────────────────────────────────┤ 56 GB\n",
    "│ Params (28GB) │ Optim+Grads (28GB) │\n",
    "└────────────────────────────────┘\n",
    "\n",
    "ZeRO-3 (per GPU):\n",
    "├──────────────────┤ 28 GB\n",
    "│ Everything sharded │\n",
    "└──────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. FSDP vs DeepSpeed\n",
    "\n",
    "Both achieve similar goals but come from different ecosystems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Comparison\n",
    "\n",
    "| Aspect | DeepSpeed | FSDP |\n",
    "|--------|-----------|------|\n",
    "| **Origin** | Microsoft | Meta (PyTorch native) |\n",
    "| **Integration** | Requires DeepSpeed library | Built into PyTorch |\n",
    "| **Configuration** | JSON config file | Python API |\n",
    "| **Features** | ZeRO-1/2/3, Offload, Infinity | Sharding, Mixed Precision |\n",
    "| **Maturity** | More features, well-tested | Newer, rapidly improving |\n",
    "| **HF Support** | Excellent (Trainer + Accelerate) | Good (Trainer + Accelerate) |\n",
    "| **CPU Offload** | Yes (ZeRO-Offload) | Yes (recent versions) |\n",
    "\n",
    "### 4.2 When to Choose\n",
    "\n",
    "**Choose DeepSpeed when:**\n",
    "- Need maximum memory efficiency\n",
    "- Want CPU/NVMe offloading\n",
    "- Using HuggingFace Trainer (great integration)\n",
    "- Need proven, battle-tested solution\n",
    "\n",
    "**Choose FSDP when:**\n",
    "- Want PyTorch-native solution\n",
    "- Prefer Python configuration over JSON\n",
    "- Already using PyTorch ecosystem heavily\n",
    "- Want simpler debugging (native PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. HuggingFace Accelerate\n",
    "\n",
    "Accelerate is a library that simplifies distributed training by providing a unified interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 What Accelerate Does\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Your Training Code                       │\n",
    "└─────────────────────────────┬───────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   HuggingFace Accelerate                    │\n",
    "│                                                             │\n",
    "│   - Handles device placement                                │\n",
    "│   - Manages distributed communication                       │\n",
    "│   - Wraps model, optimizer, dataloader                      │\n",
    "│   - Provides unified API                                    │\n",
    "└─────────────────────────────┬───────────────────────────────┘\n",
    "                              │\n",
    "         ┌────────────────────┼────────────────────┐\n",
    "         ▼                    ▼                    ▼\n",
    "┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐\n",
    "│   Single GPU    │  │   DeepSpeed     │  │     FSDP        │\n",
    "└─────────────────┘  └─────────────────┘  └─────────────────┘\n",
    "```\n",
    "\n",
    "**Key benefit:** Write code once, run anywhere (single GPU, multi-GPU, multi-node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Minimal Code Changes\n",
    "\n",
    "**Before (Single GPU):**\n",
    "```python\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "model.to('cuda')\n",
    "for batch in dataloader:\n",
    "    batch = batch.to('cuda')\n",
    "    outputs = model(batch)\n",
    "    loss = compute_loss(outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "**After (Multi-GPU with Accelerate):**\n",
    "```python\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# Accelerate handles everything!\n",
    "model, optimizer, dataloader = accelerator.prepare(\n",
    "    model, optimizer, dataloader\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    outputs = model(batch)  # No .to('cuda') needed!\n",
    "    loss = compute_loss(outputs)\n",
    "    accelerator.backward(loss)  # Instead of loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "**Changes:**\n",
    "1. Create `Accelerator()`\n",
    "2. Call `accelerator.prepare()` on model, optimizer, dataloader\n",
    "3. Use `accelerator.backward()` instead of `loss.backward()`\n",
    "4. Remove manual `.to('cuda')` calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Configuration with `accelerate config`\n",
    "\n",
    "Before running, you configure Accelerate:\n",
    "\n",
    "```bash\n",
    "$ accelerate config\n",
    "\n",
    "In which compute environment are you running? multi-GPU\n",
    "How many machines? 1\n",
    "How many GPUs? 2\n",
    "Do you want to use DeepSpeed? yes\n",
    "Which ZeRO stage? 2\n",
    "...\n",
    "```\n",
    "\n",
    "This creates a config file. Then launch with:\n",
    "\n",
    "```bash\n",
    "$ accelerate launch train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Practical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Effective Batch Size\n",
    "\n",
    "With distributed training, your effective batch size changes:\n",
    "\n",
    "```\n",
    "Effective Batch Size = per_gpu_batch × num_gpus × gradient_accumulation_steps\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- per_gpu_batch = 4\n",
    "- num_gpus = 2\n",
    "- gradient_accumulation = 8\n",
    "- **Effective batch = 4 × 2 × 8 = 64**\n",
    "\n",
    "**Why it matters:**\n",
    "- Learning rate often needs adjustment with batch size\n",
    "- Common rule: scale LR linearly with batch size\n",
    "- Or use learning rate warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Communication Overhead\n",
    "\n",
    "Distributed training adds communication costs:\n",
    "\n",
    "| Operation | When | Cost |\n",
    "|-----------|------|------|\n",
    "| All-Reduce | Gradient sync | O(model_size) |\n",
    "| All-Gather | ZeRO-3 forward | O(model_size) |\n",
    "| Reduce-Scatter | ZeRO gradient sync | O(model_size) |\n",
    "\n",
    "**Tips to reduce overhead:**\n",
    "- Use gradient accumulation (fewer syncs)\n",
    "- Overlap communication with computation\n",
    "- Use fast interconnects (NVLink > PCIe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Common Issues\n",
    "\n",
    "| Issue | Cause | Solution |\n",
    "|-------|-------|----------|\n",
    "| OOM on one GPU | Uneven model distribution | Check sharding config |\n",
    "| Training hangs | Deadlock in communication | Check all processes reach sync points |\n",
    "| Loss is NaN | Gradient explosion | Lower LR, add gradient clipping |\n",
    "| Slow training | Too much communication | Increase batch size, use ZeRO-1/2 |\n",
    "| Different results | Random seed not synced | Set seed on all processes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary: Choosing Your Strategy\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "Does your model fit on a single GPU?\n",
    "│\n",
    "├─ YES → Do you want faster training?\n",
    "│        │\n",
    "│        ├─ YES → Use Data Parallelism (DDP) or ZeRO-1\n",
    "│        │\n",
    "│        └─ NO → Stay with single GPU (simpler)\n",
    "│\n",
    "└─ NO → How much memory do you need?\n",
    "         │\n",
    "         ├─ A little more → ZeRO-2\n",
    "         │\n",
    "         └─ Much more → ZeRO-3 or FSDP\n",
    "```\n",
    "\n",
    "### For Your Lightning.ai Setup (2 GPUs)\n",
    "\n",
    "**Recommended approach:**\n",
    "1. Start with **Accelerate + DeepSpeed ZeRO-2**\n",
    "2. Use HuggingFace Trainer for simplicity\n",
    "3. Monitor memory usage\n",
    "4. Upgrade to ZeRO-3 if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "1. **Data Parallelism** — Same model on each GPU, different data. Simple, fast, no memory savings.\n",
    "\n",
    "2. **Model Parallelism** — Split model across GPUs. Complex, for huge models only.\n",
    "\n",
    "3. **FSDP / ZeRO** — Best of both worlds. Shard everything, gather when needed.\n",
    "\n",
    "4. **ZeRO Stages:**\n",
    "   - ZeRO-1: Shard optimizer → 4× memory reduction\n",
    "   - ZeRO-2: + Shard gradients → 8× reduction\n",
    "   - ZeRO-3: + Shard parameters → N× reduction\n",
    "\n",
    "5. **Accelerate** — Unified interface for distributed training. Minimal code changes.\n",
    "\n",
    "6. **Effective batch size** — Changes with distribution. Adjust learning rate accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Part 9.2**, we'll:\n",
    "- Set up Lightning.ai environment\n",
    "- Configure Accelerate with DeepSpeed\n",
    "- Run distributed fine-tuning on 2 GPUs\n",
    "- Compare performance vs single GPU\n",
    "\n",
    "Get your Lightning.ai account ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [DeepSpeed Documentation](https://www.deepspeed.ai/)\n",
    "- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n",
    "- [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/)\n",
    "- [ZeRO Paper](https://arxiv.org/abs/1910.02054)\n",
    "- [DeepSpeed ZeRO Tutorial](https://www.deepspeed.ai/tutorials/zero/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
