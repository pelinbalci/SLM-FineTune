{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \u26a1 Part 7: Unsloth vs Standard Training\n",
        "\n",
        "**The Promise:** Train 2-5x faster with 50-70% less memory.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Unsloth?\n",
        "\n",
        "Unsloth is an optimized training library that:\n",
        "\n",
        "- **Rewrites key operations** in custom CUDA/Triton kernels\n",
        "- **Fuses operations** to reduce memory transfers\n",
        "- **Optimizes attention** computation\n",
        "- **Reduces memory** through smart gradient checkpointing\n",
        "\n",
        "```\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                    STANDARD TRAINING                        \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502  HuggingFace Transformers \u2192 PyTorch \u2192 CUDA                  \u2502\n",
        "\u2502  (General purpose, not optimized for fine-tuning)           \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "                          vs\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                    UNSLOTH TRAINING                         \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502  Custom Kernels \u2192 Fused Operations \u2192 Optimized Memory       \u2502\n",
        "\u2502  (Purpose-built for LoRA fine-tuning)                       \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## What We'll Compare\n",
        "\n",
        "| Metric | Standard | Unsloth |\n",
        "|--------|----------|--------|\n",
        "| Training time | Baseline | ? |\n",
        "| GPU memory | Baseline | ? |\n",
        "| Final loss | Baseline | Should match |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tWBJC20PxNVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \u26a0\ufe0f Important: Fresh Runtime\n",
        "\n",
        "**Before running:** `Runtime` \u2192 `Restart runtime` to clear memory."
      ],
      "metadata": {
        "id": "BwHOdH5exNVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part A: Standard Training (Baseline)\n",
        "\n",
        "First, let's establish our baseline with standard HuggingFace + PEFT training."
      ],
      "metadata": {
        "id": "T24NBpsYxNVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Check GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"\u2705 GPU: {gpu_name}\")\n",
        "    print(f\"   Total Memory: {total_mem:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"No GPU available!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar8xihm4xNVz",
        "outputId": "ebf45cfb-92ce-45df-cce9-d8eddbd2e1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 GPU: Tesla T4\n",
            "   Total Memory: 15.8 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration - same for both experiments\n",
        "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 4\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "\n",
        "# LoRA settings\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(\"\ud83d\udccb Configuration:\")\n",
        "print(f\"   Model: {MODEL_ID}\")\n",
        "print(f\"   Sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"   Effective batch: {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(f\"   LoRA r={LORA_R}, alpha={LORA_ALPHA}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gM79sJUxNV0",
        "outputId": "ce25ce4d-db9d-4691-d9eb-f0da8d164a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udccb Configuration:\n",
            "   Model: Qwen/Qwen2.5-0.5B-Instruct\n",
            "   Sequence length: 512\n",
            "   Effective batch: 8\n",
            "   LoRA r=16, alpha=32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
        "print(f\"\u2705 Dataset: {len(dataset)} examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55H5kBCexNV1",
        "outputId": "1aa276e2-43d8-465d-8962-a634ccdc509d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Dataset: 1000 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpu_memory():\n",
        "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
        "    return torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "def get_peak_memory():\n",
        "    \"\"\"Get peak GPU memory usage in GB.\"\"\"\n",
        "    return torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "print(\"\u2705 Memory tracking functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVKmtY4dxNV1",
        "outputId": "1a7df076-3d7b-4fa4-e2a4-1eadb8976a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Memory tracking functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Training Run"
      ],
      "metadata": {
        "id": "uaAvFiyQxNV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset memory tracking\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STANDARD TRAINING (HuggingFace + PEFT)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "print(\"\\n\ud83d\udce6 Loading model...\")\n",
        "model_std = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model_std = prepare_model_for_kbit_training(model_std)\n",
        "\n",
        "mem_after_load = get_gpu_memory()\n",
        "print(f\"   Memory after load: {mem_after_load:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "8408bb1fbb1b48858424ddf0dc5ff0e4",
            "ca10bbeb651e406b94494d57eaf9b5d9",
            "77ac4865c6884575a9c8e14d2981300e",
            "56ac562b3a684ba78fa02a8e444041c5",
            "d9d5bcbd7ecb4d7c8b8a6920b5180a58",
            "cc874469af584e18adfb9100edc5cf7d",
            "fa742763254146ba9ef781c0082ade99",
            "f278b301c4cc4ee881e80eb606496013",
            "e9c7b2198f294f4289154a26884f2380",
            "37b744951aec4213b36357fde8df44c0",
            "426c1314fd9e4b6692b4cc3d1684135d",
            "2104518496494428b6e9747767acc9fe",
            "5b1b6883b6f34d1caf8e8ac7b33cca44",
            "e7c26faef6664f1cbf80946a3a72376a",
            "263c62bcaf9d4291bcdec7e6cc90c896",
            "eaba8a4e12af49a1ab79021308b0454c",
            "289df73a92124e7ca2221b0fdd0347e1",
            "e3e57ed07815458f892989b0a3d91ccc",
            "86ecde6e39cb45af99ded551559a7919",
            "6eac17bc8f6d47d1ac0c4a7ed88bb2b4",
            "eeb749ae57f64db98f5025f74bd500ff",
            "53b9c304924d4c6085502531352244a4",
            "3afe8f13840f4dc8a5651859f3aadaad",
            "b9e32fc37601436990d267348d5416c3",
            "2766794c917d4ab8aa45d3d20fac6636",
            "961eec2700414385a8817d9d7586b705",
            "eea53a4299dd429bab95007ddff2c598",
            "f68f04de96724389be9339701f56940e",
            "d728b0a04bb2470fa80fde6c9cc16956",
            "1ad97911ba2f48a498f807495f52bd73",
            "e075c79888f9410ca909674dc0c6b723",
            "943465c828d84f349c0812b2611bc2f9",
            "e62e54941415449483ebd747ba17c524",
            "dab52b7f140244bfa204d923788add62",
            "e51e3078d0cf4b0fb0a475c7ee43e147",
            "8673287caecc41ab949b46bbd4a16a0b",
            "7c37734d8c214404b376151577134397",
            "df60759025224476a5c8a6c4513db04a",
            "f3614bba856945048e8bea2262aa2ed7",
            "fa4d6931e94047ccbfa5e92017a54a0f",
            "c68c0546bd09480fa9479e0a35072889",
            "e051bfef6c3643ebbd9903a5368b7568",
            "53d66e4e2add4d14a112174ec79f2d19",
            "46a61ff92f3e4dce9a8eeb048bfc5956",
            "0e1d1fc9df1e44a9b08e2b5485ce9851",
            "b07cbbc1ae424f458badd580d64fc409",
            "f8dbe8e9e3ea4e1cb811482664938555",
            "30084ebe67e347d18da27b4cac476133",
            "5c864d0241f140f581313ec65f869d8e",
            "746f0b126536471381b0e9cfdd1be45b",
            "6e0926beeb3a44b3acbb57d60dd0d047",
            "978e9b81ecaa4bbd88dbbf69a7e15c09",
            "0a4d698f05a44a04a77f98880b536486",
            "7bfa4972d406414e992730e773c177c4",
            "1d23f60a5de74aaf974a84e4a5ef3e65",
            "d83b2ae694c84c01817bffca57544209",
            "241a892c495945f59237aa76ea8f0ccd",
            "f2dfa24ece26485e88f3a819141ec1f2",
            "407d7885273a496195f85a019afff791",
            "6c836472e0bd4067b464e929510d64bc",
            "1704a14da87945bfa49f8e04d621cf0d",
            "324187f8555c48f182069cacf6f33994",
            "478ef2ef892742a99d6b9244056feeef",
            "5bf081d428fd4ddd9220ee6c015c29f4",
            "769333dd90a340109343edbcfae9e52d",
            "e7266a186b6d423bb1c10b7eb6920996",
            "75981c50f1fb4b76832b8c15006b3cf3",
            "8c0ba0a9885e491bbace212b36d3460b",
            "d208be93cd1b458f9c0b432e1a9462bb",
            "7fbb67991c794a30bbfb3aa3b854a9b5",
            "7c6bf128550e467781ec3fb420ef4456",
            "515c8c9bcc63453b8cd27e96c060559e",
            "e6e7730174bb45f392d30ea3b7cd6dcb",
            "8db1cd3977814ca3b19c1df7cbb4942a",
            "693393dac2be4519a6eb95875f76b6da",
            "dfeee2d211bc4989baf740a0fbe02806",
            "4de85df608d541c9a7b7d1b0966cf9d3"
          ]
        },
        "id": "VNGDu_SixNV2",
        "outputId": "ae0e2890-9dbd-476d-aea5-f928f6a913dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STANDARD TRAINING (HuggingFace + PEFT)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udce6 Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Memory after load: 0.73 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model_std = get_peft_model(model_std, lora_config)\n",
        "model_std.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmaJoiX0xNV2",
        "outputId": "a0a142c3-b1d0-42fb-fe69-7c85184b1402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args_std = TrainingArguments(\n",
        "    output_dir=\"./standard_output\",\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    bf16=True,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"adamw_torch\",\n",
        "    warmup_ratio=0.03,\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer_std = SFTTrainer(\n",
        "    model=model_std,\n",
        "    args=training_args_std,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    #max_seq_length=MAX_SEQ_LENGTH,\n",
        ")\n",
        "\n",
        "print(\"\u2705 Trainer configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "ac2caa5a29d041a2a0e49568cfbd353f",
            "d41c22db1b9443bcabbfb1bfac1f0574",
            "8bac7dabd7f340cbacf1ad4c95c49deb",
            "0ffbfe3d2c1d4c8990c104504b1ae638",
            "ad34ee1b1a114265947224f84d33e7bc",
            "5fb78f0cafef45928c958b34cd0c0df5",
            "0b4a0028e96047aebb2ff62a7c074f66",
            "e465e58faf05488fab1e6c154286310e",
            "e57b3fbd664841abb52bea6e7ab675b4",
            "a00f8c278d6046f7a3ae27a4bdeb497e",
            "af36036ee5fd4aa4ac6c549451be238a",
            "1359b1b738d4404181e001b9bca7fc88",
            "a4a377ee1763493d98b813f6965f5439",
            "5a75832f133e4f59953d91564153314a",
            "ddf4399b496947b4aa4e393fa91f073e",
            "8b98a45e55024edcb799f5d9735a4b51",
            "75ef0f7c841e4fd781acb0445e5e6402",
            "0b7cb6d23e0d4cdd91d567f2e5b6e80f",
            "3b0eae7ba2404d4780e7d9e1c10fe326",
            "465f5bdb67844d1896092845d1b7d57b",
            "18ddd28fdd1c40d4a77266c22410d586",
            "91661fb65ae74021b0fdf79437e30a40",
            "58ece76aea6a4fc4a913316dc23cf2c9",
            "0cf3802f13e047bbbaafecda61cda0b7",
            "426a85a0f5f649ed865fb5c946fb0ae7",
            "073e37f4b9fd4083af70027f6516ec36",
            "dd8388aaa9a1426f8aafd7aae3f4d5a2",
            "469f5b0e51cd416ebfab3d2de707df46",
            "925b3eb31bad47fdac584c196d6c5407",
            "3791e6b3e1714eecbf0c19ec6dd2e7fd",
            "101ee60fa8ac46258ffdd3e5c92c272f",
            "86f756cec9dd4c638f68da7417c8f625",
            "100abf3e2add4336b450e382e6888a7e"
          ]
        },
        "id": "opOusQ5IxNV2",
        "outputId": "abe5f32e-2ac1-451a-a507-6d65f0bf7ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of \ud83e\udd17 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Trainer configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!\n",
        "print(\"\\n\ud83d\ude80 Starting standard training...\\n\")\n",
        "train_start = time.time()\n",
        "\n",
        "result_std = trainer_std.train()\n",
        "\n",
        "train_time_std = time.time() - train_start\n",
        "peak_memory_std = get_peak_memory()\n",
        "final_loss_std = result_std.training_loss\n",
        "\n",
        "print(f\"\\n\u2705 Standard training complete!\")\n",
        "print(f\"   Training time: {train_time_std:.1f} seconds\")\n",
        "print(f\"   Peak memory:   {peak_memory_std:.2f} GB\")\n",
        "print(f\"   Final loss:    {final_loss_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "sMLfRV8zxNV3",
        "outputId": "620fbda8-7a66-4890-9f1f-dd55a8a56669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\ude80 Starting standard training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Standard training complete!\n",
            "   Training time: 784.2 seconds\n",
            "   Peak memory:   4.96 GB\n",
            "   Final loss:    1.8213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results\n",
        "standard_results = {\n",
        "    \"training_time\": train_time_std,\n",
        "    \"peak_memory\": peak_memory_std,\n",
        "    \"final_loss\": final_loss_std,\n",
        "}\n",
        "\n",
        "# Cleanup\n",
        "del model_std\n",
        "del trainer_std\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\ud83e\uddf9 Cleaned up standard training objects\")\n",
        "print(f\"   GPU Memory now: {get_gpu_memory():.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-tiYYbqxNV3",
        "outputId": "4afd3880-1a1b-4b3a-bf04-d5342e22a792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\uddf9 Cleaned up standard training objects\n",
            "   GPU Memory now: 0.02 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part B: Unsloth Training\n",
        "\n",
        "Now let's run the same training with Unsloth optimizations."
      ],
      "metadata": {
        "id": "j-abCqTCxNV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Unsloth\n",
        "!pip install -q unsloth\n",
        "# For Colab, we might need specific version\n",
        "!pip install -q --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gio8_2GxNV3",
        "outputId": "a5137889-fb81-4c75-8d86-7ee236247cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m389.2/389.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m615.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"UNSLOTH_DISABLE_PATCHING\"] = \"1\"   # TRL patching kapal\u0131\n",
        "os.environ[\"UNSLOTH_USE_FUSED_LOSS\"] = \"0\"     # fused loss kapal\u0131\n",
        "os.environ[\"UNSLOTH_DISABLE_COMPILE\"] = \"1\"    # unsloth compile kapal\u0131\n",
        "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"        # torch dynamo kapal\u0131\n"
      ],
      "metadata": {
        "id": "TvrgDeMf6JQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQRAF8bQ3c_R",
        "outputId": "9ec76c30-033a-4a75-c1e9-d6c70e773ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpu_memory():\n",
        "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
        "    return torch.cuda.memory_allocated() / 1e9\n",
        "\n",
        "def get_peak_memory():\n",
        "    \"\"\"Get peak GPU memory usage in GB.\"\"\"\n",
        "    return torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "print(\"\u2705 Memory tracking functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZHxneTt7KT9",
        "outputId": "b7f3d14f-444f-4b21-e46c-0f178925e12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Memory tracking functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration - same for both experiments\n",
        "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "MAX_SEQ_LENGTH = 512\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 4\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "\n",
        "# LoRA settings\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(\"\ud83d\udccb Configuration:\")\n",
        "print(f\"   Model: {MODEL_ID}\")\n",
        "print(f\"   Sequence length: {MAX_SEQ_LENGTH}\")\n",
        "print(f\"   Effective batch: {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(f\"   LoRA r={LORA_R}, alpha={LORA_ALPHA}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX-u8z4f6SKO",
        "outputId": "c7a0277c-3a3a-4779-9341-7d06ce9084c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udccb Configuration:\n",
            "   Model: Qwen/Qwen2.5-0.5B-Instruct\n",
            "   Sequence length: 512\n",
            "   Effective batch: 8\n",
            "   LoRA r=16, alpha=32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
        "print(f\"\u2705 Dataset: {len(dataset)} examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oIKJ1Nk6YO-",
        "outputId": "5aab2b61-f852-471b-a2e5-fee6a5ef97c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Dataset: 1000 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Workaround: Use unsloth's bundled trainer instead of patching TRL\n",
        "import os\n",
        "import time\n",
        "os.environ[\"UNSLOTH_DISABLE_PATCHING\"] = \"1\"  # Disable TRL patching\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Load model with Unsloth\n",
        "print(\"\\n\ud83d\udce6 Loading model with Unsloth...\")\n",
        "model_unsloth, tokenizer_unsloth = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_ID,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto-detect\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "mem_after_load = get_gpu_memory()\n",
        "print(f\"   Memory after load: {mem_after_load:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxO3Lx-IxNV4",
        "outputId": "3207d5e8-57f5-4617-fb79-d586724c7467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udce6 Loading model with Unsloth...\n",
            "==((====))==  Unsloth 2026.1.3: Fast Qwen2 patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "   Memory after load: 1.10 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA with Unsloth (same settings)\n",
        "model_unsloth = FastLanguageModel.get_peft_model(\n",
        "    model_unsloth,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"\u2705 LoRA applied with Unsloth optimizations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0iQZCTnxNV4",
        "outputId": "05894d93-710e-4909-dceb-4b50e380ea75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2026.1.3 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 LoRA applied with Unsloth optimizations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tok_fn(examples):\n",
        "    out = tokenizer_unsloth(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=False,          # important: no padding in dataset\n",
        "    )\n",
        "    return out\n",
        "\n",
        "tokenized = dataset.map(tok_fn, batched=True, remove_columns=dataset.column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7a1225e91d744329ba56414024f2942b",
            "975834001db442b88ab9e0e93f738133",
            "2b757a1bddd0471f8cc3e0714bcc4d5e",
            "7b667a6005154083ad3598713aa79183",
            "364552413ced42f6a4b9bcef2ca89c60",
            "d8cc34d0d99e498f94e534b341a16462",
            "66157f344b02408a90deb9a5033d3593",
            "3e9657abf9924048942e87e5a9aaf71d",
            "ace8f7e650c94f6983aa1a6bcc6859d1",
            "d5e1d1c19e6e46c997319e39f76aac50",
            "5a37e45f5b954b568d87aedec070a31d"
          ]
        },
        "id": "bTkO3Qjp9KgE",
        "outputId": "95aad2c2-2e03-4fba-c429-165c02a713b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "class CausalLMDataCollator:\n",
        "    def __init__(self, tokenizer, max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.pad = DataCollatorWithPadding(\n",
        "            tokenizer=tokenizer,\n",
        "            padding=True,\n",
        "            max_length=max_length,     # \u2705 ensure pad length never exceeds max\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = self.pad(features)\n",
        "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "\n",
        "        pad_id = self.tokenizer.pad_token_id\n",
        "        if pad_id is not None:\n",
        "            batch[\"labels\"][batch[\"input_ids\"] == pad_id] = -100\n",
        "        return batch\n",
        "\n",
        "collator = CausalLMDataCollator(tokenizer_unsloth, MAX_SEQ_LENGTH)\n"
      ],
      "metadata": {
        "id": "v6rnzU2G-VMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "# Disable problematic Unsloth optimizations\n",
        "os.environ[\"UNSLOTH_DISABLE_COMPILE\"] = \"1\"\n",
        "os.environ[\"UNSLOTH_USE_FUSED_LOSS\"] = \"0\"\n",
        "TORCHDYNAMO_VERBOSE=1\n",
        "TORCH_LOGS=\"+dynamo\"\n",
        "\n",
        "# Training arguments\n",
        "training_args_unsloth = TrainingArguments(\n",
        "    output_dir=\"./unsloth_output\",\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    warmup_ratio=0.03,\n",
        "    max_grad_norm=0.3,\n",
        "    report_to=\"none\",\n",
        "    torch_compile=False,\n",
        ")\n",
        "\n",
        "# Use Unsloth's own trainer if available, otherwise standard\n",
        "try:\n",
        "    from unsloth import UnslothTrainer as SFTTrainerToUse\n",
        "    print(\"Using UnslothTrainer\")\n",
        "except ImportError:\n",
        "    from trl import SFTTrainer as SFTTrainerToUse\n",
        "    print(\"Using standard SFTTrainer\")\n",
        "\n",
        "\n",
        "trainer_unsloth = SFTTrainerToUse(\n",
        "    model=model_unsloth,\n",
        "    args=training_args_unsloth,\n",
        "    train_dataset=tokenized,\n",
        "    tokenizer=tokenizer_unsloth,\n",
        "    data_collator=collator,     # \u2705 key\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"\u2705 Unsloth trainer configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAOCvUlcxNV5",
        "outputId": "e0f5eb22-7cf2-493d-903b-a5850ba50c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using UnslothTrainer\n",
            "\u2705 Unsloth trainer configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dl = trainer_unsloth.get_train_dataloader()\n",
        "batch = next(iter(dl))\n",
        "\n",
        "print(\"input_ids:\", batch[\"input_ids\"].shape)\n",
        "print(\"labels:\", batch[\"labels\"].shape)\n",
        "print(\"attention_mask:\", batch[\"attention_mask\"].shape)\n",
        "\n",
        "# Flatten sizes (what CE effectively sees)\n",
        "print(\"flat input tokens:\", batch[\"input_ids\"].numel())\n",
        "print(\"flat label tokens:\", batch[\"labels\"].numel())\n",
        "\n",
        "# Check exact mismatch positions\n",
        "print(\"same shape?\", batch[\"input_ids\"].shape == batch[\"labels\"].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-i1AAZM8aql",
        "outputId": "c670b942-9d43-47c2-8642-22c559655984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: torch.Size([2, 512])\n",
            "labels: torch.Size([2, 512])\n",
            "attention_mask: torch.Size([2, 512])\n",
            "flat input tokens: 1024\n",
            "flat label tokens: 1024\n",
            "same shape? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with Unsloth!\n",
        "print(\"\\n\ud83d\ude80 Starting Unsloth training...\\n\")\n",
        "train_start = time.time()\n",
        "\n",
        "result_unsloth = trainer_unsloth.train()\n",
        "\n",
        "train_time_unsloth = time.time() - train_start\n",
        "peak_memory_unsloth = get_peak_memory()\n",
        "final_loss_unsloth = result_unsloth.training_loss\n",
        "\n",
        "print(f\"\\n\u2705 Unsloth training complete!\")\n",
        "print(f\"   Training time: {train_time_unsloth:.1f} seconds\")\n",
        "print(f\"   Peak memory:   {peak_memory_unsloth:.2f} GB\")\n",
        "print(f\"   Final loss:    {final_loss_unsloth:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "cXigxUGFxNV5",
        "outputId": "263af0f4-8046-4633-c9d3-4f251d3563d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\ude80 Starting Unsloth training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 125\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 2,162,688 of 496,195,456 (0.44% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Unsloth training complete!\n",
            "   Training time: 140.1 seconds\n",
            "   Peak memory:   5.64 GB\n",
            "   Final loss:    1.8396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results\n",
        "unsloth_results = {\n",
        "    \"training_time\": train_time_unsloth,\n",
        "    \"peak_memory\": peak_memory_unsloth,\n",
        "    \"final_loss\": final_loss_unsloth,\n",
        "}"
      ],
      "metadata": {
        "id": "qjwQDUM8xNV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part C: Comparison Results"
      ],
      "metadata": {
        "id": "i7UnLXW6xNV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Step\tTraining Loss\n",
        "50\t1.879600\n",
        "100\t1.786800\n",
        "\n",
        "\u2705 Standard training complete!\n",
        "   Training time: 784.2 seconds\n",
        "   Peak memory:   4.96 GB\n",
        "   Final loss:    1.8213\n"
      ],
      "metadata": {
        "id": "taacov8Z_SOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   # Store results\n",
        "standard_results = {\n",
        "    \"training_time\": 784.2,\n",
        "    \"peak_memory\": 4.96,\n",
        "    \"final_loss\": 1.8213,\n",
        "}"
      ],
      "metadata": {
        "id": "iPeKmu1Q_ft8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate improvements\n",
        "time_speedup = standard_results[\"training_time\"] / unsloth_results[\"training_time\"]\n",
        "memory_reduction = (1 - unsloth_results[\"peak_memory\"] / standard_results[\"peak_memory\"]) * 100\n",
        "loss_diff = abs(standard_results[\"final_loss\"] - unsloth_results[\"final_loss\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udcca COMPARISON: STANDARD vs UNSLOTH\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Metric':<25} {'Standard':<15} {'Unsloth':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "print(f\"{'Training Time':<25} {standard_results['training_time']:<15.1f} {unsloth_results['training_time']:<15.1f} {time_speedup:.2f}x faster\")\n",
        "print(f\"{'Peak Memory (GB)':<25} {standard_results['peak_memory']:<15.2f} {unsloth_results['peak_memory']:<15.2f} {memory_reduction:.1f}% less\")\n",
        "print(f\"{'Final Loss':<25} {standard_results['final_loss']:<15.4f} {unsloth_results['final_loss']:<15.4f} {'\u2705 Similar' if loss_diff < 0.1 else '\u26a0\ufe0f Different'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rSU9_YaxNV6",
        "outputId": "7bc3d628-c3db-4d17-c480-e7cab31784a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\ud83d\udcca COMPARISON: STANDARD vs UNSLOTH\n",
            "======================================================================\n",
            "\n",
            "Metric                    Standard        Unsloth         Improvement    \n",
            "----------------------------------------------------------------------\n",
            "Training Time             784.2           140.1           5.60x faster\n",
            "Peak Memory (GB)          4.96            5.64            -13.7% less\n",
            "Final Loss                1.8213          1.8396          \u2705 Similar\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual comparison\n",
        "print(\"\\n\ud83d\udcc8 VISUAL COMPARISON\")\n",
        "print(\"\\nTraining Time:\")\n",
        "std_bar = \"\u2588\" * int(standard_results[\"training_time\"] / 10)\n",
        "uns_bar = \"\u2588\" * int(unsloth_results[\"training_time\"] / 10)\n",
        "print(f\"  Standard: {std_bar} {standard_results['training_time']:.0f}s\")\n",
        "print(f\"  Unsloth:  {uns_bar} {unsloth_results['training_time']:.0f}s\")\n",
        "\n",
        "print(\"\\nPeak Memory:\")\n",
        "std_bar = \"\u2588\" * int(standard_results[\"peak_memory\"] * 5)\n",
        "uns_bar = \"\u2588\" * int(unsloth_results[\"peak_memory\"] * 5)\n",
        "print(f\"  Standard: {std_bar} {standard_results['peak_memory']:.1f} GB\")\n",
        "print(f\"  Unsloth:  {uns_bar} {unsloth_results['peak_memory']:.1f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY8k9vpxxNV6",
        "outputId": "65896b91-4648-4ade-c250-0cf0e55a5a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\ud83d\udcc8 VISUAL COMPARISON\n",
            "\n",
            "Training Time:\n",
            "  Standard: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 784s\n",
            "  Unsloth:  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 140s\n",
            "\n",
            "Peak Memory:\n",
            "  Standard: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 5.0 GB\n",
            "  Unsloth:  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 5.6 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part D: Unsloth-Specific Features\n",
        "\n",
        "Unsloth offers additional features beyond just speed."
      ],
      "metadata": {
        "id": "s1F-ECeHxNV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 1: Easy Model Saving\n",
        "\n",
        "Unsloth makes it easy to save in multiple formats:"
      ],
      "metadata": {
        "id": "9fCo0tTLxNV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as LoRA adapter (smallest)\n",
        "model_unsloth.save_pretrained(\"./unsloth_lora\")\n",
        "tokenizer_unsloth.save_pretrained(\"./unsloth_lora\")\n",
        "print(\"\u2705 Saved LoRA adapter\")\n",
        "\n",
        "# Check size\n",
        "import os\n",
        "lora_size = sum(os.path.getsize(os.path.join(\"./unsloth_lora\", f))\n",
        "               for f in os.listdir(\"./unsloth_lora\")\n",
        "               if os.path.isfile(os.path.join(\"./unsloth_lora\", f)))\n",
        "print(f\"   Adapter size: {lora_size / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhziplLuxNV6",
        "outputId": "bd665ae8-380f-4874-b7df-4b23bb7aad82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Saved LoRA adapter\n",
            "   Adapter size: 24.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save merged model in different formats\n",
        "print(\"\\n\ud83d\udce6 Unsloth can save in multiple formats:\")\n",
        "print(\"\"\"\n",
        "# Save as merged 16-bit (for HuggingFace)\n",
        "model.save_pretrained_merged(\"model_16bit\", tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# Save as 4-bit quantized\n",
        "model.save_pretrained_merged(\"model_4bit\", tokenizer, save_method=\"merged_4bit\")\n",
        "\n",
        "# Save as GGUF for llama.cpp\n",
        "model.save_pretrained_gguf(\"model_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "\n",
        "# Push to HuggingFace Hub\n",
        "model.push_to_hub_merged(\"username/model-name\", tokenizer, save_method=\"merged_16bit\")\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L14IQ_-XxNV6",
        "outputId": "8c70970b-13a6-4b60-abde-b07a761b3870"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 2: Fast Inference Mode"
      ],
      "metadata": {
        "id": "dPdQwgOAxNV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable fast inference\n",
        "FastLanguageModel.for_inference(model_unsloth)\n",
        "\n",
        "# Test generation\n",
        "messages = [{\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence.\"}]\n",
        "inputs = tokenizer_unsloth.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Time the generation\n",
        "start = time.time()\n",
        "outputs = model_unsloth.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=64,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ")\n",
        "gen_time = time.time() - start\n",
        "\n",
        "response = tokenizer_unsloth.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\ud83e\udd16 Response ({gen_time:.2f}s):\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLn1_1eSxNV7",
        "outputId": "f0fadb54-edb4-4843-aafd-26e5d330ab72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83e\udd16 Response (2.15s):\n",
            "system\n",
            "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "user\n",
            "Explain quantum computing in one sentence.\n",
            "assistant\n",
            "Quantum computers can process and analyze large amounts of data much faster than classical computers due to the principles of quantum mechanics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Summary: When to Use Unsloth\n",
        "\n",
        "## \u2705 Use Unsloth When:\n",
        "\n",
        "| Situation | Benefit |\n",
        "|-----------|--------|\n",
        "| Limited GPU memory | 50-70% less VRAM |\n",
        "| Long training runs | 2-5x faster |\n",
        "| Rapid experimentation | Quick iteration |\n",
        "| GGUF export needed | Built-in support |\n",
        "| HuggingFace deployment | Easy upload |\n",
        "\n",
        "## \u26a0\ufe0f Consider Standard When:\n",
        "\n",
        "| Situation | Reason |\n",
        "|-----------|--------|\n",
        "| Unsupported model | Unsloth doesn't support all architectures |\n",
        "| Custom training loops | More flexibility with standard PyTorch |\n",
        "| Debugging needed | Easier to debug standard code |\n",
        "| Production stability | HuggingFace is more battle-tested |\n",
        "\n",
        "## Supported Models (as of 2024)\n",
        "\n",
        "- Llama 2, Llama 3, Llama 3.1, Llama 3.2\n",
        "- Mistral, Mixtral\n",
        "- Qwen, Qwen2, Qwen2.5\n",
        "- Phi-3, Phi-4\n",
        "- Gemma, Gemma 2\n",
        "- And more...\n",
        "\n",
        "Check: https://github.com/unslothai/unsloth for latest supported models."
      ],
      "metadata": {
        "id": "W6sIUEEJxNV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Key Takeaways\n",
        "\n",
        "1. **Unsloth provides significant speedups** \u2014 typically 2-5x faster training\n",
        "\n",
        "2. **Memory savings are substantial** \u2014 50-70% less VRAM means larger batches or models\n",
        "\n",
        "3. **Quality is preserved** \u2014 same loss/outputs as standard training\n",
        "\n",
        "4. **API is similar** \u2014 easy to switch from standard PEFT/TRL\n",
        "\n",
        "5. **Extra features** \u2014 GGUF export, easy Hub upload, optimized inference\n",
        "\n",
        "---\n",
        "\n",
        "## Next: Part 8 - Inference & Deployment\n",
        "\n",
        "We'll cover how to deploy your fine-tuned model for actual use!"
      ],
      "metadata": {
        "id": "CWkAV3MExNV7"
      }
    }
  ]
}